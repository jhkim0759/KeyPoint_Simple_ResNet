{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21831,
     "status": "ok",
     "timestamp": 1616808796236,
     "user": {
      "displayName": "김정환",
      "photoUrl": "",
      "userId": "11315038694579600734"
     },
     "user_tz": -540
    },
    "id": "-7ZpA_5QBOIE",
    "outputId": "3a4c34c9-1060-448b-93a6-bfe27852c726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44714,
     "status": "ok",
     "timestamp": 1616808819226,
     "user": {
      "displayName": "김정환",
      "photoUrl": "",
      "userId": "11315038694579600734"
     },
     "user_tz": -540
    },
    "id": "vvnTY7y3BOFG",
    "outputId": "cf590df2-8592-4bac-dc87-c74351c156af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/146-1-1-06-Z36_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/177-1-1-07-Z36_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/178-1-1-07-Z36_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/185-1-1-08-Z36_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/186-1-1-08-Z36_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/193-2-1-09-Z36_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/194-2-1-09-Z36_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/201-2-1-10-Z36_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/202-2-1-10-Z36_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/209-2-1-11-Z36_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/210-2-1-11-Z36_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/217-2-1-12-Z36_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/218-2-1-12-Z36_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/249-2-1-13-Z57_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/250-2-1-13-Z57_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/281-2-1-14-Z57_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/282-2-1-14-Z57_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/313-2-1-15-Z57_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/314-2-1-15-Z57_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/329-2-1-16-Z57_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/330-2-1-16-Z57_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/473-1-2-21-Z134_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/474-1-2-21-Z134_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/489-1-2-22-Z134_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/490-1-2-22-Z134_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/505-1-2-23-Z134_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/506-1-2-23-Z134_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/513-1-2-24-Z134_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/514-1-2-24-Z134_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/545-1-2-25-Z134_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/546-1-2-25-Z134_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/553-1-2-26-Z134_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/554-1-2-26-Z134_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/561-1-3-27-Z134_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/562-1-3-27-Z134_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/593-1-3-28-Z134_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/594-1-3-28-Z134_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/625-1-3-29-Z134_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/626-1-3-29-Z134_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/633-2-4-30-Z148_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/634-2-4-30-Z148_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/641-2-4-31-Z148_E-0000031.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_A-0000001.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_A-0000003.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_A-0000005.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_A-0000007.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_A-0000009.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_A-0000011.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_A-0000013.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_A-0000015.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_A-0000017.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_A-0000019.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_A-0000021.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_A-0000023.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_A-0000025.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_A-0000027.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_A-0000029.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_A-0000031.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_B-0000001.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_B-0000003.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_B-0000005.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_B-0000007.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_B-0000009.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_B-0000011.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_B-0000013.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_B-0000015.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_B-0000017.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_B-0000019.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_B-0000021.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_B-0000023.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_B-0000025.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_B-0000027.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_B-0000029.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_B-0000031.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_C-0000001.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_C-0000003.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_C-0000005.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_C-0000007.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_C-0000009.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_C-0000011.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_C-0000013.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_C-0000015.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_C-0000017.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_C-0000019.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_C-0000021.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_C-0000023.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_C-0000025.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_C-0000027.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_C-0000029.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_C-0000031.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_D-0000001.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_D-0000003.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_D-0000005.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_D-0000007.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_D-0000009.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_D-0000011.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_D-0000013.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_D-0000015.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_D-0000017.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_D-0000019.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_D-0000021.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_D-0000023.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_D-0000025.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_D-0000027.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_D-0000029.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_D-0000031.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_E-0000001.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_E-0000003.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_E-0000005.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_E-0000007.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_E-0000009.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_E-0000011.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_E-0000013.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_E-0000015.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_E-0000017.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_E-0000019.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_E-0000021.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_E-0000023.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_E-0000025.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_E-0000027.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_E-0000029.jpg  \n",
      "  inflating: ./train_imgs/642-2-4-31-Z148_E-0000031.jpg  \n",
      "Archive:  /content/drive/MyDrive/OpenPose/test_imgs.zip\n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_A-0000001.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_A-0000003.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_A-0000005.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_A-0000007.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_A-0000009.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_A-0000011.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_A-0000013.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_A-0000015.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_A-0000017.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_A-0000019.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_A-0000021.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_A-0000023.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_A-0000025.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_A-0000027.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_A-0000029.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_A-0000031.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_B-0000001.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_B-0000003.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_B-0000005.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_B-0000007.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_B-0000009.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_B-0000011.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_B-0000013.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_B-0000015.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_B-0000017.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_B-0000019.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_B-0000021.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_B-0000023.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_B-0000025.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_B-0000027.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_B-0000029.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_B-0000031.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_C-0000001.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_C-0000003.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_C-0000005.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_C-0000007.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_C-0000009.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_C-0000011.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_C-0000013.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_C-0000015.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_C-0000017.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_C-0000019.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_C-0000021.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_C-0000023.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_C-0000025.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_C-0000027.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_C-0000029.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_C-0000031.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_D-0000001.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_D-0000003.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_D-0000005.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_D-0000007.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_D-0000009.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_D-0000011.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_D-0000013.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_D-0000015.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_D-0000017.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_D-0000019.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_D-0000021.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_D-0000023.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_D-0000025.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_D-0000027.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_D-0000029.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_D-0000031.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_E-0000001.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_E-0000003.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_E-0000005.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_E-0000007.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_E-0000009.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_E-0000011.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_E-0000013.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_E-0000015.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_E-0000017.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_E-0000019.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_E-0000021.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_E-0000023.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_E-0000025.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_E-0000027.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_E-0000029.jpg  \n",
      "  inflating: ./test_imgs/649-2-4-32-Z148_E-0000031.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_A-0000001.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_A-0000003.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_A-0000005.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_A-0000007.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_A-0000009.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_A-0000011.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_A-0000013.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_A-0000015.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_A-0000017.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_A-0000019.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_A-0000021.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_A-0000023.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_A-0000025.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_A-0000027.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_A-0000029.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_A-0000031.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_B-0000001.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_B-0000003.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_B-0000005.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_B-0000007.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_B-0000009.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_B-0000011.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_B-0000013.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_B-0000015.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_B-0000017.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_B-0000019.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_B-0000021.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_B-0000023.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_B-0000025.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_B-0000027.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_B-0000029.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_B-0000031.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_C-0000001.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_C-0000003.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_C-0000005.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_C-0000007.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_C-0000009.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_C-0000011.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_C-0000013.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_C-0000015.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_C-0000017.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_C-0000019.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_C-0000021.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_C-0000023.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_C-0000025.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_C-0000027.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_C-0000029.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_C-0000031.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_D-0000001.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_D-0000003.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_D-0000005.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_D-0000007.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_D-0000009.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_D-0000011.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_D-0000013.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_D-0000015.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_D-0000017.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_D-0000019.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_D-0000021.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_D-0000023.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_D-0000025.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_D-0000027.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_D-0000029.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_D-0000031.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_E-0000001.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_E-0000003.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_E-0000005.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_E-0000007.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_E-0000009.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_E-0000011.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_E-0000013.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_E-0000015.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_E-0000017.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_E-0000019.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_E-0000021.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_E-0000023.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_E-0000025.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_E-0000027.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_E-0000029.jpg  \n",
      "  inflating: ./test_imgs/650-2-4-32-Z148_E-0000031.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_A-0000001.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_A-0000003.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_A-0000005.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_A-0000007.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_A-0000009.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_A-0000011.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_A-0000013.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_A-0000015.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_A-0000017.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_A-0000019.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_A-0000021.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_A-0000023.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_A-0000025.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_A-0000027.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_A-0000029.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_A-0000031.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_B-0000001.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_B-0000003.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_B-0000005.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_B-0000007.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_B-0000009.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_B-0000011.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_B-0000013.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_B-0000015.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_B-0000017.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_B-0000019.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_B-0000021.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_B-0000023.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_B-0000025.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_B-0000027.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_B-0000029.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_B-0000031.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_C-0000001.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_C-0000003.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_C-0000005.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_C-0000007.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_C-0000009.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_C-0000011.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_C-0000013.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_C-0000015.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_C-0000017.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_C-0000019.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_C-0000021.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_C-0000023.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_C-0000025.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_C-0000027.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_C-0000029.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_C-0000031.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_D-0000001.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_D-0000003.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_D-0000005.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_D-0000007.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_D-0000009.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_D-0000011.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_D-0000013.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_D-0000015.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_D-0000017.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_D-0000019.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_D-0000021.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_D-0000023.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_D-0000025.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_D-0000027.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_D-0000029.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_D-0000031.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_E-0000001.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_E-0000003.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_E-0000005.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_E-0000007.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_E-0000009.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_E-0000011.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_E-0000013.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_E-0000015.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_E-0000017.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_E-0000019.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_E-0000021.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_E-0000023.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_E-0000025.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_E-0000027.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_E-0000029.jpg  \n",
      "  inflating: ./test_imgs/665-2-4-33-Z148_E-0000031.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_A-0000001.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_A-0000003.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_A-0000005.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_A-0000007.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_A-0000009.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_A-0000011.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_A-0000013.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_A-0000015.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_A-0000017.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_A-0000019.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_A-0000021.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_A-0000023.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_A-0000025.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_A-0000027.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_A-0000029.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_A-0000031.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_B-0000001.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_B-0000003.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_B-0000005.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_B-0000007.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_B-0000009.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_B-0000011.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_B-0000013.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_B-0000015.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_B-0000017.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_B-0000019.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_B-0000021.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_B-0000023.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_B-0000025.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_B-0000027.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_B-0000029.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_B-0000031.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_C-0000001.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_C-0000003.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_C-0000005.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_C-0000007.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_C-0000009.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_C-0000011.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_C-0000013.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_C-0000015.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_C-0000017.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_C-0000019.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_C-0000021.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_C-0000023.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_C-0000025.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_C-0000027.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_C-0000029.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_C-0000031.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_D-0000001.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_D-0000003.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_D-0000005.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_D-0000007.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_D-0000009.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_D-0000011.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_D-0000013.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_D-0000015.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_D-0000017.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_D-0000019.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_D-0000021.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_D-0000023.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_D-0000025.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_D-0000027.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_D-0000029.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_D-0000031.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_E-0000001.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_E-0000003.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_E-0000005.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_E-0000007.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_E-0000009.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_E-0000011.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_E-0000013.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_E-0000015.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_E-0000017.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_E-0000019.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_E-0000021.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_E-0000023.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_E-0000025.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_E-0000027.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_E-0000029.jpg  \n",
      "  inflating: ./test_imgs/666-2-4-33-Z148_E-0000031.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_A-0000001.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_A-0000003.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_A-0000005.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_A-0000007.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_A-0000009.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_A-0000011.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_A-0000013.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_A-0000015.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_A-0000017.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_A-0000019.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_A-0000021.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_A-0000023.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_A-0000025.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_A-0000027.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_A-0000029.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_A-0000031.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_B-0000001.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_B-0000003.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_B-0000005.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_B-0000007.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_B-0000009.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_B-0000011.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_B-0000013.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_B-0000015.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_B-0000017.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_B-0000019.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_B-0000021.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_B-0000023.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_B-0000025.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_B-0000027.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_B-0000029.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_B-0000031.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_C-0000001.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_C-0000003.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_C-0000005.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_C-0000007.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_C-0000009.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_C-0000011.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_C-0000013.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_C-0000015.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_C-0000017.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_C-0000019.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_C-0000021.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_C-0000023.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_C-0000025.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_C-0000027.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_C-0000029.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_C-0000031.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_D-0000001.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_D-0000003.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_D-0000005.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_D-0000007.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_D-0000009.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_D-0000011.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_D-0000013.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_D-0000015.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_D-0000017.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_D-0000019.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_D-0000021.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_D-0000023.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_D-0000025.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_D-0000027.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_D-0000029.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_D-0000031.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_E-0000001.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_E-0000003.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_E-0000005.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_E-0000007.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_E-0000009.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_E-0000011.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_E-0000013.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_E-0000015.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_E-0000017.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_E-0000019.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_E-0000021.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_E-0000023.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_E-0000025.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_E-0000027.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_E-0000029.jpg  \n",
      "  inflating: ./test_imgs/697-3-5-34-Z94_E-0000031.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_A-0000001.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_A-0000003.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_A-0000005.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_A-0000007.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_A-0000009.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_A-0000011.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_A-0000013.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_A-0000015.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_A-0000017.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_A-0000019.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_A-0000021.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_A-0000023.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_A-0000025.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_A-0000027.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_A-0000029.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_A-0000031.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_B-0000001.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_B-0000003.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_B-0000005.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_B-0000007.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_B-0000009.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_B-0000011.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_B-0000013.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_B-0000015.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_B-0000017.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_B-0000019.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_B-0000021.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_B-0000023.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_B-0000025.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_B-0000027.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_B-0000029.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_B-0000031.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_C-0000001.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_C-0000003.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_C-0000005.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_C-0000007.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_C-0000009.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_C-0000011.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_C-0000013.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_C-0000015.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_C-0000017.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_C-0000019.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_C-0000021.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_C-0000023.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_C-0000025.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_C-0000027.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_C-0000029.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_C-0000031.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_D-0000001.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_D-0000003.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_D-0000005.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_D-0000007.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_D-0000009.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_D-0000011.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_D-0000013.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_D-0000015.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_D-0000017.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_D-0000019.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_D-0000021.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_D-0000023.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_D-0000025.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_D-0000027.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_D-0000029.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_D-0000031.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_E-0000001.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_E-0000003.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_E-0000005.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_E-0000007.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_E-0000009.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_E-0000011.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_E-0000013.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_E-0000015.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_E-0000017.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_E-0000019.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_E-0000021.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_E-0000023.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_E-0000025.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_E-0000027.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_E-0000029.jpg  \n",
      "  inflating: ./test_imgs/698-3-5-34-Z94_E-0000031.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_A-0000001.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_A-0000003.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_A-0000005.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_A-0000007.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_A-0000009.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_A-0000011.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_A-0000013.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_A-0000015.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_A-0000017.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_A-0000019.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_A-0000021.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_A-0000023.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_A-0000025.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_A-0000027.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_A-0000029.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_A-0000031.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_B-0000001.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_B-0000003.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_B-0000005.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_B-0000007.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_B-0000009.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_B-0000011.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_B-0000013.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_B-0000015.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_B-0000017.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_B-0000019.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_B-0000021.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_B-0000023.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_B-0000025.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_B-0000027.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_B-0000029.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_B-0000031.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_C-0000001.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_C-0000003.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_C-0000005.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_C-0000007.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_C-0000009.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_C-0000011.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_C-0000013.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_C-0000015.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_C-0000017.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_C-0000019.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_C-0000021.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_C-0000023.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_C-0000025.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_C-0000027.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_C-0000029.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_C-0000031.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_D-0000001.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_D-0000003.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_D-0000005.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_D-0000007.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_D-0000009.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_D-0000011.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_D-0000013.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_D-0000015.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_D-0000017.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_D-0000019.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_D-0000021.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_D-0000023.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_D-0000025.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_D-0000027.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_D-0000029.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_D-0000031.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_E-0000001.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_E-0000003.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_E-0000005.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_E-0000007.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_E-0000009.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_E-0000011.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_E-0000013.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_E-0000015.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_E-0000017.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_E-0000019.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_E-0000021.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_E-0000023.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_E-0000025.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_E-0000027.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_E-0000029.jpg  \n",
      "  inflating: ./test_imgs/713-3-5-35-Z94_E-0000031.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_A-0000001.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_A-0000003.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_A-0000005.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_A-0000007.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_A-0000009.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_A-0000011.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_A-0000013.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_A-0000015.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_A-0000017.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_A-0000019.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_A-0000021.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_A-0000023.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_A-0000025.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_A-0000027.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_A-0000029.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_A-0000031.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_B-0000001.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_B-0000003.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_B-0000005.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_B-0000007.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_B-0000009.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_B-0000011.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_B-0000013.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_B-0000015.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_B-0000017.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_B-0000019.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_B-0000021.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_B-0000023.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_B-0000025.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_B-0000027.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_B-0000029.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_B-0000031.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_C-0000001.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_C-0000003.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_C-0000005.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_C-0000007.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_C-0000009.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_C-0000011.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_C-0000013.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_C-0000015.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_C-0000017.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_C-0000019.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_C-0000021.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_C-0000023.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_C-0000025.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_C-0000027.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_C-0000029.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_C-0000031.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_D-0000001.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_D-0000003.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_D-0000005.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_D-0000007.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_D-0000009.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_D-0000011.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_D-0000013.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_D-0000015.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_D-0000017.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_D-0000019.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_D-0000021.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_D-0000023.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_D-0000025.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_D-0000027.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_D-0000029.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_D-0000031.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_E-0000001.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_E-0000003.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_E-0000005.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_E-0000007.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_E-0000009.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_E-0000011.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_E-0000013.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_E-0000015.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_E-0000017.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_E-0000019.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_E-0000021.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_E-0000023.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_E-0000025.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_E-0000027.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_E-0000029.jpg  \n",
      "  inflating: ./test_imgs/714-3-5-35-Z94_E-0000031.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_A-0000001.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_A-0000003.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_A-0000005.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_A-0000007.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_A-0000009.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_A-0000011.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_A-0000013.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_A-0000015.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_A-0000017.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_A-0000019.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_A-0000021.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_A-0000023.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_A-0000025.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_A-0000027.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_A-0000029.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_A-0000031.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_B-0000001.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_B-0000003.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_B-0000005.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_B-0000007.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_B-0000009.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_B-0000011.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_B-0000013.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_B-0000015.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_B-0000017.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_B-0000019.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_B-0000021.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_B-0000023.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_B-0000025.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_B-0000027.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_B-0000029.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_B-0000031.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_C-0000001.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_C-0000003.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_C-0000005.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_C-0000007.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_C-0000009.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_C-0000011.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_C-0000013.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_C-0000015.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_C-0000017.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_C-0000019.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_C-0000021.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_C-0000023.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_C-0000025.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_C-0000027.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_C-0000029.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_C-0000031.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_D-0000001.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_D-0000003.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_D-0000005.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_D-0000007.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_D-0000009.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_D-0000011.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_D-0000013.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_D-0000015.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_D-0000017.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_D-0000019.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_D-0000021.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_D-0000023.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_D-0000025.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_D-0000027.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_D-0000029.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_D-0000031.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_E-0000001.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_E-0000003.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_E-0000005.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_E-0000007.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_E-0000009.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_E-0000011.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_E-0000013.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_E-0000015.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_E-0000017.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_E-0000019.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_E-0000021.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_E-0000023.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_E-0000025.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_E-0000027.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_E-0000029.jpg  \n",
      "  inflating: ./test_imgs/729-3-5-36-Z94_E-0000031.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_A-0000001.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_A-0000003.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_A-0000005.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_A-0000007.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_A-0000009.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_A-0000011.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_A-0000013.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_A-0000015.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_A-0000017.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_A-0000019.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_A-0000021.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_A-0000023.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_A-0000025.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_A-0000027.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_A-0000029.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_A-0000031.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_B-0000001.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_B-0000003.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_B-0000005.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_B-0000007.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_B-0000009.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_B-0000011.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_B-0000013.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_B-0000015.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_B-0000017.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_B-0000019.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_B-0000021.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_B-0000023.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_B-0000025.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_B-0000027.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_B-0000029.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_B-0000031.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_C-0000001.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_C-0000003.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_C-0000005.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_C-0000007.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_C-0000009.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_C-0000011.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_C-0000013.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_C-0000015.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_C-0000017.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_C-0000019.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_C-0000021.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_C-0000023.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_C-0000025.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_C-0000027.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_C-0000029.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_C-0000031.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_D-0000001.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_D-0000003.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_D-0000005.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_D-0000007.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_D-0000009.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_D-0000011.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_D-0000013.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_D-0000015.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_D-0000017.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_D-0000019.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_D-0000021.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_D-0000023.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_D-0000025.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_D-0000027.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_D-0000029.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_D-0000031.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_E-0000001.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_E-0000003.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_E-0000005.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_E-0000007.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_E-0000009.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_E-0000011.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_E-0000013.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_E-0000015.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_E-0000017.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_E-0000019.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_E-0000021.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_E-0000023.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_E-0000025.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_E-0000027.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_E-0000029.jpg  \n",
      "  inflating: ./test_imgs/730-3-5-36-Z94_E-0000031.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_A-0000001.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_A-0000003.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_A-0000005.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_A-0000007.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_A-0000009.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_A-0000011.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_A-0000013.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_A-0000015.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_A-0000017.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_A-0000019.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_A-0000021.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_A-0000023.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_A-0000025.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_A-0000027.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_A-0000029.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_A-0000031.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_B-0000001.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_B-0000003.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_B-0000005.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_B-0000007.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_B-0000009.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_B-0000011.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_B-0000013.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_B-0000015.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_B-0000017.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_B-0000019.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_B-0000021.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_B-0000023.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_B-0000025.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_B-0000027.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_B-0000029.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_B-0000031.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_C-0000001.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_C-0000003.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_C-0000005.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_C-0000007.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_C-0000009.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_C-0000011.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_C-0000013.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_C-0000015.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_C-0000017.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_C-0000019.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_C-0000021.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_C-0000023.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_C-0000025.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_C-0000027.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_C-0000029.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_C-0000031.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_D-0000001.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_D-0000003.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_D-0000005.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_D-0000007.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_D-0000009.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_D-0000011.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_D-0000013.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_D-0000015.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_D-0000017.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_D-0000019.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_D-0000021.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_D-0000023.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_D-0000025.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_D-0000027.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_D-0000029.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_D-0000031.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_E-0000001.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_E-0000003.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_E-0000005.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_E-0000007.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_E-0000009.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_E-0000011.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_E-0000013.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_E-0000015.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_E-0000017.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_E-0000019.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_E-0000021.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_E-0000023.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_E-0000025.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_E-0000027.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_E-0000029.jpg  \n",
      "  inflating: ./test_imgs/737-3-5-37-Z94_E-0000031.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_A-0000001.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_A-0000003.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_A-0000005.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_A-0000007.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_A-0000009.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_A-0000011.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_A-0000013.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_A-0000015.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_A-0000017.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_A-0000019.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_A-0000021.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_A-0000023.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_A-0000025.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_A-0000027.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_A-0000029.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_A-0000031.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_B-0000001.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_B-0000003.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_B-0000005.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_B-0000007.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_B-0000009.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_B-0000011.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_B-0000013.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_B-0000015.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_B-0000017.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_B-0000019.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_B-0000021.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_B-0000023.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_B-0000025.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_B-0000027.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_B-0000029.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_B-0000031.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_C-0000001.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_C-0000003.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_C-0000005.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_C-0000007.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_C-0000009.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_C-0000011.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_C-0000013.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_C-0000015.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_C-0000017.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_C-0000019.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_C-0000021.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_C-0000023.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_C-0000025.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_C-0000027.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_C-0000029.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_C-0000031.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_D-0000001.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_D-0000003.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_D-0000005.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_D-0000007.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_D-0000009.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_D-0000011.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_D-0000013.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_D-0000015.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_D-0000017.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_D-0000019.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_D-0000021.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_D-0000023.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_D-0000025.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_D-0000027.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_D-0000029.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_D-0000031.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_E-0000001.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_E-0000003.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_E-0000005.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_E-0000007.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_E-0000009.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_E-0000011.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_E-0000013.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_E-0000015.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_E-0000017.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_E-0000019.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_E-0000021.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_E-0000023.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_E-0000025.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_E-0000027.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_E-0000029.jpg  \n",
      "  inflating: ./test_imgs/738-3-5-37-Z94_E-0000031.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_A-0000001.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_A-0000003.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_A-0000005.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_A-0000007.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_A-0000009.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_A-0000011.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_A-0000013.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_A-0000015.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_A-0000017.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_A-0000019.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_A-0000021.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_A-0000023.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_A-0000025.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_A-0000027.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_A-0000029.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_A-0000031.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_B-0000001.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_B-0000003.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_B-0000005.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_B-0000007.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_B-0000009.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_B-0000011.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_B-0000013.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_B-0000015.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_B-0000017.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_B-0000019.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_B-0000021.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_B-0000023.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_B-0000025.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_B-0000027.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_B-0000029.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_B-0000031.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_C-0000001.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_C-0000003.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_C-0000005.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_C-0000007.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_C-0000009.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_C-0000011.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_C-0000013.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_C-0000015.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_C-0000017.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_C-0000019.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_C-0000021.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_C-0000023.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_C-0000025.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_C-0000027.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_C-0000029.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_C-0000031.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_D-0000001.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_D-0000003.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_D-0000005.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_D-0000007.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_D-0000009.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_D-0000011.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_D-0000013.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_D-0000015.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_D-0000017.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_D-0000019.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_D-0000021.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_D-0000023.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_D-0000025.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_D-0000027.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_D-0000029.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_D-0000031.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_E-0000001.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_E-0000003.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_E-0000005.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_E-0000007.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_E-0000009.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_E-0000011.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_E-0000013.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_E-0000015.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_E-0000017.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_E-0000019.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_E-0000021.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_E-0000023.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_E-0000025.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_E-0000027.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_E-0000029.jpg  \n",
      "  inflating: ./test_imgs/753-3-5-38-Z94_E-0000031.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_A-0000001.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_A-0000003.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_A-0000005.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_A-0000007.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_A-0000009.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_A-0000011.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_A-0000013.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_A-0000015.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_A-0000017.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_A-0000019.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_A-0000021.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_A-0000023.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_A-0000025.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_A-0000027.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_A-0000029.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_A-0000031.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_B-0000001.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_B-0000003.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_B-0000005.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_B-0000007.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_B-0000009.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_B-0000011.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_B-0000013.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_B-0000015.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_B-0000017.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_B-0000019.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_B-0000021.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_B-0000023.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_B-0000025.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_B-0000027.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_B-0000029.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_B-0000031.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_C-0000001.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_C-0000003.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_C-0000005.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_C-0000007.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_C-0000009.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_C-0000011.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_C-0000013.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_C-0000015.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_C-0000017.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_C-0000019.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_C-0000021.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_C-0000023.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_C-0000025.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_C-0000027.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_C-0000029.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_C-0000031.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_D-0000001.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_D-0000003.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_D-0000005.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_D-0000007.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_D-0000009.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_D-0000011.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_D-0000013.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_D-0000015.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_D-0000017.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_D-0000019.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_D-0000021.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_D-0000023.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_D-0000025.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_D-0000027.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_D-0000029.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_D-0000031.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_E-0000001.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_E-0000003.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_E-0000005.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_E-0000007.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_E-0000009.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_E-0000011.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_E-0000013.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_E-0000015.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_E-0000017.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_E-0000019.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_E-0000021.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_E-0000023.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_E-0000025.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_E-0000027.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_E-0000029.jpg  \n",
      "  inflating: ./test_imgs/754-3-5-38-Z94_E-0000031.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_A-0000001.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_A-0000003.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_A-0000005.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_A-0000007.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_A-0000009.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_A-0000011.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_A-0000013.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_A-0000015.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_A-0000017.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_A-0000019.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_A-0000021.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_A-0000023.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_A-0000025.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_A-0000027.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_A-0000029.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_A-0000031.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_B-0000001.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_B-0000003.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_B-0000005.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_B-0000007.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_B-0000009.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_B-0000011.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_B-0000013.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_B-0000015.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_B-0000017.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_B-0000019.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_B-0000021.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_B-0000023.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_B-0000025.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_B-0000027.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_B-0000029.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_B-0000031.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_C-0000001.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_C-0000003.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_C-0000005.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_C-0000007.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_C-0000009.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_C-0000011.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_C-0000013.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_C-0000015.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_C-0000017.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_C-0000019.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_C-0000021.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_C-0000023.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_C-0000025.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_C-0000027.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_C-0000029.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_C-0000031.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_D-0000001.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_D-0000003.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_D-0000005.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_D-0000007.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_D-0000009.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_D-0000011.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_D-0000013.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_D-0000015.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_D-0000017.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_D-0000019.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_D-0000021.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_D-0000023.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_D-0000025.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_D-0000027.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_D-0000029.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_D-0000031.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_E-0000001.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_E-0000003.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_E-0000005.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_E-0000007.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_E-0000009.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_E-0000011.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_E-0000013.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_E-0000015.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_E-0000017.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_E-0000019.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_E-0000021.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_E-0000023.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_E-0000025.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_E-0000027.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_E-0000029.jpg  \n",
      "  inflating: ./test_imgs/761-3-5-39-Z94_E-0000031.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_A-0000001.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_A-0000003.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_A-0000005.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_A-0000007.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_A-0000009.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_A-0000011.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_A-0000013.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_A-0000015.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_A-0000017.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_A-0000019.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_A-0000021.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_A-0000023.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_A-0000025.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_A-0000027.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_A-0000029.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_A-0000031.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_B-0000001.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_B-0000003.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_B-0000005.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_B-0000007.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_B-0000009.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_B-0000011.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_B-0000013.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_B-0000015.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_B-0000017.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_B-0000019.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_B-0000021.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_B-0000023.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_B-0000025.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_B-0000027.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_B-0000029.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_B-0000031.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_C-0000001.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_C-0000003.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_C-0000005.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_C-0000007.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_C-0000009.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_C-0000011.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_C-0000013.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_C-0000015.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_C-0000017.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_C-0000019.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_C-0000021.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_C-0000023.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_C-0000025.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_C-0000027.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_C-0000029.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_C-0000031.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_D-0000001.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_D-0000003.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_D-0000005.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_D-0000007.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_D-0000009.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_D-0000011.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_D-0000013.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_D-0000015.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_D-0000017.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_D-0000019.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_D-0000021.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_D-0000023.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_D-0000025.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_D-0000027.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_D-0000029.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_D-0000031.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_E-0000001.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_E-0000003.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_E-0000005.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_E-0000007.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_E-0000009.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_E-0000011.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_E-0000013.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_E-0000015.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_E-0000017.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_E-0000019.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_E-0000021.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_E-0000023.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_E-0000025.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_E-0000027.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_E-0000029.jpg  \n",
      "  inflating: ./test_imgs/762-3-5-39-Z94_E-0000031.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_A-0000001.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_A-0000003.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_A-0000005.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_A-0000007.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_A-0000009.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_A-0000011.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_A-0000013.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_A-0000015.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_A-0000017.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_A-0000019.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_A-0000021.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_A-0000023.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_A-0000025.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_A-0000027.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_A-0000029.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_A-0000031.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_B-0000001.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_B-0000003.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_B-0000005.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_B-0000007.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_B-0000009.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_B-0000011.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_B-0000013.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_B-0000015.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_B-0000017.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_B-0000019.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_B-0000021.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_B-0000023.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_B-0000025.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_B-0000027.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_B-0000029.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_B-0000031.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_C-0000001.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_C-0000003.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_C-0000005.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_C-0000007.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_C-0000009.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_C-0000011.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_C-0000013.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_C-0000015.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_C-0000017.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_C-0000019.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_C-0000021.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_C-0000023.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_C-0000025.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_C-0000027.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_C-0000029.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_C-0000031.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_D-0000001.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_D-0000003.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_D-0000005.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_D-0000007.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_D-0000009.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_D-0000011.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_D-0000013.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_D-0000015.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_D-0000017.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_D-0000019.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_D-0000021.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_D-0000023.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_D-0000025.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_D-0000027.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_D-0000029.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_D-0000031.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_E-0000001.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_E-0000003.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_E-0000005.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_E-0000007.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_E-0000009.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_E-0000011.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_E-0000013.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_E-0000015.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_E-0000017.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_E-0000019.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_E-0000021.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_E-0000023.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_E-0000025.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_E-0000027.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_E-0000029.jpg  \n",
      "  inflating: ./test_imgs/769-3-5-40-Z94_E-0000031.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_A-0000001.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_A-0000003.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_A-0000005.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_A-0000007.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_A-0000009.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_A-0000011.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_A-0000013.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_A-0000015.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_A-0000017.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_A-0000019.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_A-0000021.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_A-0000023.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_A-0000025.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_A-0000027.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_A-0000029.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_A-0000031.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_B-0000001.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_B-0000003.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_B-0000005.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_B-0000007.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_B-0000009.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_B-0000011.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_B-0000013.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_B-0000015.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_B-0000017.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_B-0000019.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_B-0000021.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_B-0000023.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_B-0000025.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_B-0000027.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_B-0000029.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_B-0000031.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_C-0000001.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_C-0000003.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_C-0000005.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_C-0000007.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_C-0000009.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_C-0000011.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_C-0000013.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_C-0000015.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_C-0000017.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_C-0000019.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_C-0000021.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_C-0000023.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_C-0000025.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_C-0000027.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_C-0000029.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_C-0000031.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_D-0000001.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_D-0000003.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_D-0000005.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_D-0000007.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_D-0000009.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_D-0000011.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_D-0000013.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_D-0000015.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_D-0000017.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_D-0000019.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_D-0000021.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_D-0000023.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_D-0000025.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_D-0000027.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_D-0000029.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_D-0000031.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_E-0000001.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_E-0000003.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_E-0000005.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_E-0000007.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_E-0000009.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_E-0000011.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_E-0000013.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_E-0000015.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_E-0000017.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_E-0000019.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_E-0000021.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_E-0000023.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_E-0000025.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_E-0000027.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_E-0000029.jpg  \n",
      "  inflating: ./test_imgs/770-3-5-40-Z94_E-0000031.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_A-0000001.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_A-0000003.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_A-0000005.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_A-0000007.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_A-0000009.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_A-0000011.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_A-0000013.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_A-0000015.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_A-0000017.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_A-0000019.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_A-0000021.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_A-0000023.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_A-0000025.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_A-0000027.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_A-0000029.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_A-0000031.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_B-0000001.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_B-0000003.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_B-0000005.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_B-0000007.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_B-0000009.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_B-0000011.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_B-0000013.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_B-0000015.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_B-0000017.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_B-0000019.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_B-0000021.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_B-0000023.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_B-0000025.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_B-0000027.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_B-0000029.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_B-0000031.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_C-0000001.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_C-0000003.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_C-0000005.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_C-0000007.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_C-0000009.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_C-0000011.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_C-0000013.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_C-0000015.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_C-0000017.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_C-0000019.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_C-0000021.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_C-0000023.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_C-0000025.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_C-0000027.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_C-0000029.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_C-0000031.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_D-0000001.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_D-0000003.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_D-0000005.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_D-0000007.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_D-0000009.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_D-0000011.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_D-0000013.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_D-0000015.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_D-0000017.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_D-0000019.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_D-0000021.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_D-0000023.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_D-0000025.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_D-0000027.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_D-0000029.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_D-0000031.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_E-0000001.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_E-0000003.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_E-0000005.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_E-0000007.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_E-0000009.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_E-0000011.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_E-0000013.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_E-0000015.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_E-0000017.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_E-0000019.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_E-0000021.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_E-0000023.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_E-0000025.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_E-0000027.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_E-0000029.jpg  \n",
      "  inflating: ./test_imgs/785-3-5-41-Z94_E-0000031.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_A-0000001.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_A-0000003.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_A-0000005.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_A-0000007.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_A-0000009.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_A-0000011.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_A-0000013.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_A-0000015.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_A-0000017.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_A-0000019.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_A-0000021.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_A-0000023.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_A-0000025.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_A-0000027.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_A-0000029.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_A-0000031.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_B-0000001.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_B-0000003.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_B-0000005.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_B-0000007.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_B-0000009.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_B-0000011.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_B-0000013.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_B-0000015.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_B-0000017.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_B-0000019.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_B-0000021.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_B-0000023.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_B-0000025.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_B-0000027.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_B-0000029.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_B-0000031.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_C-0000001.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_C-0000003.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_C-0000005.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_C-0000007.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_C-0000009.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_C-0000011.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_C-0000013.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_C-0000015.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_C-0000017.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_C-0000019.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_C-0000021.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_C-0000023.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_C-0000025.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_C-0000027.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_C-0000029.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_C-0000031.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_D-0000001.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_D-0000003.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_D-0000005.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_D-0000007.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_D-0000009.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_D-0000011.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_D-0000013.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_D-0000015.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_D-0000017.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_D-0000019.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_D-0000021.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_D-0000023.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_D-0000025.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_D-0000027.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_D-0000029.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_D-0000031.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_E-0000001.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_E-0000003.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_E-0000005.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_E-0000007.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_E-0000009.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_E-0000011.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_E-0000013.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_E-0000015.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_E-0000017.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_E-0000019.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_E-0000021.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_E-0000023.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_E-0000025.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_E-0000027.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_E-0000029.jpg  \n",
      "  inflating: ./test_imgs/786-3-5-41-Z94_E-0000031.jpg  \n"
     ]
    }
   ],
   "source": [
    "from google.colab import output\n",
    "\n",
    "!mkdir \"./train_imgs\"\n",
    "!unzip \"/data/train_imgs.zip\" -d \"./train_imgs\"\n",
    "\n",
    "!mkdir \"./test_imgs\"\n",
    "!unzip \"/data/test_imgs.zip\" -d \"./test_imgs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 26092,
     "status": "ok",
     "timestamp": 1616808822909,
     "user": {
      "displayName": "김정환",
      "photoUrl": "",
      "userId": "11315038694579600734"
     },
     "user_tz": -540
    },
    "id": "TmgWvgIyBMSS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "import warnings    \n",
    "import matplotlib.pyplot as plt       \n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "warnings.filterwarnings('ignore')       \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 57902,
     "status": "ok",
     "timestamp": 1616808854892,
     "user": {
      "displayName": "김정환",
      "photoUrl": "",
      "userId": "11315038694579600734"
     },
     "user_tz": -540
    },
    "id": "CC57GLB0SWfc"
   },
   "outputs": [],
   "source": [
    "train_y = pd.read_csv(\"./data/train_augumentation.csv\")\n",
    "valid_y = pd.read_csv(\"./data/valid.csv\")\n",
    "precomputed_train = pd.read_csv(\"./data/precomputed_train.csv\")\n",
    "precomputed_valid = pd.read_csv(\"./data/precomputed_valid.csv\")\n",
    "columns = train_y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59054,
     "status": "ok",
     "timestamp": 1616808856410,
     "user": {
      "displayName": "김정환",
      "photoUrl": "",
      "userId": "11315038694579600734"
     },
     "user_tz": -540
    },
    "id": "QnbLvPlHdnGJ",
    "outputId": "ccd833c4-cb55-4f6d-ad22-d0efdf231fbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3776 validated image filenames.\n",
      "Found 419 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "datagen=ImageDataGenerator(rescale = 1/255) \n",
    "test_datagen=ImageDataGenerator(rescale = 1/255) \n",
    "\n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "    dataframe=train_y[:3776],\n",
    "        directory='train_imgs/',\n",
    "            x_col=\"image\",\n",
    "                y_col=columns[1:],\n",
    "                    batch_size=1,\n",
    "                        shuffle=False,\n",
    "                            class_mode=\"raw\",\n",
    "                                target_size=(1080,1920))\n",
    "valid_generator=test_datagen.flow_from_dataframe(\n",
    "    dataframe=valid_y[:],\n",
    "        directory='train_imgs/',\n",
    "            x_col=\"image\",\n",
    "                y_col=columns[1:],\n",
    "                    batch_size=1,\n",
    "                        shuffle=False,\n",
    "                            class_mode=\"raw\",\n",
    "                                target_size=(1080,1920))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 58887,
     "status": "ok",
     "timestamp": 1616808856412,
     "user": {
      "displayName": "김정환",
      "photoUrl": "",
      "userId": "11315038694579600734"
     },
     "user_tz": -540
    },
    "id": "0agLfZxhT-xr"
   },
   "outputs": [],
   "source": [
    "train_y = train_y.values[:,1:].astype(float)\n",
    "valid_y = valid_y.values[:,1:].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59457,
     "status": "ok",
     "timestamp": 1616808857182,
     "user": {
      "displayName": "김정환",
      "photoUrl": "",
      "userId": "11315038694579600734"
     },
     "user_tz": -540
    },
    "id": "4oK3PnWPSpLX",
    "outputId": "8da03b4a-7e90-45d4-a6c9-2401a2d32f4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32494, 2048), (419, 2048), (32494, 48), (419, 48))"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precomputed_train = precomputed_train.values[:,1:].astype(float)\n",
    "precomputed_valid = precomputed_valid.values[:,1:].astype(float)\n",
    "precomputed_train.shape, precomputed_valid.shape , train_y.shape, valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 59256,
     "status": "ok",
     "timestamp": 1616808857183,
     "user": {
      "displayName": "김정환",
      "photoUrl": "",
      "userId": "11315038694579600734"
     },
     "user_tz": -540
    },
    "id": "xtOWBNk0BMSc"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets.cifar10 import load_data                           \n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70565,
     "status": "ok",
     "timestamp": 1616808868699,
     "user": {
      "displayName": "김정환",
      "photoUrl": "",
      "userId": "11315038694579600734"
     },
     "user_tz": -540
    },
    "id": "omejcj4lBMSc",
    "outputId": "1d969953-6f95-4237-fa0b-d2cd4f502906"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "234553344/234545216 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "my_rnet = tf.keras.applications.ResNet152V2(\n",
    "    include_top=False, weights='imagenet',input_shape=(1080,1920,3), pooling = \"avg\")\n",
    "my_rnet.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70569,
     "status": "ok",
     "timestamp": 1616808869168,
     "user": {
      "displayName": "김정환",
      "photoUrl": "",
      "userId": "11315038694579600734"
     },
     "user_tz": -540
    },
    "id": "Oern6rSuBMSc",
    "outputId": "b5aac920-f399-4055-bffb-cb187230c42e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet152v2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1080, 1920,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 1086, 1926, 3 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 540, 960, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 542, 962, 64) 0           conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 270, 480, 64) 0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_preact_bn (BatchNo (None, 270, 480, 64) 256         pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_preact_relu (Activ (None, 270, 480, 64) 0           conv2_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 270, 480, 64) 4096        conv2_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 270, 480, 64) 256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 270, 480, 64) 0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_pad (ZeroPadding (None, 272, 482, 64) 0           conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 270, 480, 64) 36864       conv2_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 270, 480, 64) 256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 270, 480, 64) 0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 270, 480, 256 16640       conv2_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 270, 480, 256 16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Add)          (None, 270, 480, 256 0           conv2_block1_0_conv[0][0]        \n",
      "                                                                 conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_preact_bn (BatchNo (None, 270, 480, 256 1024        conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_preact_relu (Activ (None, 270, 480, 256 0           conv2_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 270, 480, 64) 16384       conv2_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 270, 480, 64) 256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 270, 480, 64) 0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_pad (ZeroPadding (None, 272, 482, 64) 0           conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 270, 480, 64) 36864       conv2_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 270, 480, 64) 256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 270, 480, 64) 0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 270, 480, 256 16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Add)          (None, 270, 480, 256 0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_preact_bn (BatchNo (None, 270, 480, 256 1024        conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_preact_relu (Activ (None, 270, 480, 256 0           conv2_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 270, 480, 64) 16384       conv2_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 270, 480, 64) 256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 270, 480, 64) 0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_pad (ZeroPadding (None, 272, 482, 64) 0           conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 135, 240, 64) 36864       conv2_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 135, 240, 64) 256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 135, 240, 64) 0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 135, 240, 256 0           conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 135, 240, 256 16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Add)          (None, 135, 240, 256 0           max_pooling2d[0][0]              \n",
      "                                                                 conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_preact_bn (BatchNo (None, 135, 240, 256 1024        conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_preact_relu (Activ (None, 135, 240, 256 0           conv3_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 135, 240, 128 32768       conv3_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 135, 240, 128 512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 135, 240, 128 0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_pad (ZeroPadding (None, 137, 242, 128 0           conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 135, 240, 128 147456      conv3_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 135, 240, 128 512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 135, 240, 128 0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 135, 240, 512 131584      conv3_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 135, 240, 512 66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Add)          (None, 135, 240, 512 0           conv3_block1_0_conv[0][0]        \n",
      "                                                                 conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_preact_bn (BatchNo (None, 135, 240, 512 2048        conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_preact_relu (Activ (None, 135, 240, 512 0           conv3_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 135, 240, 128 65536       conv3_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 135, 240, 128 512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 135, 240, 128 0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_pad (ZeroPadding (None, 137, 242, 128 0           conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 135, 240, 128 147456      conv3_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 135, 240, 128 512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 135, 240, 128 0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 135, 240, 512 66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Add)          (None, 135, 240, 512 0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_preact_bn (BatchNo (None, 135, 240, 512 2048        conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_preact_relu (Activ (None, 135, 240, 512 0           conv3_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 135, 240, 128 65536       conv3_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 135, 240, 128 512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 135, 240, 128 0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_pad (ZeroPadding (None, 137, 242, 128 0           conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 135, 240, 128 147456      conv3_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 135, 240, 128 512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 135, 240, 128 0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 135, 240, 512 66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Add)          (None, 135, 240, 512 0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_preact_bn (BatchNo (None, 135, 240, 512 2048        conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_preact_relu (Activ (None, 135, 240, 512 0           conv3_block4_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 135, 240, 128 65536       conv3_block4_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 135, 240, 128 512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 135, 240, 128 0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_pad (ZeroPadding (None, 137, 242, 128 0           conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 135, 240, 128 147456      conv3_block4_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 135, 240, 128 512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 135, 240, 128 0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 135, 240, 512 66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Add)          (None, 135, 240, 512 0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_preact_bn (BatchNo (None, 135, 240, 512 2048        conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_preact_relu (Activ (None, 135, 240, 512 0           conv3_block5_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)    (None, 135, 240, 128 65536       conv3_block5_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormali (None, 135, 240, 128 512         conv3_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation (None, 135, 240, 128 0           conv3_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_pad (ZeroPadding (None, 137, 242, 128 0           conv3_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)    (None, 135, 240, 128 147456      conv3_block5_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_bn (BatchNormali (None, 135, 240, 128 512         conv3_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_relu (Activation (None, 135, 240, 128 0           conv3_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_3_conv (Conv2D)    (None, 135, 240, 512 66048       conv3_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_out (Add)          (None, 135, 240, 512 0           conv3_block4_out[0][0]           \n",
      "                                                                 conv3_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_preact_bn (BatchNo (None, 135, 240, 512 2048        conv3_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_preact_relu (Activ (None, 135, 240, 512 0           conv3_block6_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)    (None, 135, 240, 128 65536       conv3_block6_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormali (None, 135, 240, 128 512         conv3_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation (None, 135, 240, 128 0           conv3_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_pad (ZeroPadding (None, 137, 242, 128 0           conv3_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)    (None, 135, 240, 128 147456      conv3_block6_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_bn (BatchNormali (None, 135, 240, 128 512         conv3_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_relu (Activation (None, 135, 240, 128 0           conv3_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_3_conv (Conv2D)    (None, 135, 240, 512 66048       conv3_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_out (Add)          (None, 135, 240, 512 0           conv3_block5_out[0][0]           \n",
      "                                                                 conv3_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_preact_bn (BatchNo (None, 135, 240, 512 2048        conv3_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_preact_relu (Activ (None, 135, 240, 512 0           conv3_block7_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)    (None, 135, 240, 128 65536       conv3_block7_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormali (None, 135, 240, 128 512         conv3_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation (None, 135, 240, 128 0           conv3_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_pad (ZeroPadding (None, 137, 242, 128 0           conv3_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)    (None, 135, 240, 128 147456      conv3_block7_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_bn (BatchNormali (None, 135, 240, 128 512         conv3_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_relu (Activation (None, 135, 240, 128 0           conv3_block7_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_3_conv (Conv2D)    (None, 135, 240, 512 66048       conv3_block7_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_out (Add)          (None, 135, 240, 512 0           conv3_block6_out[0][0]           \n",
      "                                                                 conv3_block7_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_preact_bn (BatchNo (None, 135, 240, 512 2048        conv3_block7_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_preact_relu (Activ (None, 135, 240, 512 0           conv3_block8_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)    (None, 135, 240, 128 65536       conv3_block8_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormali (None, 135, 240, 128 512         conv3_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation (None, 135, 240, 128 0           conv3_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_pad (ZeroPadding (None, 137, 242, 128 0           conv3_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)    (None, 68, 120, 128) 147456      conv3_block8_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_bn (BatchNormali (None, 68, 120, 128) 512         conv3_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_relu (Activation (None, 68, 120, 128) 0           conv3_block8_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 68, 120, 512) 0           conv3_block7_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_3_conv (Conv2D)    (None, 68, 120, 512) 66048       conv3_block8_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_out (Add)          (None, 68, 120, 512) 0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv3_block8_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_preact_bn (BatchNo (None, 68, 120, 512) 2048        conv3_block8_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_preact_relu (Activ (None, 68, 120, 512) 0           conv4_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 68, 120, 256) 131072      conv4_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 68, 120, 256) 1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 68, 120, 256) 0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_pad (ZeroPadding (None, 70, 122, 256) 0           conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 68, 120, 256) 589824      conv4_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 68, 120, 256) 1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 68, 120, 256) 0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 68, 120, 1024 525312      conv4_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 68, 120, 1024 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Add)          (None, 68, 120, 1024 0           conv4_block1_0_conv[0][0]        \n",
      "                                                                 conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_preact_bn (BatchNo (None, 68, 120, 1024 4096        conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_preact_relu (Activ (None, 68, 120, 1024 0           conv4_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 68, 120, 256) 262144      conv4_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 68, 120, 256) 1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 68, 120, 256) 0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_pad (ZeroPadding (None, 70, 122, 256) 0           conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 68, 120, 256) 589824      conv4_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 68, 120, 256) 1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 68, 120, 256) 0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 68, 120, 1024 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Add)          (None, 68, 120, 1024 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_preact_bn (BatchNo (None, 68, 120, 1024 4096        conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_preact_relu (Activ (None, 68, 120, 1024 0           conv4_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 68, 120, 256) 262144      conv4_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 68, 120, 256) 1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 68, 120, 256) 0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_pad (ZeroPadding (None, 70, 122, 256) 0           conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 68, 120, 256) 589824      conv4_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 68, 120, 256) 1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 68, 120, 256) 0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 68, 120, 1024 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Add)          (None, 68, 120, 1024 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_preact_bn (BatchNo (None, 68, 120, 1024 4096        conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_preact_relu (Activ (None, 68, 120, 1024 0           conv4_block4_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 68, 120, 256) 262144      conv4_block4_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 68, 120, 256) 1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 68, 120, 256) 0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_pad (ZeroPadding (None, 70, 122, 256) 0           conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 68, 120, 256) 589824      conv4_block4_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 68, 120, 256) 1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 68, 120, 256) 0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 68, 120, 1024 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Add)          (None, 68, 120, 1024 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_preact_bn (BatchNo (None, 68, 120, 1024 4096        conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_preact_relu (Activ (None, 68, 120, 1024 0           conv4_block5_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 68, 120, 256) 262144      conv4_block5_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 68, 120, 256) 1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 68, 120, 256) 0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_pad (ZeroPadding (None, 70, 122, 256) 0           conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 68, 120, 256) 589824      conv4_block5_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 68, 120, 256) 1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 68, 120, 256) 0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 68, 120, 1024 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Add)          (None, 68, 120, 1024 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_preact_bn (BatchNo (None, 68, 120, 1024 4096        conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_preact_relu (Activ (None, 68, 120, 1024 0           conv4_block6_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 68, 120, 256) 262144      conv4_block6_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 68, 120, 256) 1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 68, 120, 256) 0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_pad (ZeroPadding (None, 70, 122, 256) 0           conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 68, 120, 256) 589824      conv4_block6_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 68, 120, 256) 1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 68, 120, 256) 0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 68, 120, 1024 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Add)          (None, 68, 120, 1024 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_preact_bn (BatchNo (None, 68, 120, 1024 4096        conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_preact_relu (Activ (None, 68, 120, 1024 0           conv4_block7_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, 68, 120, 256) 262144      conv4_block7_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 68, 120, 256) 1024        conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 68, 120, 256) 0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_pad (ZeroPadding (None, 70, 122, 256) 0           conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, 68, 120, 256) 589824      conv4_block7_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_bn (BatchNormali (None, 68, 120, 256) 1024        conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_relu (Activation (None, 68, 120, 256) 0           conv4_block7_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_3_conv (Conv2D)    (None, 68, 120, 1024 263168      conv4_block7_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_out (Add)          (None, 68, 120, 1024 0           conv4_block6_out[0][0]           \n",
      "                                                                 conv4_block7_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_preact_bn (BatchNo (None, 68, 120, 1024 4096        conv4_block7_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_preact_relu (Activ (None, 68, 120, 1024 0           conv4_block8_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, 68, 120, 256) 262144      conv4_block8_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 68, 120, 256) 1024        conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 68, 120, 256) 0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_pad (ZeroPadding (None, 70, 122, 256) 0           conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, 68, 120, 256) 589824      conv4_block8_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_bn (BatchNormali (None, 68, 120, 256) 1024        conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_relu (Activation (None, 68, 120, 256) 0           conv4_block8_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_3_conv (Conv2D)    (None, 68, 120, 1024 263168      conv4_block8_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_out (Add)          (None, 68, 120, 1024 0           conv4_block7_out[0][0]           \n",
      "                                                                 conv4_block8_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_preact_bn (BatchNo (None, 68, 120, 1024 4096        conv4_block8_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_preact_relu (Activ (None, 68, 120, 1024 0           conv4_block9_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, 68, 120, 256) 262144      conv4_block9_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 68, 120, 256) 1024        conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 68, 120, 256) 0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_pad (ZeroPadding (None, 70, 122, 256) 0           conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, 68, 120, 256) 589824      conv4_block9_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_bn (BatchNormali (None, 68, 120, 256) 1024        conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_relu (Activation (None, 68, 120, 256) 0           conv4_block9_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_3_conv (Conv2D)    (None, 68, 120, 1024 263168      conv4_block9_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_out (Add)          (None, 68, 120, 1024 0           conv4_block8_out[0][0]           \n",
      "                                                                 conv4_block9_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block9_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block10_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block10_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block10_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block10_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block10_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_out (Add)         (None, 68, 120, 1024 0           conv4_block9_out[0][0]           \n",
      "                                                                 conv4_block10_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block10_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block11_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block11_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block11_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block11_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block11_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_out (Add)         (None, 68, 120, 1024 0           conv4_block10_out[0][0]          \n",
      "                                                                 conv4_block11_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block11_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block12_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block12_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block12_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block12_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block12_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_out (Add)         (None, 68, 120, 1024 0           conv4_block11_out[0][0]          \n",
      "                                                                 conv4_block12_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block12_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block13_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block13_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block13_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block13_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block13_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_out (Add)         (None, 68, 120, 1024 0           conv4_block12_out[0][0]          \n",
      "                                                                 conv4_block13_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block13_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block14_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block14_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block14_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block14_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block14_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_out (Add)         (None, 68, 120, 1024 0           conv4_block13_out[0][0]          \n",
      "                                                                 conv4_block14_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block14_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block15_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block15_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block15_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block15_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block15_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_out (Add)         (None, 68, 120, 1024 0           conv4_block14_out[0][0]          \n",
      "                                                                 conv4_block15_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block15_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block16_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block16_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block16_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block16_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block16_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_out (Add)         (None, 68, 120, 1024 0           conv4_block15_out[0][0]          \n",
      "                                                                 conv4_block16_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block16_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block17_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block17_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block17_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block17_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block17_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_out (Add)         (None, 68, 120, 1024 0           conv4_block16_out[0][0]          \n",
      "                                                                 conv4_block17_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block17_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block18_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block18_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block18_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block18_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block18_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_out (Add)         (None, 68, 120, 1024 0           conv4_block17_out[0][0]          \n",
      "                                                                 conv4_block18_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block18_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block19_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block19_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block19_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block19_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block19_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_out (Add)         (None, 68, 120, 1024 0           conv4_block18_out[0][0]          \n",
      "                                                                 conv4_block19_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block19_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block20_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block20_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block20_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block20_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block20_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_out (Add)         (None, 68, 120, 1024 0           conv4_block19_out[0][0]          \n",
      "                                                                 conv4_block20_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block20_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block21_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block21_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block21_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block21_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block21_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_out (Add)         (None, 68, 120, 1024 0           conv4_block20_out[0][0]          \n",
      "                                                                 conv4_block21_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block21_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block22_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block22_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block22_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block22_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block22_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_out (Add)         (None, 68, 120, 1024 0           conv4_block21_out[0][0]          \n",
      "                                                                 conv4_block22_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block22_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block23_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block23_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block23_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block23_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block23_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_out (Add)         (None, 68, 120, 1024 0           conv4_block22_out[0][0]          \n",
      "                                                                 conv4_block23_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block23_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block24_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block24_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block24_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block24_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block24_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block24_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block24_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block24_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block24_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_out (Add)         (None, 68, 120, 1024 0           conv4_block23_out[0][0]          \n",
      "                                                                 conv4_block24_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block24_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block25_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block25_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block25_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block25_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block25_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block25_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block25_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block25_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block25_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_out (Add)         (None, 68, 120, 1024 0           conv4_block24_out[0][0]          \n",
      "                                                                 conv4_block25_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block25_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block26_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block26_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block26_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block26_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block26_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block26_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block26_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block26_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block26_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_out (Add)         (None, 68, 120, 1024 0           conv4_block25_out[0][0]          \n",
      "                                                                 conv4_block26_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block26_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block27_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block27_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block27_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block27_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block27_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block27_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block27_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block27_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block27_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_out (Add)         (None, 68, 120, 1024 0           conv4_block26_out[0][0]          \n",
      "                                                                 conv4_block27_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block27_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block28_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block28_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block28_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block28_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block28_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block28_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block28_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block28_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block28_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_out (Add)         (None, 68, 120, 1024 0           conv4_block27_out[0][0]          \n",
      "                                                                 conv4_block28_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block28_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block29_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block29_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block29_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block29_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block29_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block29_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block29_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block29_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block29_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_out (Add)         (None, 68, 120, 1024 0           conv4_block28_out[0][0]          \n",
      "                                                                 conv4_block29_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block29_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block30_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block30_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block30_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block30_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block30_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block30_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block30_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block30_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block30_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_out (Add)         (None, 68, 120, 1024 0           conv4_block29_out[0][0]          \n",
      "                                                                 conv4_block30_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block30_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block31_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block31_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block31_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block31_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block31_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block31_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block31_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block31_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block31_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_out (Add)         (None, 68, 120, 1024 0           conv4_block30_out[0][0]          \n",
      "                                                                 conv4_block31_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block31_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block32_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block32_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block32_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block32_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block32_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block32_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block32_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block32_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block32_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_out (Add)         (None, 68, 120, 1024 0           conv4_block31_out[0][0]          \n",
      "                                                                 conv4_block32_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block32_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block33_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block33_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block33_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block33_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block33_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block33_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block33_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block33_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block33_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_out (Add)         (None, 68, 120, 1024 0           conv4_block32_out[0][0]          \n",
      "                                                                 conv4_block33_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block33_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block34_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block34_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block34_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block34_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block34_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block34_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block34_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block34_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block34_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_out (Add)         (None, 68, 120, 1024 0           conv4_block33_out[0][0]          \n",
      "                                                                 conv4_block34_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block34_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block35_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block35_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block35_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block35_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block35_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_2_conv (Conv2D)   (None, 68, 120, 256) 589824      conv4_block35_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_2_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block35_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_2_relu (Activatio (None, 68, 120, 256) 0           conv4_block35_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_3_conv (Conv2D)   (None, 68, 120, 1024 263168      conv4_block35_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_out (Add)         (None, 68, 120, 1024 0           conv4_block34_out[0][0]          \n",
      "                                                                 conv4_block35_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_preact_bn (BatchN (None, 68, 120, 1024 4096        conv4_block35_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_preact_relu (Acti (None, 68, 120, 1024 0           conv4_block36_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_1_conv (Conv2D)   (None, 68, 120, 256) 262144      conv4_block36_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_1_bn (BatchNormal (None, 68, 120, 256) 1024        conv4_block36_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_1_relu (Activatio (None, 68, 120, 256) 0           conv4_block36_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_2_pad (ZeroPaddin (None, 70, 122, 256) 0           conv4_block36_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_2_conv (Conv2D)   (None, 34, 60, 256)  589824      conv4_block36_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_2_bn (BatchNormal (None, 34, 60, 256)  1024        conv4_block36_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_2_relu (Activatio (None, 34, 60, 256)  0           conv4_block36_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 34, 60, 1024) 0           conv4_block35_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_3_conv (Conv2D)   (None, 34, 60, 1024) 263168      conv4_block36_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_out (Add)         (None, 34, 60, 1024) 0           max_pooling2d_2[0][0]            \n",
      "                                                                 conv4_block36_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_preact_bn (BatchNo (None, 34, 60, 1024) 4096        conv4_block36_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_preact_relu (Activ (None, 34, 60, 1024) 0           conv5_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 34, 60, 512)  524288      conv5_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 34, 60, 512)  2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 34, 60, 512)  0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_pad (ZeroPadding (None, 36, 62, 512)  0           conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 34, 60, 512)  2359296     conv5_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 34, 60, 512)  2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 34, 60, 512)  0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 34, 60, 2048) 2099200     conv5_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 34, 60, 2048) 1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Add)          (None, 34, 60, 2048) 0           conv5_block1_0_conv[0][0]        \n",
      "                                                                 conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_preact_bn (BatchNo (None, 34, 60, 2048) 8192        conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_preact_relu (Activ (None, 34, 60, 2048) 0           conv5_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 34, 60, 512)  1048576     conv5_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 34, 60, 512)  2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 34, 60, 512)  0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_pad (ZeroPadding (None, 36, 62, 512)  0           conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 34, 60, 512)  2359296     conv5_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 34, 60, 512)  2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 34, 60, 512)  0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 34, 60, 2048) 1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Add)          (None, 34, 60, 2048) 0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_preact_bn (BatchNo (None, 34, 60, 2048) 8192        conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_preact_relu (Activ (None, 34, 60, 2048) 0           conv5_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 34, 60, 512)  1048576     conv5_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 34, 60, 512)  2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 34, 60, 512)  0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_pad (ZeroPadding (None, 36, 62, 512)  0           conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 34, 60, 512)  2359296     conv5_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 34, 60, 512)  2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 34, 60, 512)  0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 34, 60, 2048) 1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Add)          (None, 34, 60, 2048) 0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "post_bn (BatchNormalization)    (None, 34, 60, 2048) 8192        conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "post_relu (Activation)          (None, 34, 60, 2048) 0           post_bn[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           post_relu[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 58,331,648\n",
      "Trainable params: 0\n",
      "Non-trainable params: 58,331,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_rnet.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "my_rnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70373,
     "status": "ok",
     "timestamp": 1616808869169,
     "user": {
      "displayName": "김정환",
      "photoUrl": "",
      "userId": "11315038694579600734"
     },
     "user_tz": -540
    },
    "id": "LoMlkw9yLeOm",
    "outputId": "2c101df7-b98f-406d-e992-f5195ff5b286"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization (BatchNo (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 48)                12336     \n",
      "=================================================================\n",
      "Total params: 4,167,216\n",
      "Trainable params: 4,155,952\n",
      "Non-trainable params: 11,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Se propondra la capa densa\n",
    "topModel = Sequential()\n",
    "topModel.add(Input((2048,)))\n",
    "topModel.add(BatchNormalization())\n",
    "\n",
    "topModel.add(Dense(1024,  activation='relu'))\n",
    "topModel.add(BatchNormalization())\n",
    "topModel.add(Dense(1024,  activation='relu'))\n",
    "topModel.add(BatchNormalization())\n",
    "\n",
    "\n",
    "topModel.add(Dense(512,  activation='relu'))\n",
    "topModel.add(BatchNormalization())\n",
    "topModel.add(Dense(512,  activation='relu'))\n",
    "topModel.add(BatchNormalization())\n",
    "\n",
    "topModel.add(Dense(256,  activation='relu'))\n",
    "topModel.add(BatchNormalization())\n",
    "topModel.add(Dense(256,  activation='relu'))\n",
    "topModel.add(BatchNormalization())\n",
    "\n",
    "topModel.add(Dense(48)) \n",
    "\n",
    "topModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5925673,
     "status": "ok",
     "timestamp": 1616814726262,
     "user": {
      "displayName": "김정환",
      "photoUrl": "",
      "userId": "11315038694579600734"
     },
     "user_tz": -540
    },
    "id": "ZtGsOKuHhGEi",
    "outputId": "237a6dbf-f078-46e0-85b1-52bca893a329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
      "Epoch 8751/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 121.8352 - mae: 8.7312 - val_loss: 2163801893852122430742622451531776.0000 - val_mae: 1764189223780352.0000\n",
      "\n",
      "Epoch 08751: val_mae did not improve from 6.28420\n",
      "Epoch 8752/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 76.7570 - mae: 7.0002 - val_loss: 15609564765001381212567176472428544.0000 - val_mae: 4747333115838464.0000\n",
      "\n",
      "Epoch 08752: val_mae did not improve from 6.28420\n",
      "Epoch 8753/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.1877 - mae: 7.3033 - val_loss: 5519195970888800141582283365679104.0000 - val_mae: 2936734891376640.0000\n",
      "\n",
      "Epoch 08753: val_mae did not improve from 6.28420\n",
      "Epoch 8754/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 80.1814 - mae: 7.0304 - val_loss: 7629827034413730305718479749120.0000 - val_mae: 103966511726592.0000\n",
      "\n",
      "Epoch 08754: val_mae did not improve from 6.28420\n",
      "Epoch 8755/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 82.2688 - mae: 7.2270 - val_loss: 213.0041 - val_mae: 8.0685\n",
      "\n",
      "Epoch 08755: val_mae did not improve from 6.28420\n",
      "Epoch 8756/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 82.2511 - mae: 7.1006 - val_loss: 217.4085 - val_mae: 8.3924\n",
      "\n",
      "Epoch 08756: val_mae did not improve from 6.28420\n",
      "Epoch 8757/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 104.7288 - mae: 8.2055 - val_loss: 253.2458 - val_mae: 9.2450\n",
      "\n",
      "Epoch 08757: val_mae did not improve from 6.28420\n",
      "Epoch 8758/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 95.1472 - mae: 7.7872 - val_loss: 213.6100 - val_mae: 8.0731\n",
      "\n",
      "Epoch 08758: val_mae did not improve from 6.28420\n",
      "Epoch 8759/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 67.9979 - mae: 6.5812 - val_loss: 203.2287 - val_mae: 7.8878\n",
      "\n",
      "Epoch 08759: val_mae did not improve from 6.28420\n",
      "Epoch 8760/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 76.2285 - mae: 7.0195 - val_loss: 240.4003 - val_mae: 9.4429\n",
      "\n",
      "Epoch 08760: val_mae did not improve from 6.28420\n",
      "Epoch 8761/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 123.2815 - mae: 8.8094 - val_loss: 220.7916 - val_mae: 8.6778\n",
      "\n",
      "Epoch 08761: val_mae did not improve from 6.28420\n",
      "Epoch 8762/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 151.3529 - mae: 10.0936 - val_loss: 3046027340895567740928.0000 - val_mae: 2119910144.0000\n",
      "\n",
      "Epoch 08762: val_mae did not improve from 6.28420\n",
      "Epoch 8763/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 97.3754 - mae: 7.6862 - val_loss: 217.9690 - val_mae: 8.5219\n",
      "\n",
      "Epoch 08763: val_mae did not improve from 6.28420\n",
      "Epoch 8764/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 93.4772 - mae: 7.6910 - val_loss: 14126796771572394091347968.0000 - val_mae: 149051244544.0000\n",
      "\n",
      "Epoch 08764: val_mae did not improve from 6.28420\n",
      "Epoch 8765/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 101.2805 - mae: 7.9175 - val_loss: 23648541435054908076195840.0000 - val_mae: 240306356224.0000\n",
      "\n",
      "Epoch 08765: val_mae did not improve from 6.28420\n",
      "Epoch 8766/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.0085 - mae: 7.6924 - val_loss: 3221495574015461034033152.0000 - val_mae: 69371584512.0000\n",
      "\n",
      "Epoch 08766: val_mae did not improve from 6.28420\n",
      "Epoch 8767/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 93.7620 - mae: 7.4251 - val_loss: 231959265884391484048801792.0000 - val_mae: 806969999360.0000\n",
      "\n",
      "Epoch 08767: val_mae did not improve from 6.28420\n",
      "Epoch 8768/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 117.5621 - mae: 8.8138 - val_loss: 229659731661650998763454464.0000 - val_mae: 680109867008.0000\n",
      "\n",
      "Epoch 08768: val_mae did not improve from 6.28420\n",
      "Epoch 8769/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 123.1848 - mae: 8.6947 - val_loss: 1269251247528189688459296768.0000 - val_mae: 1817417482240.0000\n",
      "\n",
      "Epoch 08769: val_mae did not improve from 6.28420\n",
      "Epoch 8770/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 84.4884 - mae: 7.2628 - val_loss: 245.5600 - val_mae: 9.9443\n",
      "\n",
      "Epoch 08770: val_mae did not improve from 6.28420\n",
      "Epoch 8771/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 135.8977 - mae: 9.3584 - val_loss: 248.6447 - val_mae: 9.9743\n",
      "\n",
      "Epoch 08771: val_mae did not improve from 6.28420\n",
      "Epoch 8772/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 110.2099 - mae: 8.4875 - val_loss: 225.6428 - val_mae: 8.8113\n",
      "\n",
      "Epoch 08772: val_mae did not improve from 6.28420\n",
      "Epoch 8773/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 103.5672 - mae: 8.0140 - val_loss: 247.7705 - val_mae: 8.9507\n",
      "\n",
      "Epoch 08773: val_mae did not improve from 6.28420\n",
      "Epoch 8774/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 98.4475 - mae: 7.9026 - val_loss: 232.8245 - val_mae: 8.7705\n",
      "\n",
      "Epoch 08774: val_mae did not improve from 6.28420\n",
      "Epoch 8775/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 114.9845 - mae: 8.4419 - val_loss: 333.2160 - val_mae: 13.0640\n",
      "\n",
      "Epoch 08775: val_mae did not improve from 6.28420\n",
      "Epoch 8776/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 147.5392 - mae: 9.7500 - val_loss: 235.7335 - val_mae: 9.0576\n",
      "\n",
      "Epoch 08776: val_mae did not improve from 6.28420\n",
      "Epoch 8777/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 118.6147 - mae: 8.4744 - val_loss: 270.8900 - val_mae: 10.7015\n",
      "\n",
      "Epoch 08777: val_mae did not improve from 6.28420\n",
      "Epoch 8778/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 108.8505 - mae: 8.2516 - val_loss: 225.5052 - val_mae: 8.8650\n",
      "\n",
      "Epoch 08778: val_mae did not improve from 6.28420\n",
      "Epoch 8779/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 87.7465 - mae: 7.4117 - val_loss: 223.0482 - val_mae: 8.7867\n",
      "\n",
      "Epoch 08779: val_mae did not improve from 6.28420\n",
      "Epoch 8780/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 93.7886 - mae: 7.7458 - val_loss: 214.9089 - val_mae: 8.3807\n",
      "\n",
      "Epoch 08780: val_mae did not improve from 6.28420\n",
      "Epoch 8781/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 86.0023 - mae: 7.3952 - val_loss: 199.4795 - val_mae: 7.7568\n",
      "\n",
      "Epoch 08781: val_mae did not improve from 6.28420\n",
      "Epoch 8782/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 102.8288 - mae: 8.1435 - val_loss: 250.2378 - val_mae: 9.9901\n",
      "\n",
      "Epoch 08782: val_mae did not improve from 6.28420\n",
      "Epoch 8783/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 100.5102 - mae: 7.8776 - val_loss: 205.2853 - val_mae: 7.9709\n",
      "\n",
      "Epoch 08783: val_mae did not improve from 6.28420\n",
      "Epoch 8784/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 94.0810 - mae: 7.6906 - val_loss: 276.6937 - val_mae: 11.2261\n",
      "\n",
      "Epoch 08784: val_mae did not improve from 6.28420\n",
      "Epoch 8785/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 156.9076 - mae: 9.9270 - val_loss: 223.9895 - val_mae: 8.9061\n",
      "\n",
      "Epoch 08785: val_mae did not improve from 6.28420\n",
      "Epoch 8786/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 89.9979 - mae: 7.5210 - val_loss: 220.4240 - val_mae: 8.6128\n",
      "\n",
      "Epoch 08786: val_mae did not improve from 6.28420\n",
      "Epoch 8787/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 89.4555 - mae: 7.4283 - val_loss: 250.5510 - val_mae: 9.9808\n",
      "\n",
      "Epoch 08787: val_mae did not improve from 6.28420\n",
      "Epoch 8788/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 83.9292 - mae: 6.9455 - val_loss: 202.9114 - val_mae: 7.5786\n",
      "\n",
      "Epoch 08788: val_mae did not improve from 6.28420\n",
      "Epoch 8789/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.5641 - mae: 6.5792 - val_loss: 268.7199 - val_mae: 10.7314\n",
      "\n",
      "Epoch 08789: val_mae did not improve from 6.28420\n",
      "Epoch 8790/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 115.8223 - mae: 8.4483 - val_loss: 349.6671 - val_mae: 13.0352\n",
      "\n",
      "Epoch 08790: val_mae did not improve from 6.28420\n",
      "Epoch 8791/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 174.8260 - mae: 10.4377 - val_loss: 246.0806 - val_mae: 9.5480\n",
      "\n",
      "Epoch 08791: val_mae did not improve from 6.28420\n",
      "Epoch 8792/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 106.3380 - mae: 8.1547 - val_loss: 214.9851 - val_mae: 8.2903\n",
      "\n",
      "Epoch 08792: val_mae did not improve from 6.28420\n",
      "Epoch 8793/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 74.7488 - mae: 6.8788 - val_loss: 218.2007 - val_mae: 8.3443\n",
      "\n",
      "Epoch 08793: val_mae did not improve from 6.28420\n",
      "Epoch 8794/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 94.8422 - mae: 7.6484 - val_loss: 232.1178 - val_mae: 9.2041\n",
      "\n",
      "Epoch 08794: val_mae did not improve from 6.28420\n",
      "Epoch 8795/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 93.3571 - mae: 7.5012 - val_loss: 6344880410292640907526144.0000 - val_mae: 105472901120.0000\n",
      "\n",
      "Epoch 08795: val_mae did not improve from 6.28420\n",
      "Epoch 8796/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.3815 - mae: 7.3323 - val_loss: 256.4554 - val_mae: 10.2104\n",
      "\n",
      "Epoch 08796: val_mae did not improve from 6.28420\n",
      "Epoch 8797/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 125.1777 - mae: 8.8080 - val_loss: 236131144630613488823173120.0000 - val_mae: 608730873856.0000\n",
      "\n",
      "Epoch 08797: val_mae did not improve from 6.28420\n",
      "Epoch 8798/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 103.7841 - mae: 8.1329 - val_loss: 22766381239961969898815488.0000 - val_mae: 192723288064.0000\n",
      "\n",
      "Epoch 08798: val_mae did not improve from 6.28420\n",
      "Epoch 8799/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 93.7795 - mae: 7.5288 - val_loss: 5575725545855999746768896.0000 - val_mae: 94883962880.0000\n",
      "\n",
      "Epoch 08799: val_mae did not improve from 6.28420\n",
      "Epoch 8800/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 102.4498 - mae: 7.9345 - val_loss: 68778025807725428083785728.0000 - val_mae: 344551817216.0000\n",
      "\n",
      "Epoch 08800: val_mae did not improve from 6.28420\n",
      "Epoch 8801/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 69.5699 - mae: 6.6413 - val_loss: 216.6112 - val_mae: 8.4175\n",
      "\n",
      "Epoch 08801: val_mae did not improve from 6.28420\n",
      "Epoch 8802/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 80.9424 - mae: 7.2113 - val_loss: 221321397511964659822886912.0000 - val_mae: 594727534592.0000\n",
      "\n",
      "Epoch 08802: val_mae did not improve from 6.28420\n",
      "Epoch 8803/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.0670 - mae: 7.5237 - val_loss: 2815860950546857069901774848.0000 - val_mae: 2218351132672.0000\n",
      "\n",
      "Epoch 08803: val_mae did not improve from 6.28420\n",
      "Epoch 8804/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 70.6996 - mae: 6.5340 - val_loss: 200.6869 - val_mae: 7.6445\n",
      "\n",
      "Epoch 08804: val_mae did not improve from 6.28420\n",
      "Epoch 8805/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 115.0202 - mae: 8.0521 - val_loss: 225.6075 - val_mae: 8.8224\n",
      "\n",
      "Epoch 08805: val_mae did not improve from 6.28420\n",
      "Epoch 8806/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 85.2179 - mae: 7.1621 - val_loss: 94341594145341499768832.0000 - val_mae: 11943910400.0000\n",
      "\n",
      "Epoch 08806: val_mae did not improve from 6.28420\n",
      "Epoch 8807/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 147.9471 - mae: 9.6417 - val_loss: 243.6935 - val_mae: 9.3484\n",
      "\n",
      "Epoch 08807: val_mae did not improve from 6.28420\n",
      "Epoch 8808/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 120.0060 - mae: 8.6125 - val_loss: 157284614106212676403200.0000 - val_mae: 15674994688.0000\n",
      "\n",
      "Epoch 08808: val_mae did not improve from 6.28420\n",
      "Epoch 8809/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 98.0820 - mae: 7.8731 - val_loss: 335578003100583524323819520.0000 - val_mae: 669351280640.0000\n",
      "\n",
      "Epoch 08809: val_mae did not improve from 6.28420\n",
      "Epoch 8810/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 100.3538 - mae: 8.0733 - val_loss: 524078234133585275239006208.0000 - val_mae: 851702513664.0000\n",
      "\n",
      "Epoch 08810: val_mae did not improve from 6.28420\n",
      "Epoch 8811/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 131.7721 - mae: 9.0215 - val_loss: 196.1713 - val_mae: 7.5284\n",
      "\n",
      "Epoch 08811: val_mae did not improve from 6.28420\n",
      "Epoch 8812/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 88.0031 - mae: 7.5091 - val_loss: 206.5244 - val_mae: 8.2541\n",
      "\n",
      "Epoch 08812: val_mae did not improve from 6.28420\n",
      "Epoch 8813/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 117.0747 - mae: 8.6811 - val_loss: 224.6257 - val_mae: 8.8791\n",
      "\n",
      "Epoch 08813: val_mae did not improve from 6.28420\n",
      "Epoch 8814/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 133.5454 - mae: 9.0557 - val_loss: 282.4279 - val_mae: 10.8253\n",
      "\n",
      "Epoch 08814: val_mae did not improve from 6.28420\n",
      "Epoch 8815/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 98.4000 - mae: 7.7894 - val_loss: 262.5690 - val_mae: 10.5648\n",
      "\n",
      "Epoch 08815: val_mae did not improve from 6.28420\n",
      "Epoch 8816/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 112.6038 - mae: 8.4796 - val_loss: 262.3917 - val_mae: 10.5052\n",
      "\n",
      "Epoch 08816: val_mae did not improve from 6.28420\n",
      "Epoch 8817/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 91.0563 - mae: 7.6307 - val_loss: 212.0945 - val_mae: 8.4062\n",
      "\n",
      "Epoch 08817: val_mae did not improve from 6.28420\n",
      "Epoch 8818/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 88.4567 - mae: 7.4637 - val_loss: 222.7559 - val_mae: 8.9112\n",
      "\n",
      "Epoch 08818: val_mae did not improve from 6.28420\n",
      "Epoch 8819/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 75.1335 - mae: 6.7833 - val_loss: 238.6441 - val_mae: 9.9023\n",
      "\n",
      "Epoch 08819: val_mae did not improve from 6.28420\n",
      "Epoch 8820/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 80.6892 - mae: 7.1273 - val_loss: 205.0506 - val_mae: 8.1168\n",
      "\n",
      "Epoch 08820: val_mae did not improve from 6.28420\n",
      "Epoch 8821/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 90.5760 - mae: 7.6617 - val_loss: 237.9597 - val_mae: 9.6122\n",
      "\n",
      "Epoch 08821: val_mae did not improve from 6.28420\n",
      "Epoch 8822/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 117.1093 - mae: 8.7062 - val_loss: 211.7691 - val_mae: 8.3993\n",
      "\n",
      "Epoch 08822: val_mae did not improve from 6.28420\n",
      "Epoch 8823/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 75.2512 - mae: 6.8250 - val_loss: 198.7937 - val_mae: 7.4917\n",
      "\n",
      "Epoch 08823: val_mae did not improve from 6.28420\n",
      "Epoch 8824/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 62.1109 - mae: 6.2666 - val_loss: 219.1302 - val_mae: 8.7713\n",
      "\n",
      "Epoch 08824: val_mae did not improve from 6.28420\n",
      "Epoch 8825/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 96.2878 - mae: 7.9462 - val_loss: 223.5359 - val_mae: 8.6770\n",
      "\n",
      "Epoch 08825: val_mae did not improve from 6.28420\n",
      "Epoch 8826/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 120.3799 - mae: 8.6104 - val_loss: 217.6471 - val_mae: 8.5002\n",
      "\n",
      "Epoch 08826: val_mae did not improve from 6.28420\n",
      "Epoch 8827/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 122.8904 - mae: 8.8193 - val_loss: 225.3413 - val_mae: 8.5387\n",
      "\n",
      "Epoch 08827: val_mae did not improve from 6.28420\n",
      "Epoch 8828/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 81.9517 - mae: 7.2677 - val_loss: 207.6021 - val_mae: 7.8506\n",
      "\n",
      "Epoch 08828: val_mae did not improve from 6.28420\n",
      "Epoch 8829/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.2213 - mae: 6.6789 - val_loss: 214.8969 - val_mae: 8.2420\n",
      "\n",
      "Epoch 08829: val_mae did not improve from 6.28420\n",
      "Epoch 8830/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 88.1002 - mae: 7.5101 - val_loss: 242.0983 - val_mae: 9.6302\n",
      "\n",
      "Epoch 08830: val_mae did not improve from 6.28420\n",
      "Epoch 8831/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 94.5721 - mae: 7.7938 - val_loss: 218.7922 - val_mae: 8.6715\n",
      "\n",
      "Epoch 08831: val_mae did not improve from 6.28420\n",
      "Epoch 8832/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 91.6898 - mae: 7.5676 - val_loss: 201.4025 - val_mae: 7.7016\n",
      "\n",
      "Epoch 08832: val_mae did not improve from 6.28420\n",
      "Epoch 8833/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 92.5066 - mae: 7.6337 - val_loss: 237.8837 - val_mae: 9.6361\n",
      "\n",
      "Epoch 08833: val_mae did not improve from 6.28420\n",
      "Epoch 8834/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 113.1987 - mae: 8.2749 - val_loss: 225.0764 - val_mae: 8.9749\n",
      "\n",
      "Epoch 08834: val_mae did not improve from 6.28420\n",
      "Epoch 8835/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 143.4940 - mae: 9.5182 - val_loss: 317.8280 - val_mae: 12.4123\n",
      "\n",
      "Epoch 08835: val_mae did not improve from 6.28420\n",
      "Epoch 8836/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 179.8404 - mae: 10.7276 - val_loss: 225.6687 - val_mae: 8.6827\n",
      "\n",
      "Epoch 08836: val_mae did not improve from 6.28420\n",
      "Epoch 8837/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 115.8977 - mae: 8.5162 - val_loss: 251.1018 - val_mae: 10.0803\n",
      "\n",
      "Epoch 08837: val_mae did not improve from 6.28420\n",
      "Epoch 8838/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 105.5035 - mae: 8.2694 - val_loss: 419.3876 - val_mae: 8.8964\n",
      "\n",
      "Epoch 08838: val_mae did not improve from 6.28420\n",
      "Epoch 8839/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 95.0951 - mae: 7.7127 - val_loss: 1136265463802573971521536.0000 - val_mae: 44588916736.0000\n",
      "\n",
      "Epoch 08839: val_mae did not improve from 6.28420\n",
      "Epoch 8840/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 113.1910 - mae: 8.4460 - val_loss: 218.0492 - val_mae: 8.2207\n",
      "\n",
      "Epoch 08840: val_mae did not improve from 6.28420\n",
      "Epoch 8841/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 54.7024 - mae: 5.8171 - val_loss: 3835565778720664477237248.0000 - val_mae: 78889574400.0000\n",
      "\n",
      "Epoch 08841: val_mae did not improve from 6.28420\n",
      "Epoch 8842/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 81.1218 - mae: 7.0376 - val_loss: 494517326755465718774366208.0000 - val_mae: 895453298688.0000\n",
      "\n",
      "Epoch 08842: val_mae did not improve from 6.28420\n",
      "Epoch 8843/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 91.2614 - mae: 7.6667 - val_loss: 768410782732625957001101312.0000 - val_mae: 1106934497280.0000\n",
      "\n",
      "Epoch 08843: val_mae did not improve from 6.28420\n",
      "Epoch 8844/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 114.9354 - mae: 8.4230 - val_loss: 1957197037463389401958580224.0000 - val_mae: 1797861146624.0000\n",
      "\n",
      "Epoch 08844: val_mae did not improve from 6.28420\n",
      "Epoch 8845/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 94.1043 - mae: 7.7734 - val_loss: 1335025548493218824406958080.0000 - val_mae: 1571443310592.0000\n",
      "\n",
      "Epoch 08845: val_mae did not improve from 6.28420\n",
      "Epoch 8846/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 90.2831 - mae: 7.6039 - val_loss: 2823155531023364774992805888.0000 - val_mae: 2105375588352.0000\n",
      "\n",
      "Epoch 08846: val_mae did not improve from 6.28420\n",
      "Epoch 8847/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 88.8015 - mae: 7.4160 - val_loss: 216.1346 - val_mae: 8.4652\n",
      "\n",
      "Epoch 08847: val_mae did not improve from 6.28420\n",
      "Epoch 8848/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.8334 - mae: 7.2979 - val_loss: 382.6845 - val_mae: 13.4199\n",
      "\n",
      "Epoch 08848: val_mae did not improve from 6.28420\n",
      "Epoch 8849/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 164.5949 - mae: 10.5275 - val_loss: 308.9758 - val_mae: 12.2045\n",
      "\n",
      "Epoch 08849: val_mae did not improve from 6.28420\n",
      "Epoch 8850/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 137.4205 - mae: 9.2011 - val_loss: 6390798041029232704981303296.0000 - val_mae: 3200977207296.0000\n",
      "\n",
      "Epoch 08850: val_mae did not improve from 6.28420\n",
      "Epoch 8851/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 111.3108 - mae: 8.4404 - val_loss: 11881862485543243385977962496.0000 - val_mae: 4406793732096.0000\n",
      "\n",
      "Epoch 08851: val_mae did not improve from 6.28420\n",
      "Epoch 8852/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 128.5777 - mae: 9.0284 - val_loss: 210.2279 - val_mae: 8.1007\n",
      "\n",
      "Epoch 08852: val_mae did not improve from 6.28420\n",
      "Epoch 8853/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.2623 - mae: 7.5394 - val_loss: 305.3917 - val_mae: 12.3645\n",
      "\n",
      "Epoch 08853: val_mae did not improve from 6.28420\n",
      "Epoch 8854/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 109.8908 - mae: 8.3410 - val_loss: 218.0142 - val_mae: 8.2468\n",
      "\n",
      "Epoch 08854: val_mae did not improve from 6.28420\n",
      "Epoch 8855/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 103.6978 - mae: 8.1519 - val_loss: 271.0241 - val_mae: 10.9705\n",
      "\n",
      "Epoch 08855: val_mae did not improve from 6.28420\n",
      "Epoch 8856/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 133.7380 - mae: 9.4104 - val_loss: 527.7360 - val_mae: 17.2910\n",
      "\n",
      "Epoch 08856: val_mae did not improve from 6.28420\n",
      "Epoch 8857/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 167.7225 - mae: 10.0453 - val_loss: 253.5640 - val_mae: 10.1543\n",
      "\n",
      "Epoch 08857: val_mae did not improve from 6.28420\n",
      "Epoch 8858/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 122.2004 - mae: 9.0464 - val_loss: 320.3965 - val_mae: 12.2423\n",
      "\n",
      "Epoch 08858: val_mae did not improve from 6.28420\n",
      "Epoch 8859/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 112.9982 - mae: 8.3466 - val_loss: 243.5380 - val_mae: 9.6454\n",
      "\n",
      "Epoch 08859: val_mae did not improve from 6.28420\n",
      "Epoch 8860/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 103.1221 - mae: 7.9329 - val_loss: 224.6702 - val_mae: 8.9321\n",
      "\n",
      "Epoch 08860: val_mae did not improve from 6.28420\n",
      "Epoch 8861/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 106.7812 - mae: 8.1055 - val_loss: 205.4073 - val_mae: 8.0155\n",
      "\n",
      "Epoch 08861: val_mae did not improve from 6.28420\n",
      "Epoch 8862/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 104.5042 - mae: 8.0375 - val_loss: 250.5144 - val_mae: 10.2602\n",
      "\n",
      "Epoch 08862: val_mae did not improve from 6.28420\n",
      "Epoch 8863/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 133.3530 - mae: 9.1157 - val_loss: 240.3784 - val_mae: 9.6427\n",
      "\n",
      "Epoch 08863: val_mae did not improve from 6.28420\n",
      "Epoch 8864/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 129.6845 - mae: 9.1024 - val_loss: 285.4651 - val_mae: 11.4394\n",
      "\n",
      "Epoch 08864: val_mae did not improve from 6.28420\n",
      "Epoch 8865/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 124.9304 - mae: 9.0438 - val_loss: 216.4713 - val_mae: 8.6100\n",
      "\n",
      "Epoch 08865: val_mae did not improve from 6.28420\n",
      "Epoch 8866/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 86.4091 - mae: 7.4139 - val_loss: 230.6057 - val_mae: 9.1569\n",
      "\n",
      "Epoch 08866: val_mae did not improve from 6.28420\n",
      "Epoch 8867/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 102.7565 - mae: 8.1106 - val_loss: 223.5330 - val_mae: 8.7020\n",
      "\n",
      "Epoch 08867: val_mae did not improve from 6.28420\n",
      "Epoch 8868/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 107.1157 - mae: 8.3248 - val_loss: 534.7358 - val_mae: 12.2682\n",
      "\n",
      "Epoch 08868: val_mae did not improve from 6.28420\n",
      "Epoch 8869/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 113.6095 - mae: 8.4051 - val_loss: 216.8116 - val_mae: 8.5393\n",
      "\n",
      "Epoch 08869: val_mae did not improve from 6.28420\n",
      "Epoch 8870/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.0694 - mae: 6.5837 - val_loss: 196.2301 - val_mae: 7.4003\n",
      "\n",
      "Epoch 08870: val_mae did not improve from 6.28420\n",
      "Epoch 8871/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 96.0791 - mae: 7.5744 - val_loss: 253.6864 - val_mae: 10.2111\n",
      "\n",
      "Epoch 08871: val_mae did not improve from 6.28420\n",
      "Epoch 8872/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 124.4805 - mae: 8.8884 - val_loss: 1941530622397035996774400.0000 - val_mae: 54472884224.0000\n",
      "\n",
      "Epoch 08872: val_mae did not improve from 6.28420\n",
      "Epoch 8873/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 86.4730 - mae: 7.4903 - val_loss: 339.2693 - val_mae: 13.8237\n",
      "\n",
      "Epoch 08873: val_mae did not improve from 6.28420\n",
      "Epoch 8874/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 116.2086 - mae: 8.7786 - val_loss: 268.8177 - val_mae: 10.7831\n",
      "\n",
      "Epoch 08874: val_mae did not improve from 6.28420\n",
      "Epoch 8875/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.3461 - mae: 7.5543 - val_loss: 324.7574 - val_mae: 12.7913\n",
      "\n",
      "Epoch 08875: val_mae did not improve from 6.28420\n",
      "Epoch 8876/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 140.8066 - mae: 9.4968 - val_loss: 206.1316 - val_mae: 8.0801\n",
      "\n",
      "Epoch 08876: val_mae did not improve from 6.28420\n",
      "Epoch 8877/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 87.9562 - mae: 7.3418 - val_loss: 219.5808 - val_mae: 8.8447\n",
      "\n",
      "Epoch 08877: val_mae did not improve from 6.28420\n",
      "Epoch 8878/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 82.9665 - mae: 7.4082 - val_loss: 224.5733 - val_mae: 8.7603\n",
      "\n",
      "Epoch 08878: val_mae did not improve from 6.28420\n",
      "Epoch 8879/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 76.3153 - mae: 6.8636 - val_loss: 202.4064 - val_mae: 7.9161\n",
      "\n",
      "Epoch 08879: val_mae did not improve from 6.28420\n",
      "Epoch 8880/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 127.6961 - mae: 8.9045 - val_loss: 211.6906 - val_mae: 8.5970\n",
      "\n",
      "Epoch 08880: val_mae did not improve from 6.28420\n",
      "Epoch 8881/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 78.4284 - mae: 6.9329 - val_loss: 494.2590 - val_mae: 8.6815\n",
      "\n",
      "Epoch 08881: val_mae did not improve from 6.28420\n",
      "Epoch 8882/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 86.9184 - mae: 7.4103 - val_loss: 3301762833857318175637504.0000 - val_mae: 82701860864.0000\n",
      "\n",
      "Epoch 08882: val_mae did not improve from 6.28420\n",
      "Epoch 8883/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 120.0644 - mae: 8.8569 - val_loss: 235.3441 - val_mae: 9.5176\n",
      "\n",
      "Epoch 08883: val_mae did not improve from 6.28420\n",
      "Epoch 8884/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 136.5501 - mae: 9.0984 - val_loss: 7756656427574569470001152.0000 - val_mae: 106967703552.0000\n",
      "\n",
      "Epoch 08884: val_mae did not improve from 6.28420\n",
      "Epoch 8885/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 122.6626 - mae: 8.7716 - val_loss: 424790253435096833982464.0000 - val_mae: 26503186432.0000\n",
      "\n",
      "Epoch 08885: val_mae did not improve from 6.28420\n",
      "Epoch 8886/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.2707 - mae: 7.3004 - val_loss: 8307641073390695460372480.0000 - val_mae: 116050034688.0000\n",
      "\n",
      "Epoch 08886: val_mae did not improve from 6.28420\n",
      "Epoch 8887/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 67.8319 - mae: 6.5952 - val_loss: 1612475583997571953065984.0000 - val_mae: 50356805632.0000\n",
      "\n",
      "Epoch 08887: val_mae did not improve from 6.28420\n",
      "Epoch 8888/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 75.0336 - mae: 6.8493 - val_loss: 240.0185 - val_mae: 9.5300\n",
      "\n",
      "Epoch 08888: val_mae did not improve from 6.28420\n",
      "Epoch 8889/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 92.2058 - mae: 7.6295 - val_loss: 253.3471 - val_mae: 9.9338\n",
      "\n",
      "Epoch 08889: val_mae did not improve from 6.28420\n",
      "Epoch 8890/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 141.2937 - mae: 9.2188 - val_loss: 225.6297 - val_mae: 8.9810\n",
      "\n",
      "Epoch 08890: val_mae did not improve from 6.28420\n",
      "Epoch 8891/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 108.2250 - mae: 8.2305 - val_loss: 4171741573281593618351983165440.0000 - val_mae: 79863339810816.0000\n",
      "\n",
      "Epoch 08891: val_mae did not improve from 6.28420\n",
      "Epoch 8892/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 115.8339 - mae: 8.6042 - val_loss: 240374202420337208938852127866880.0000 - val_mae: 650345625681920.0000\n",
      "\n",
      "Epoch 08892: val_mae did not improve from 6.28420\n",
      "Epoch 8893/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 115.7476 - mae: 8.5163 - val_loss: 227.9966 - val_mae: 8.8754\n",
      "\n",
      "Epoch 08893: val_mae did not improve from 6.28420\n",
      "Epoch 8894/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 81.4810 - mae: 7.0197 - val_loss: 244.1178 - val_mae: 9.8319\n",
      "\n",
      "Epoch 08894: val_mae did not improve from 6.28420\n",
      "Epoch 8895/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.3144 - mae: 7.8770 - val_loss: 215.3402 - val_mae: 8.3025\n",
      "\n",
      "Epoch 08895: val_mae did not improve from 6.28420\n",
      "Epoch 8896/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 80.8999 - mae: 7.0804 - val_loss: 318.7485 - val_mae: 10.6949\n",
      "\n",
      "Epoch 08896: val_mae did not improve from 6.28420\n",
      "Epoch 8897/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.9453 - mae: 7.3057 - val_loss: 3688909995253218410496.0000 - val_mae: 2391462144.0000\n",
      "\n",
      "Epoch 08897: val_mae did not improve from 6.28420\n",
      "Epoch 8898/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 70.6950 - mae: 6.6436 - val_loss: 234.3757 - val_mae: 9.0897\n",
      "\n",
      "Epoch 08898: val_mae did not improve from 6.28420\n",
      "Epoch 8899/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.5274 - mae: 7.2017 - val_loss: 214.1141 - val_mae: 8.3132\n",
      "\n",
      "Epoch 08899: val_mae did not improve from 6.28420\n",
      "Epoch 8900/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 99.1995 - mae: 7.9309 - val_loss: 68764832174247266750194815860736.0000 - val_mae: 529835118034944.0000\n",
      "\n",
      "Epoch 08900: val_mae did not improve from 6.28420\n",
      "Epoch 8901/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 97.0447 - mae: 7.6471 - val_loss: 235285770701968041312256.0000 - val_mae: 17837807616.0000\n",
      "\n",
      "Epoch 08901: val_mae did not improve from 6.28420\n",
      "Epoch 8902/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 118.2312 - mae: 8.5623 - val_loss: 360925673872537662641534633246720.0000 - val_mae: 1227390655135744.0000\n",
      "\n",
      "Epoch 08902: val_mae did not improve from 6.28420\n",
      "Epoch 8903/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 108.8518 - mae: 8.1901 - val_loss: 303.7706 - val_mae: 9.2512\n",
      "\n",
      "Epoch 08903: val_mae did not improve from 6.28420\n",
      "Epoch 8904/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 95.6144 - mae: 7.6942 - val_loss: 268.1194 - val_mae: 9.0244\n",
      "\n",
      "Epoch 08904: val_mae did not improve from 6.28420\n",
      "Epoch 8905/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 98.8880 - mae: 7.6487 - val_loss: 279.0124 - val_mae: 10.4523\n",
      "\n",
      "Epoch 08905: val_mae did not improve from 6.28420\n",
      "Epoch 8906/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 114.1489 - mae: 8.4031 - val_loss: 290.0771 - val_mae: 10.6940\n",
      "\n",
      "Epoch 08906: val_mae did not improve from 6.28420\n",
      "Epoch 8907/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 93.2800 - mae: 7.7177 - val_loss: 238.4238 - val_mae: 9.2522\n",
      "\n",
      "Epoch 08907: val_mae did not improve from 6.28420\n",
      "Epoch 8908/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 108.4760 - mae: 8.2977 - val_loss: 236.8361 - val_mae: 9.2514\n",
      "\n",
      "Epoch 08908: val_mae did not improve from 6.28420\n",
      "Epoch 8909/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 92.9709 - mae: 7.6768 - val_loss: 229.8539 - val_mae: 8.9394\n",
      "\n",
      "Epoch 08909: val_mae did not improve from 6.28420\n",
      "Epoch 8910/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 108.4061 - mae: 8.2069 - val_loss: 226.4162 - val_mae: 7.9470\n",
      "\n",
      "Epoch 08910: val_mae did not improve from 6.28420\n",
      "Epoch 8911/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 95.6402 - mae: 7.6671 - val_loss: 216.1769 - val_mae: 8.1644\n",
      "\n",
      "Epoch 08911: val_mae did not improve from 6.28420\n",
      "Epoch 8912/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 79.2883 - mae: 6.9790 - val_loss: 273.8835 - val_mae: 10.4284\n",
      "\n",
      "Epoch 08912: val_mae did not improve from 6.28420\n",
      "Epoch 8913/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 99.1142 - mae: 7.9201 - val_loss: 211.1686 - val_mae: 8.1846\n",
      "\n",
      "Epoch 08913: val_mae did not improve from 6.28420\n",
      "Epoch 8914/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 80.6807 - mae: 7.1683 - val_loss: 254.8775 - val_mae: 10.1358\n",
      "\n",
      "Epoch 08914: val_mae did not improve from 6.28420\n",
      "Epoch 8915/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.0709 - mae: 7.0323 - val_loss: 217.3846 - val_mae: 8.4697\n",
      "\n",
      "Epoch 08915: val_mae did not improve from 6.28420\n",
      "Epoch 8916/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.6094 - mae: 7.2192 - val_loss: 213.2459 - val_mae: 8.4879\n",
      "\n",
      "Epoch 08916: val_mae did not improve from 6.28420\n",
      "Epoch 8917/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 89.3747 - mae: 7.5198 - val_loss: 203.7455 - val_mae: 7.8587\n",
      "\n",
      "Epoch 08917: val_mae did not improve from 6.28420\n",
      "Epoch 8918/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 65.7799 - mae: 6.4263 - val_loss: 260.6991 - val_mae: 10.3144\n",
      "\n",
      "Epoch 08918: val_mae did not improve from 6.28420\n",
      "Epoch 8919/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 108.4662 - mae: 8.2480 - val_loss: 248.9916 - val_mae: 9.7155\n",
      "\n",
      "Epoch 08919: val_mae did not improve from 6.28420\n",
      "Epoch 8920/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 118.7441 - mae: 8.8378 - val_loss: 233.1299 - val_mae: 9.3199\n",
      "\n",
      "Epoch 08920: val_mae did not improve from 6.28420\n",
      "Epoch 8921/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 83.6381 - mae: 7.3180 - val_loss: 226.6456 - val_mae: 8.7877\n",
      "\n",
      "Epoch 08921: val_mae did not improve from 6.28420\n",
      "Epoch 8922/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 101.0412 - mae: 8.1561 - val_loss: 246.4248 - val_mae: 9.6192\n",
      "\n",
      "Epoch 08922: val_mae did not improve from 6.28420\n",
      "Epoch 8923/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 91.5263 - mae: 7.6252 - val_loss: 208.3634 - val_mae: 7.4662\n",
      "\n",
      "Epoch 08923: val_mae did not improve from 6.28420\n",
      "Epoch 8924/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 67.1707 - mae: 6.4068 - val_loss: 228.4524 - val_mae: 8.8549\n",
      "\n",
      "Epoch 08924: val_mae did not improve from 6.28420\n",
      "Epoch 8925/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 95.7204 - mae: 7.9483 - val_loss: 200.7037 - val_mae: 7.4840\n",
      "\n",
      "Epoch 08925: val_mae did not improve from 6.28420\n",
      "Epoch 8926/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.8669 - mae: 7.3878 - val_loss: 226.8548 - val_mae: 9.1338\n",
      "\n",
      "Epoch 08926: val_mae did not improve from 6.28420\n",
      "Epoch 8927/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 68.5799 - mae: 6.7004 - val_loss: 237.9377 - val_mae: 9.2282\n",
      "\n",
      "Epoch 08927: val_mae did not improve from 6.28420\n",
      "Epoch 8928/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 74.2764 - mae: 6.7137 - val_loss: 217.7108 - val_mae: 8.3386\n",
      "\n",
      "Epoch 08928: val_mae did not improve from 6.28420\n",
      "Epoch 8929/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 98.8376 - mae: 7.9576 - val_loss: 268.8995 - val_mae: 10.7062\n",
      "\n",
      "Epoch 08929: val_mae did not improve from 6.28420\n",
      "Epoch 8930/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 84.8312 - mae: 7.3433 - val_loss: 223.4799 - val_mae: 8.6695\n",
      "\n",
      "Epoch 08930: val_mae did not improve from 6.28420\n",
      "Epoch 8931/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 103.5896 - mae: 7.9555 - val_loss: 213.8115 - val_mae: 8.2157\n",
      "\n",
      "Epoch 08931: val_mae did not improve from 6.28420\n",
      "Epoch 8932/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 122.7761 - mae: 8.6312 - val_loss: 303.4059 - val_mae: 11.4311\n",
      "\n",
      "Epoch 08932: val_mae did not improve from 6.28420\n",
      "Epoch 8933/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 155.5867 - mae: 9.8855 - val_loss: 235.1638 - val_mae: 9.2431\n",
      "\n",
      "Epoch 08933: val_mae did not improve from 6.28420\n",
      "Epoch 8934/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 96.3285 - mae: 7.8707 - val_loss: 226.9603 - val_mae: 8.7142\n",
      "\n",
      "Epoch 08934: val_mae did not improve from 6.28420\n",
      "Epoch 8935/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 86.1757 - mae: 7.3736 - val_loss: 223.0284 - val_mae: 8.6324\n",
      "\n",
      "Epoch 08935: val_mae did not improve from 6.28420\n",
      "Epoch 8936/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 87.1390 - mae: 7.4795 - val_loss: 276.2862 - val_mae: 10.8147\n",
      "\n",
      "Epoch 08936: val_mae did not improve from 6.28420\n",
      "Epoch 8937/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 112.7176 - mae: 8.5995 - val_loss: 253.3671 - val_mae: 10.1541\n",
      "\n",
      "Epoch 08937: val_mae did not improve from 6.28420\n",
      "Epoch 8938/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 104.9285 - mae: 8.2166 - val_loss: 215.2621 - val_mae: 8.2778\n",
      "\n",
      "Epoch 08938: val_mae did not improve from 6.28420\n",
      "Epoch 8939/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 95.9727 - mae: 7.7532 - val_loss: 252.7271 - val_mae: 9.9209\n",
      "\n",
      "Epoch 08939: val_mae did not improve from 6.28420\n",
      "Epoch 8940/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 127.2126 - mae: 9.0253 - val_loss: 285.0230 - val_mae: 11.0680\n",
      "\n",
      "Epoch 08940: val_mae did not improve from 6.28420\n",
      "Epoch 8941/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 141.7509 - mae: 9.5233 - val_loss: 249.4237 - val_mae: 9.6489\n",
      "\n",
      "Epoch 08941: val_mae did not improve from 6.28420\n",
      "Epoch 8942/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 119.6522 - mae: 8.6666 - val_loss: 231.2245 - val_mae: 8.9111\n",
      "\n",
      "Epoch 08942: val_mae did not improve from 6.28420\n",
      "Epoch 8943/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.8988 - mae: 7.5005 - val_loss: 227.9760 - val_mae: 8.8025\n",
      "\n",
      "Epoch 08943: val_mae did not improve from 6.28420\n",
      "Epoch 8944/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 122.0776 - mae: 8.8504 - val_loss: 248.8556 - val_mae: 9.6242\n",
      "\n",
      "Epoch 08944: val_mae did not improve from 6.28420\n",
      "Epoch 8945/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 88.0831 - mae: 7.5599 - val_loss: 256.6729 - val_mae: 10.1998\n",
      "\n",
      "Epoch 08945: val_mae did not improve from 6.28420\n",
      "Epoch 8946/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 104.9305 - mae: 8.1258 - val_loss: 217.7227 - val_mae: 8.2407\n",
      "\n",
      "Epoch 08946: val_mae did not improve from 6.28420\n",
      "Epoch 8947/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 104.7413 - mae: 8.1373 - val_loss: 218.5817 - val_mae: 8.4497\n",
      "\n",
      "Epoch 08947: val_mae did not improve from 6.28420\n",
      "Epoch 8948/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 105.0112 - mae: 8.2229 - val_loss: 234.9540 - val_mae: 9.2312\n",
      "\n",
      "Epoch 08948: val_mae did not improve from 6.28420\n",
      "Epoch 8949/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 80.2930 - mae: 7.0227 - val_loss: 226.8665 - val_mae: 8.9203\n",
      "\n",
      "Epoch 08949: val_mae did not improve from 6.28420\n",
      "Epoch 8950/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 128.5995 - mae: 9.0508 - val_loss: 275.6512 - val_mae: 10.8802\n",
      "\n",
      "Epoch 08950: val_mae did not improve from 6.28420\n",
      "Epoch 8951/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 104.1593 - mae: 8.0452 - val_loss: 231.6629 - val_mae: 8.9907\n",
      "\n",
      "Epoch 08951: val_mae did not improve from 6.28420\n",
      "Epoch 8952/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 104.4433 - mae: 8.0280 - val_loss: 267.9367 - val_mae: 10.6143\n",
      "\n",
      "Epoch 08952: val_mae did not improve from 6.28420\n",
      "Epoch 8953/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 101.0841 - mae: 8.0706 - val_loss: 220.6249 - val_mae: 8.3737\n",
      "\n",
      "Epoch 08953: val_mae did not improve from 6.28420\n",
      "Epoch 8954/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 67.6062 - mae: 6.3508 - val_loss: 271.4442 - val_mae: 10.5482\n",
      "\n",
      "Epoch 08954: val_mae did not improve from 6.28420\n",
      "Epoch 8955/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 112.8813 - mae: 8.5627 - val_loss: 212.0939 - val_mae: 8.2156\n",
      "\n",
      "Epoch 08955: val_mae did not improve from 6.28420\n",
      "Epoch 8956/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 88.9017 - mae: 7.5788 - val_loss: 245.0805 - val_mae: 9.7207\n",
      "\n",
      "Epoch 08956: val_mae did not improve from 6.28420\n",
      "Epoch 8957/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 98.6617 - mae: 7.7716 - val_loss: 369.2579 - val_mae: 14.3490\n",
      "\n",
      "Epoch 08957: val_mae did not improve from 6.28420\n",
      "Epoch 8958/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 130.9016 - mae: 9.0136 - val_loss: 248.9944 - val_mae: 9.6846\n",
      "\n",
      "Epoch 08958: val_mae did not improve from 6.28420\n",
      "Epoch 8959/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 131.4419 - mae: 9.2819 - val_loss: 256.0843 - val_mae: 9.9616\n",
      "\n",
      "Epoch 08959: val_mae did not improve from 6.28420\n",
      "Epoch 8960/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 83.7149 - mae: 7.1919 - val_loss: 237.6943 - val_mae: 8.9910\n",
      "\n",
      "Epoch 08960: val_mae did not improve from 6.28420\n",
      "Epoch 8961/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 87.3007 - mae: 7.3987 - val_loss: 241.8783 - val_mae: 9.4242\n",
      "\n",
      "Epoch 08961: val_mae did not improve from 6.28420\n",
      "Epoch 8962/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 75.0233 - mae: 6.9329 - val_loss: 221.3240 - val_mae: 8.1277\n",
      "\n",
      "Epoch 08962: val_mae did not improve from 6.28420\n",
      "Epoch 8963/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.3922 - mae: 7.4796 - val_loss: 255.9239 - val_mae: 9.6137\n",
      "\n",
      "Epoch 08963: val_mae did not improve from 6.28420\n",
      "Epoch 8964/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 116.1259 - mae: 8.5586 - val_loss: 204.5449 - val_mae: 7.6707\n",
      "\n",
      "Epoch 08964: val_mae did not improve from 6.28420\n",
      "Epoch 8965/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 97.1623 - mae: 7.6697 - val_loss: 207.8717 - val_mae: 7.5920\n",
      "\n",
      "Epoch 08965: val_mae did not improve from 6.28420\n",
      "Epoch 8966/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.7054 - mae: 6.5481 - val_loss: 210.1154 - val_mae: 7.9757\n",
      "\n",
      "Epoch 08966: val_mae did not improve from 6.28420\n",
      "Epoch 8967/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 88.8712 - mae: 7.4876 - val_loss: 231.2872 - val_mae: 9.1608\n",
      "\n",
      "Epoch 08967: val_mae did not improve from 6.28420\n",
      "Epoch 8968/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 85.1376 - mae: 7.4288 - val_loss: 259.6057 - val_mae: 10.2160\n",
      "\n",
      "Epoch 08968: val_mae did not improve from 6.28420\n",
      "Epoch 8969/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 111.5779 - mae: 8.7079 - val_loss: 243.6399 - val_mae: 9.6460\n",
      "\n",
      "Epoch 08969: val_mae did not improve from 6.28420\n",
      "Epoch 8970/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 104.0658 - mae: 8.1259 - val_loss: 240.5670 - val_mae: 9.6208\n",
      "\n",
      "Epoch 08970: val_mae did not improve from 6.28420\n",
      "Epoch 8971/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 88.7345 - mae: 7.4927 - val_loss: 254.1536 - val_mae: 10.0841\n",
      "\n",
      "Epoch 08971: val_mae did not improve from 6.28420\n",
      "Epoch 8972/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 86.2169 - mae: 7.4371 - val_loss: 252.3451 - val_mae: 10.1823\n",
      "\n",
      "Epoch 08972: val_mae did not improve from 6.28420\n",
      "Epoch 8973/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 81.8209 - mae: 7.2398 - val_loss: 216.4516 - val_mae: 8.2880\n",
      "\n",
      "Epoch 08973: val_mae did not improve from 6.28420\n",
      "Epoch 8974/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 83.1685 - mae: 7.2428 - val_loss: 241.0486 - val_mae: 9.5339\n",
      "\n",
      "Epoch 08974: val_mae did not improve from 6.28420\n",
      "Epoch 8975/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 86.0811 - mae: 7.4063 - val_loss: 258.9072 - val_mae: 10.1368\n",
      "\n",
      "Epoch 08975: val_mae did not improve from 6.28420\n",
      "Epoch 8976/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 130.6471 - mae: 9.0007 - val_loss: 214.3501 - val_mae: 8.2038\n",
      "\n",
      "Epoch 08976: val_mae did not improve from 6.28420\n",
      "Epoch 8977/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 107.8720 - mae: 8.2945 - val_loss: 212.6713 - val_mae: 8.1749\n",
      "\n",
      "Epoch 08977: val_mae did not improve from 6.28420\n",
      "Epoch 8978/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 87.7288 - mae: 7.4062 - val_loss: 255.9289 - val_mae: 10.2265\n",
      "\n",
      "Epoch 08978: val_mae did not improve from 6.28420\n",
      "Epoch 8979/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 93.4440 - mae: 7.6930 - val_loss: 208.1705 - val_mae: 7.8901\n",
      "\n",
      "Epoch 08979: val_mae did not improve from 6.28420\n",
      "Epoch 8980/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 76.7432 - mae: 6.9193 - val_loss: 209.3322 - val_mae: 7.8131\n",
      "\n",
      "Epoch 08980: val_mae did not improve from 6.28420\n",
      "Epoch 8981/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 85.7491 - mae: 7.2113 - val_loss: 216.3461 - val_mae: 8.3426\n",
      "\n",
      "Epoch 08981: val_mae did not improve from 6.28420\n",
      "Epoch 8982/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.4233 - mae: 6.7664 - val_loss: 211.9984 - val_mae: 8.4580\n",
      "\n",
      "Epoch 08982: val_mae did not improve from 6.28420\n",
      "Epoch 8983/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 72.5668 - mae: 6.7678 - val_loss: 211.5533 - val_mae: 8.3292\n",
      "\n",
      "Epoch 08983: val_mae did not improve from 6.28420\n",
      "Epoch 8984/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 109.1917 - mae: 8.1668 - val_loss: 281.6279 - val_mae: 11.2694\n",
      "\n",
      "Epoch 08984: val_mae did not improve from 6.28420\n",
      "Epoch 8985/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 104.3093 - mae: 8.2670 - val_loss: 246.0085 - val_mae: 9.5331\n",
      "\n",
      "Epoch 08985: val_mae did not improve from 6.28420\n",
      "Epoch 8986/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 163.3207 - mae: 10.2263 - val_loss: 229.3072 - val_mae: 9.0307\n",
      "\n",
      "Epoch 08986: val_mae did not improve from 6.28420\n",
      "Epoch 8987/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 105.9846 - mae: 8.1567 - val_loss: 219.4627 - val_mae: 8.6398\n",
      "\n",
      "Epoch 08987: val_mae did not improve from 6.28420\n",
      "Epoch 8988/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 78.9637 - mae: 7.1935 - val_loss: 213.6492 - val_mae: 8.2931\n",
      "\n",
      "Epoch 08988: val_mae did not improve from 6.28420\n",
      "Epoch 8989/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 60.3378 - mae: 6.3040 - val_loss: 218.8427 - val_mae: 8.2635\n",
      "\n",
      "Epoch 08989: val_mae did not improve from 6.28420\n",
      "Epoch 8990/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 99.6296 - mae: 7.6878 - val_loss: 208.4180 - val_mae: 8.2276\n",
      "\n",
      "Epoch 08990: val_mae did not improve from 6.28420\n",
      "Epoch 8991/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.4719 - mae: 7.6058 - val_loss: 343.4415 - val_mae: 12.8761\n",
      "\n",
      "Epoch 08991: val_mae did not improve from 6.28420\n",
      "Epoch 8992/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 125.3807 - mae: 8.7363 - val_loss: 211.8872 - val_mae: 8.2658\n",
      "\n",
      "Epoch 08992: val_mae did not improve from 6.28420\n",
      "Epoch 8993/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 98.9870 - mae: 8.0594 - val_loss: 221.4516 - val_mae: 8.6605\n",
      "\n",
      "Epoch 08993: val_mae did not improve from 6.28420\n",
      "Epoch 8994/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 91.9355 - mae: 7.7265 - val_loss: 273.2857 - val_mae: 10.7755\n",
      "\n",
      "Epoch 08994: val_mae did not improve from 6.28420\n",
      "Epoch 8995/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 113.2404 - mae: 8.4386 - val_loss: 231.8925 - val_mae: 9.1778\n",
      "\n",
      "Epoch 08995: val_mae did not improve from 6.28420\n",
      "Epoch 8996/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 88.2911 - mae: 7.3485 - val_loss: 203.9926 - val_mae: 7.8017\n",
      "\n",
      "Epoch 08996: val_mae did not improve from 6.28420\n",
      "Epoch 8997/10000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 74.5798 - mae: 6.8022 - val_loss: 196.3008 - val_mae: 7.5337\n",
      "\n",
      "Epoch 08997: val_mae did not improve from 6.28420\n",
      "Epoch 8998/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 134.0100 - mae: 9.2695 - val_loss: 297.4500 - val_mae: 11.2236\n",
      "\n",
      "Epoch 08998: val_mae did not improve from 6.28420\n",
      "Epoch 8999/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 179.3949 - mae: 10.7798 - val_loss: 254.4796 - val_mae: 9.9992\n",
      "\n",
      "Epoch 08999: val_mae did not improve from 6.28420\n",
      "Epoch 9000/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 118.1708 - mae: 8.7381 - val_loss: 310.7318 - val_mae: 12.1729\n",
      "\n",
      "Epoch 09000: val_mae did not improve from 6.28420\n",
      "Epoch 9001/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 94.5410 - mae: 7.8697 - val_loss: 211.2027 - val_mae: 8.0293\n",
      "\n",
      "Epoch 09001: val_mae did not improve from 6.28420\n",
      "Epoch 9002/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 68.7369 - mae: 6.6744 - val_loss: 282.2413 - val_mae: 10.8690\n",
      "\n",
      "Epoch 09002: val_mae did not improve from 6.28420\n",
      "Epoch 9003/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 87.1224 - mae: 7.4305 - val_loss: 323.7458 - val_mae: 12.1647\n",
      "\n",
      "Epoch 09003: val_mae did not improve from 6.28420\n",
      "Epoch 9004/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 123.6201 - mae: 8.7285 - val_loss: 330.0481 - val_mae: 12.9151\n",
      "\n",
      "Epoch 09004: val_mae did not improve from 6.28420\n",
      "Epoch 9005/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 113.9534 - mae: 8.4996 - val_loss: 221.1351 - val_mae: 8.8229\n",
      "\n",
      "Epoch 09005: val_mae did not improve from 6.28420\n",
      "Epoch 9006/10000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 78.7313 - mae: 7.0829 - val_loss: 189.8680 - val_mae: 7.3888\n",
      "\n",
      "Epoch 09006: val_mae did not improve from 6.28420\n",
      "Epoch 9007/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 79.6058 - mae: 7.0874 - val_loss: 201.2937 - val_mae: 7.8034\n",
      "\n",
      "Epoch 09007: val_mae did not improve from 6.28420\n",
      "Epoch 9008/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 100.0429 - mae: 8.0200 - val_loss: 208.4031 - val_mae: 8.1112\n",
      "\n",
      "Epoch 09008: val_mae did not improve from 6.28420\n",
      "Epoch 9009/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 102.5297 - mae: 8.0937 - val_loss: 219.6110 - val_mae: 8.6566\n",
      "\n",
      "Epoch 09009: val_mae did not improve from 6.28420\n",
      "Epoch 9010/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 109.8395 - mae: 8.4729 - val_loss: 302.0299 - val_mae: 11.7047\n",
      "\n",
      "Epoch 09010: val_mae did not improve from 6.28420\n",
      "Epoch 9011/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 108.2351 - mae: 8.2720 - val_loss: 261.4240 - val_mae: 10.6083\n",
      "\n",
      "Epoch 09011: val_mae did not improve from 6.28420\n",
      "Epoch 9012/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 130.8921 - mae: 8.9792 - val_loss: 337.5120 - val_mae: 13.1986\n",
      "\n",
      "Epoch 09012: val_mae did not improve from 6.28420\n",
      "Epoch 9013/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 153.6666 - mae: 9.5457 - val_loss: 243.5686 - val_mae: 9.9598\n",
      "\n",
      "Epoch 09013: val_mae did not improve from 6.28420\n",
      "Epoch 9014/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 98.3138 - mae: 7.7481 - val_loss: 206.7968 - val_mae: 8.0166\n",
      "\n",
      "Epoch 09014: val_mae did not improve from 6.28420\n",
      "Epoch 9015/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.2906 - mae: 6.8166 - val_loss: 218.6740 - val_mae: 8.4688\n",
      "\n",
      "Epoch 09015: val_mae did not improve from 6.28420\n",
      "Epoch 9016/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 104.6079 - mae: 7.9689 - val_loss: 239.0274 - val_mae: 9.6309\n",
      "\n",
      "Epoch 09016: val_mae did not improve from 6.28420\n",
      "Epoch 9017/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 125.2183 - mae: 8.9136 - val_loss: 215.7655 - val_mae: 8.4223\n",
      "\n",
      "Epoch 09017: val_mae did not improve from 6.28420\n",
      "Epoch 9018/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 105.0568 - mae: 8.1260 - val_loss: 223.5396 - val_mae: 8.7217\n",
      "\n",
      "Epoch 09018: val_mae did not improve from 6.28420\n",
      "Epoch 9019/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 75.6912 - mae: 6.9080 - val_loss: 220.3218 - val_mae: 8.6076\n",
      "\n",
      "Epoch 09019: val_mae did not improve from 6.28420\n",
      "Epoch 9020/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.8696 - mae: 6.8833 - val_loss: 196.7763 - val_mae: 7.4458\n",
      "\n",
      "Epoch 09020: val_mae did not improve from 6.28420\n",
      "Epoch 9021/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 67.0184 - mae: 6.4839 - val_loss: 200.9223 - val_mae: 7.5823\n",
      "\n",
      "Epoch 09021: val_mae did not improve from 6.28420\n",
      "Epoch 9022/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 64.0992 - mae: 6.3609 - val_loss: 265.2988 - val_mae: 10.5530\n",
      "\n",
      "Epoch 09022: val_mae did not improve from 6.28420\n",
      "Epoch 9023/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 106.8860 - mae: 8.1178 - val_loss: 201.2891 - val_mae: 7.5506\n",
      "\n",
      "Epoch 09023: val_mae did not improve from 6.28420\n",
      "Epoch 9024/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 95.1369 - mae: 7.6520 - val_loss: 283.4589 - val_mae: 11.3550\n",
      "\n",
      "Epoch 09024: val_mae did not improve from 6.28420\n",
      "Epoch 9025/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 161.9387 - mae: 10.1386 - val_loss: 372.4412 - val_mae: 14.3192\n",
      "\n",
      "Epoch 09025: val_mae did not improve from 6.28420\n",
      "Epoch 9026/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 101.1429 - mae: 8.0141 - val_loss: 231.1784 - val_mae: 8.7627\n",
      "\n",
      "Epoch 09026: val_mae did not improve from 6.28420\n",
      "Epoch 9027/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 155.8448 - mae: 9.8806 - val_loss: 235.5235 - val_mae: 9.1419\n",
      "\n",
      "Epoch 09027: val_mae did not improve from 6.28420\n",
      "Epoch 9028/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 104.9131 - mae: 8.1958 - val_loss: 219.9354 - val_mae: 8.3541\n",
      "\n",
      "Epoch 09028: val_mae did not improve from 6.28420\n",
      "Epoch 9029/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 83.5012 - mae: 7.1908 - val_loss: 221.1832 - val_mae: 8.6919\n",
      "\n",
      "Epoch 09029: val_mae did not improve from 6.28420\n",
      "Epoch 9030/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 75.0154 - mae: 6.8962 - val_loss: 293.5348 - val_mae: 11.4978\n",
      "\n",
      "Epoch 09030: val_mae did not improve from 6.28420\n",
      "Epoch 9031/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 122.4602 - mae: 8.7147 - val_loss: 232.1289 - val_mae: 9.2564\n",
      "\n",
      "Epoch 09031: val_mae did not improve from 6.28420\n",
      "Epoch 9032/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.4792 - mae: 6.8099 - val_loss: 210.4621 - val_mae: 8.2867\n",
      "\n",
      "Epoch 09032: val_mae did not improve from 6.28420\n",
      "Epoch 9033/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 116.9558 - mae: 8.7240 - val_loss: 222.2256 - val_mae: 8.6658\n",
      "\n",
      "Epoch 09033: val_mae did not improve from 6.28420\n",
      "Epoch 9034/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 115.8720 - mae: 8.5140 - val_loss: 218.5916 - val_mae: 8.2423\n",
      "\n",
      "Epoch 09034: val_mae did not improve from 6.28420\n",
      "Epoch 9035/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 105.8978 - mae: 8.2038 - val_loss: 237.5713 - val_mae: 8.8183\n",
      "\n",
      "Epoch 09035: val_mae did not improve from 6.28420\n",
      "Epoch 9036/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 87.7580 - mae: 7.3233 - val_loss: 233.8420 - val_mae: 8.9974\n",
      "\n",
      "Epoch 09036: val_mae did not improve from 6.28420\n",
      "Epoch 9037/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 78.5271 - mae: 7.1505 - val_loss: 215.6238 - val_mae: 8.5306\n",
      "\n",
      "Epoch 09037: val_mae did not improve from 6.28420\n",
      "Epoch 9038/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 96.9353 - mae: 7.6646 - val_loss: 214.1371 - val_mae: 8.0573\n",
      "\n",
      "Epoch 09038: val_mae did not improve from 6.28420\n",
      "Epoch 9039/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 99.1526 - mae: 7.8224 - val_loss: 206.3031 - val_mae: 7.7674\n",
      "\n",
      "Epoch 09039: val_mae did not improve from 6.28420\n",
      "Epoch 9040/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 76.3652 - mae: 6.8700 - val_loss: 203.3447 - val_mae: 7.6439\n",
      "\n",
      "Epoch 09040: val_mae did not improve from 6.28420\n",
      "Epoch 9041/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 75.7428 - mae: 6.9142 - val_loss: 200.1129 - val_mae: 7.5119\n",
      "\n",
      "Epoch 09041: val_mae did not improve from 6.28420\n",
      "Epoch 9042/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 74.6117 - mae: 6.8383 - val_loss: 268.8065 - val_mae: 10.4025\n",
      "\n",
      "Epoch 09042: val_mae did not improve from 6.28420\n",
      "Epoch 9043/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.8065 - mae: 7.4753 - val_loss: 235.6762 - val_mae: 9.3456\n",
      "\n",
      "Epoch 09043: val_mae did not improve from 6.28420\n",
      "Epoch 9044/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 105.2328 - mae: 8.2911 - val_loss: 262.6807 - val_mae: 10.0657\n",
      "\n",
      "Epoch 09044: val_mae did not improve from 6.28420\n",
      "Epoch 9045/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 104.5065 - mae: 8.1548 - val_loss: 225.7591 - val_mae: 8.8122\n",
      "\n",
      "Epoch 09045: val_mae did not improve from 6.28420\n",
      "Epoch 9046/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 113.1730 - mae: 8.4046 - val_loss: 250.2399 - val_mae: 9.5648\n",
      "\n",
      "Epoch 09046: val_mae did not improve from 6.28420\n",
      "Epoch 9047/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 113.6152 - mae: 8.4605 - val_loss: 213.8961 - val_mae: 8.2160\n",
      "\n",
      "Epoch 09047: val_mae did not improve from 6.28420\n",
      "Epoch 9048/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 106.1462 - mae: 8.2820 - val_loss: 201.9848 - val_mae: 7.7119\n",
      "\n",
      "Epoch 09048: val_mae did not improve from 6.28420\n",
      "Epoch 9049/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 66.5957 - mae: 6.4939 - val_loss: 207.2061 - val_mae: 8.0996\n",
      "\n",
      "Epoch 09049: val_mae did not improve from 6.28420\n",
      "Epoch 9050/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 83.4697 - mae: 7.1824 - val_loss: 225.8124 - val_mae: 8.8810\n",
      "\n",
      "Epoch 09050: val_mae did not improve from 6.28420\n",
      "Epoch 9051/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 97.4425 - mae: 7.8893 - val_loss: 205.9732 - val_mae: 7.8115\n",
      "\n",
      "Epoch 09051: val_mae did not improve from 6.28420\n",
      "Epoch 9052/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 82.2142 - mae: 7.1741 - val_loss: 225.8218 - val_mae: 8.8185\n",
      "\n",
      "Epoch 09052: val_mae did not improve from 6.28420\n",
      "Epoch 9053/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 68.2598 - mae: 6.6147 - val_loss: 195.8480 - val_mae: 7.0835\n",
      "\n",
      "Epoch 09053: val_mae did not improve from 6.28420\n",
      "Epoch 9054/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 67.0392 - mae: 6.3927 - val_loss: 205.5013 - val_mae: 7.7449\n",
      "\n",
      "Epoch 09054: val_mae did not improve from 6.28420\n",
      "Epoch 9055/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.7267 - mae: 6.8314 - val_loss: 304.9792 - val_mae: 12.1490\n",
      "\n",
      "Epoch 09055: val_mae did not improve from 6.28420\n",
      "Epoch 9056/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 97.0847 - mae: 7.8918 - val_loss: 359.2468 - val_mae: 14.1500\n",
      "\n",
      "Epoch 09056: val_mae did not improve from 6.28420\n",
      "Epoch 9057/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 155.6752 - mae: 9.8062 - val_loss: 353.4786 - val_mae: 13.9626\n",
      "\n",
      "Epoch 09057: val_mae did not improve from 6.28420\n",
      "Epoch 9058/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 151.6309 - mae: 9.9085 - val_loss: 224.5974 - val_mae: 8.9209\n",
      "\n",
      "Epoch 09058: val_mae did not improve from 6.28420\n",
      "Epoch 9059/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.8597 - mae: 7.6023 - val_loss: 210.2300 - val_mae: 8.2030\n",
      "\n",
      "Epoch 09059: val_mae did not improve from 6.28420\n",
      "Epoch 9060/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.5303 - mae: 7.0692 - val_loss: 240.0232 - val_mae: 9.7532\n",
      "\n",
      "Epoch 09060: val_mae did not improve from 6.28420\n",
      "Epoch 9061/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 78.1251 - mae: 7.0747 - val_loss: 215.3178 - val_mae: 8.5410\n",
      "\n",
      "Epoch 09061: val_mae did not improve from 6.28420\n",
      "Epoch 9062/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 74.7430 - mae: 6.8656 - val_loss: 208.8946 - val_mae: 7.6655\n",
      "\n",
      "Epoch 09062: val_mae did not improve from 6.28420\n",
      "Epoch 9063/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.6729 - mae: 7.0101 - val_loss: 265.7965 - val_mae: 10.3506\n",
      "\n",
      "Epoch 09063: val_mae did not improve from 6.28420\n",
      "Epoch 9064/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 117.4324 - mae: 8.7017 - val_loss: 253.3948 - val_mae: 10.2588\n",
      "\n",
      "Epoch 09064: val_mae did not improve from 6.28420\n",
      "Epoch 9065/10000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 120.7790 - mae: 8.8370 - val_loss: 290.5521 - val_mae: 10.9798\n",
      "\n",
      "Epoch 09065: val_mae did not improve from 6.28420\n",
      "Epoch 9066/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 103.3536 - mae: 8.1127 - val_loss: 209.2886 - val_mae: 7.9215\n",
      "\n",
      "Epoch 09066: val_mae did not improve from 6.28420\n",
      "Epoch 9067/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 65.3766 - mae: 6.4077 - val_loss: 229.0689 - val_mae: 9.0316\n",
      "\n",
      "Epoch 09067: val_mae did not improve from 6.28420\n",
      "Epoch 9068/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 99.3639 - mae: 7.8649 - val_loss: 239.8060 - val_mae: 9.3314\n",
      "\n",
      "Epoch 09068: val_mae did not improve from 6.28420\n",
      "Epoch 9069/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 115.3455 - mae: 8.5734 - val_loss: 212.5233 - val_mae: 7.7281\n",
      "\n",
      "Epoch 09069: val_mae did not improve from 6.28420\n",
      "Epoch 9070/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.1713 - mae: 6.5922 - val_loss: 220.3049 - val_mae: 8.6073\n",
      "\n",
      "Epoch 09070: val_mae did not improve from 6.28420\n",
      "Epoch 9071/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 62.5010 - mae: 6.3657 - val_loss: 202.6208 - val_mae: 7.5410\n",
      "\n",
      "Epoch 09071: val_mae did not improve from 6.28420\n",
      "Epoch 9072/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 101.6595 - mae: 7.8788 - val_loss: 213.6873 - val_mae: 8.1515\n",
      "\n",
      "Epoch 09072: val_mae did not improve from 6.28420\n",
      "Epoch 9073/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.5958 - mae: 7.0836 - val_loss: 369.5921 - val_mae: 14.3020\n",
      "\n",
      "Epoch 09073: val_mae did not improve from 6.28420\n",
      "Epoch 9074/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 160.5174 - mae: 10.3558 - val_loss: 215.3475 - val_mae: 8.2444\n",
      "\n",
      "Epoch 09074: val_mae did not improve from 6.28420\n",
      "Epoch 9075/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 119.3584 - mae: 8.5956 - val_loss: 247.7786 - val_mae: 9.8777\n",
      "\n",
      "Epoch 09075: val_mae did not improve from 6.28420\n",
      "Epoch 9076/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 86.2637 - mae: 7.4124 - val_loss: 297.3654 - val_mae: 11.8989\n",
      "\n",
      "Epoch 09076: val_mae did not improve from 6.28420\n",
      "Epoch 9077/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 123.6728 - mae: 8.8921 - val_loss: 230.4611 - val_mae: 9.3633\n",
      "\n",
      "Epoch 09077: val_mae did not improve from 6.28420\n",
      "Epoch 9078/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 142.3054 - mae: 9.6482 - val_loss: 225.9092 - val_mae: 9.1050\n",
      "\n",
      "Epoch 09078: val_mae did not improve from 6.28420\n",
      "Epoch 9079/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 161.9938 - mae: 10.0318 - val_loss: 245.0644 - val_mae: 9.7849\n",
      "\n",
      "Epoch 09079: val_mae did not improve from 6.28420\n",
      "Epoch 9080/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 110.8185 - mae: 8.4270 - val_loss: 261.9955 - val_mae: 10.3051\n",
      "\n",
      "Epoch 09080: val_mae did not improve from 6.28420\n",
      "Epoch 9081/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 141.7488 - mae: 9.5771 - val_loss: 270.0099 - val_mae: 10.4292\n",
      "\n",
      "Epoch 09081: val_mae did not improve from 6.28420\n",
      "Epoch 9082/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 120.4520 - mae: 8.6492 - val_loss: 267.1143 - val_mae: 10.5242\n",
      "\n",
      "Epoch 09082: val_mae did not improve from 6.28420\n",
      "Epoch 9083/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 102.2467 - mae: 8.1004 - val_loss: 226.1525 - val_mae: 8.6735\n",
      "\n",
      "Epoch 09083: val_mae did not improve from 6.28420\n",
      "Epoch 9084/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 93.7367 - mae: 7.6775 - val_loss: 250.4723 - val_mae: 10.0107\n",
      "\n",
      "Epoch 09084: val_mae did not improve from 6.28420\n",
      "Epoch 9085/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 84.9345 - mae: 7.2633 - val_loss: 217.0572 - val_mae: 8.5136\n",
      "\n",
      "Epoch 09085: val_mae did not improve from 6.28420\n",
      "Epoch 9086/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 92.1105 - mae: 7.4849 - val_loss: 205.5627 - val_mae: 7.7323\n",
      "\n",
      "Epoch 09086: val_mae did not improve from 6.28420\n",
      "Epoch 9087/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 66.7258 - mae: 6.5287 - val_loss: 232.6780 - val_mae: 9.0363\n",
      "\n",
      "Epoch 09087: val_mae did not improve from 6.28420\n",
      "Epoch 9088/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 96.9005 - mae: 7.7186 - val_loss: 218.0176 - val_mae: 8.1179\n",
      "\n",
      "Epoch 09088: val_mae did not improve from 6.28420\n",
      "Epoch 9089/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 82.0951 - mae: 7.2728 - val_loss: 223.3831 - val_mae: 8.7312\n",
      "\n",
      "Epoch 09089: val_mae did not improve from 6.28420\n",
      "Epoch 9090/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 65.2807 - mae: 6.3268 - val_loss: 213.6934 - val_mae: 8.3179\n",
      "\n",
      "Epoch 09090: val_mae did not improve from 6.28420\n",
      "Epoch 9091/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 87.1532 - mae: 7.3315 - val_loss: 287.5027 - val_mae: 11.4405\n",
      "\n",
      "Epoch 09091: val_mae did not improve from 6.28420\n",
      "Epoch 9092/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 89.8841 - mae: 7.3813 - val_loss: 205.4099 - val_mae: 7.8475\n",
      "\n",
      "Epoch 09092: val_mae did not improve from 6.28420\n",
      "Epoch 9093/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 80.8961 - mae: 7.1409 - val_loss: 235.5958 - val_mae: 9.0613\n",
      "\n",
      "Epoch 09093: val_mae did not improve from 6.28420\n",
      "Epoch 9094/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 92.2798 - mae: 7.7620 - val_loss: 204.6713 - val_mae: 7.6254\n",
      "\n",
      "Epoch 09094: val_mae did not improve from 6.28420\n",
      "Epoch 9095/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 83.9423 - mae: 7.3452 - val_loss: 207.2954 - val_mae: 7.9555\n",
      "\n",
      "Epoch 09095: val_mae did not improve from 6.28420\n",
      "Epoch 9096/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.9385 - mae: 6.5675 - val_loss: 215.3423 - val_mae: 8.5039\n",
      "\n",
      "Epoch 09096: val_mae did not improve from 6.28420\n",
      "Epoch 9097/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 110.7524 - mae: 8.2676 - val_loss: 228.7488 - val_mae: 9.0503\n",
      "\n",
      "Epoch 09097: val_mae did not improve from 6.28420\n",
      "Epoch 9098/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 85.5382 - mae: 7.2252 - val_loss: 199.0970 - val_mae: 7.5013\n",
      "\n",
      "Epoch 09098: val_mae did not improve from 6.28420\n",
      "Epoch 9099/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.5755 - mae: 6.7616 - val_loss: 240.0784 - val_mae: 9.9112\n",
      "\n",
      "Epoch 09099: val_mae did not improve from 6.28420\n",
      "Epoch 9100/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 94.1175 - mae: 7.5679 - val_loss: 325.1777 - val_mae: 12.6666\n",
      "\n",
      "Epoch 09100: val_mae did not improve from 6.28420\n",
      "Epoch 9101/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 75.6177 - mae: 6.9528 - val_loss: 263.4859 - val_mae: 10.8211\n",
      "\n",
      "Epoch 09101: val_mae did not improve from 6.28420\n",
      "Epoch 9102/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 92.1642 - mae: 7.6491 - val_loss: 231.3924 - val_mae: 9.2782\n",
      "\n",
      "Epoch 09102: val_mae did not improve from 6.28420\n",
      "Epoch 9103/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 113.4164 - mae: 8.5924 - val_loss: 354.0137 - val_mae: 13.4351\n",
      "\n",
      "Epoch 09103: val_mae did not improve from 6.28420\n",
      "Epoch 9104/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 165.0249 - mae: 10.1155 - val_loss: 235.3616 - val_mae: 9.5348\n",
      "\n",
      "Epoch 09104: val_mae did not improve from 6.28420\n",
      "Epoch 9105/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 145.2745 - mae: 9.5378 - val_loss: 328.3641 - val_mae: 13.0589\n",
      "\n",
      "Epoch 09105: val_mae did not improve from 6.28420\n",
      "Epoch 9106/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 125.8359 - mae: 9.0707 - val_loss: 233.9066 - val_mae: 9.3641\n",
      "\n",
      "Epoch 09106: val_mae did not improve from 6.28420\n",
      "Epoch 9107/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 89.1415 - mae: 7.4493 - val_loss: 231.8486 - val_mae: 9.2052\n",
      "\n",
      "Epoch 09107: val_mae did not improve from 6.28420\n",
      "Epoch 9108/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 92.0352 - mae: 7.5868 - val_loss: 212.0002 - val_mae: 8.1550\n",
      "\n",
      "Epoch 09108: val_mae did not improve from 6.28420\n",
      "Epoch 9109/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 62.5932 - mae: 6.2200 - val_loss: 207.2647 - val_mae: 7.7326\n",
      "\n",
      "Epoch 09109: val_mae did not improve from 6.28420\n",
      "Epoch 9110/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 74.5413 - mae: 6.6545 - val_loss: 209.1624 - val_mae: 8.1070\n",
      "\n",
      "Epoch 09110: val_mae did not improve from 6.28420\n",
      "Epoch 9111/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 92.8643 - mae: 7.4309 - val_loss: 208.0665 - val_mae: 8.0429\n",
      "\n",
      "Epoch 09111: val_mae did not improve from 6.28420\n",
      "Epoch 9112/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 82.2529 - mae: 7.1434 - val_loss: 237.2263 - val_mae: 9.3187\n",
      "\n",
      "Epoch 09112: val_mae did not improve from 6.28420\n",
      "Epoch 9113/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 115.9128 - mae: 8.6016 - val_loss: 224.0933 - val_mae: 8.7876\n",
      "\n",
      "Epoch 09113: val_mae did not improve from 6.28420\n",
      "Epoch 9114/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.0797 - mae: 7.4319 - val_loss: 231.1272 - val_mae: 9.0057\n",
      "\n",
      "Epoch 09114: val_mae did not improve from 6.28420\n",
      "Epoch 9115/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 89.7959 - mae: 7.4205 - val_loss: 296.8321 - val_mae: 11.7472\n",
      "\n",
      "Epoch 09115: val_mae did not improve from 6.28420\n",
      "Epoch 9116/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 91.6167 - mae: 7.6830 - val_loss: 228.2896 - val_mae: 9.0302\n",
      "\n",
      "Epoch 09116: val_mae did not improve from 6.28420\n",
      "Epoch 9117/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 88.0905 - mae: 7.5229 - val_loss: 211.7992 - val_mae: 8.3304\n",
      "\n",
      "Epoch 09117: val_mae did not improve from 6.28420\n",
      "Epoch 9118/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 92.4733 - mae: 7.6560 - val_loss: 224.4129 - val_mae: 9.0337\n",
      "\n",
      "Epoch 09118: val_mae did not improve from 6.28420\n",
      "Epoch 9119/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 86.7288 - mae: 7.5739 - val_loss: 229.0040 - val_mae: 9.3591\n",
      "\n",
      "Epoch 09119: val_mae did not improve from 6.28420\n",
      "Epoch 9120/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 81.9854 - mae: 7.2599 - val_loss: 209.0396 - val_mae: 8.2173\n",
      "\n",
      "Epoch 09120: val_mae did not improve from 6.28420\n",
      "Epoch 9121/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 83.0559 - mae: 7.1977 - val_loss: 281.9870 - val_mae: 11.3209\n",
      "\n",
      "Epoch 09121: val_mae did not improve from 6.28420\n",
      "Epoch 9122/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 106.0850 - mae: 8.2647 - val_loss: 232.8345 - val_mae: 9.0872\n",
      "\n",
      "Epoch 09122: val_mae did not improve from 6.28420\n",
      "Epoch 9123/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 101.1159 - mae: 7.8197 - val_loss: 217.7256 - val_mae: 8.2199\n",
      "\n",
      "Epoch 09123: val_mae did not improve from 6.28420\n",
      "Epoch 9124/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 88.2660 - mae: 7.4769 - val_loss: 201.4363 - val_mae: 7.8233\n",
      "\n",
      "Epoch 09124: val_mae did not improve from 6.28420\n",
      "Epoch 9125/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 77.3277 - mae: 7.0274 - val_loss: 202.8127 - val_mae: 7.7089\n",
      "\n",
      "Epoch 09125: val_mae did not improve from 6.28420\n",
      "Epoch 9126/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 109.5986 - mae: 8.4107 - val_loss: 283.1602 - val_mae: 11.0885\n",
      "\n",
      "Epoch 09126: val_mae did not improve from 6.28420\n",
      "Epoch 9127/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.7227 - mae: 6.9567 - val_loss: 246.4687 - val_mae: 9.7818\n",
      "\n",
      "Epoch 09127: val_mae did not improve from 6.28420\n",
      "Epoch 9128/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 86.9906 - mae: 7.1877 - val_loss: 208.9173 - val_mae: 7.7569\n",
      "\n",
      "Epoch 09128: val_mae did not improve from 6.28420\n",
      "Epoch 9129/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 81.1152 - mae: 7.2942 - val_loss: 228.2355 - val_mae: 9.2850\n",
      "\n",
      "Epoch 09129: val_mae did not improve from 6.28420\n",
      "Epoch 9130/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 72.6258 - mae: 6.6187 - val_loss: 202.3564 - val_mae: 8.0422\n",
      "\n",
      "Epoch 09130: val_mae did not improve from 6.28420\n",
      "Epoch 9131/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 68.9689 - mae: 6.4917 - val_loss: 228.7492 - val_mae: 9.1302\n",
      "\n",
      "Epoch 09131: val_mae did not improve from 6.28420\n",
      "Epoch 9132/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 96.1965 - mae: 7.5829 - val_loss: 210.5364 - val_mae: 8.5140\n",
      "\n",
      "Epoch 09132: val_mae did not improve from 6.28420\n",
      "Epoch 9133/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 67.0002 - mae: 6.5279 - val_loss: 214.3248 - val_mae: 8.5825\n",
      "\n",
      "Epoch 09133: val_mae did not improve from 6.28420\n",
      "Epoch 9134/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 101.6387 - mae: 7.6894 - val_loss: 251.1234 - val_mae: 10.2305\n",
      "\n",
      "Epoch 09134: val_mae did not improve from 6.28420\n",
      "Epoch 9135/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 120.6750 - mae: 8.7374 - val_loss: 288.6636 - val_mae: 11.2942\n",
      "\n",
      "Epoch 09135: val_mae did not improve from 6.28420\n",
      "Epoch 9136/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 137.4882 - mae: 9.1828 - val_loss: 201.0296 - val_mae: 7.9428\n",
      "\n",
      "Epoch 09136: val_mae did not improve from 6.28420\n",
      "Epoch 9137/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 95.2865 - mae: 7.7811 - val_loss: 217.7205 - val_mae: 8.7525\n",
      "\n",
      "Epoch 09137: val_mae did not improve from 6.28420\n",
      "Epoch 9138/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 95.6165 - mae: 7.4700 - val_loss: 200.6861 - val_mae: 7.6085\n",
      "\n",
      "Epoch 09138: val_mae did not improve from 6.28420\n",
      "Epoch 9139/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 67.4163 - mae: 6.4900 - val_loss: 211.6104 - val_mae: 8.4285\n",
      "\n",
      "Epoch 09139: val_mae did not improve from 6.28420\n",
      "Epoch 9140/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 80.4395 - mae: 7.1059 - val_loss: 197.4189 - val_mae: 7.5318\n",
      "\n",
      "Epoch 09140: val_mae did not improve from 6.28420\n",
      "Epoch 9141/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 95.0272 - mae: 7.8460 - val_loss: 228.5654 - val_mae: 8.9118\n",
      "\n",
      "Epoch 09141: val_mae did not improve from 6.28420\n",
      "Epoch 9142/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 100.0496 - mae: 8.0331 - val_loss: 246.2023 - val_mae: 9.9706\n",
      "\n",
      "Epoch 09142: val_mae did not improve from 6.28420\n",
      "Epoch 9143/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 85.3156 - mae: 7.4455 - val_loss: 322.9149 - val_mae: 12.9663\n",
      "\n",
      "Epoch 09143: val_mae did not improve from 6.28420\n",
      "Epoch 9144/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 128.9599 - mae: 8.9966 - val_loss: 206.6833 - val_mae: 8.1647\n",
      "\n",
      "Epoch 09144: val_mae did not improve from 6.28420\n",
      "Epoch 9145/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 88.5031 - mae: 7.4471 - val_loss: 256.1484 - val_mae: 10.5812\n",
      "\n",
      "Epoch 09145: val_mae did not improve from 6.28420\n",
      "Epoch 9146/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 96.2589 - mae: 7.8530 - val_loss: 256.7299 - val_mae: 10.5444\n",
      "\n",
      "Epoch 09146: val_mae did not improve from 6.28420\n",
      "Epoch 9147/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 123.5271 - mae: 9.1246 - val_loss: 229.3373 - val_mae: 9.2100\n",
      "\n",
      "Epoch 09147: val_mae did not improve from 6.28420\n",
      "Epoch 9148/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 146.5070 - mae: 9.3743 - val_loss: 248.6128 - val_mae: 9.8024\n",
      "\n",
      "Epoch 09148: val_mae did not improve from 6.28420\n",
      "Epoch 9149/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 123.8253 - mae: 8.7506 - val_loss: 245.2319 - val_mae: 9.9604\n",
      "\n",
      "Epoch 09149: val_mae did not improve from 6.28420\n",
      "Epoch 9150/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 136.5631 - mae: 9.2530 - val_loss: 214.8257 - val_mae: 8.2858\n",
      "\n",
      "Epoch 09150: val_mae did not improve from 6.28420\n",
      "Epoch 9151/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 115.7572 - mae: 8.5773 - val_loss: 247.6713 - val_mae: 9.8668\n",
      "\n",
      "Epoch 09151: val_mae did not improve from 6.28420\n",
      "Epoch 9152/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 110.0378 - mae: 8.1552 - val_loss: 318.4830 - val_mae: 12.8202\n",
      "\n",
      "Epoch 09152: val_mae did not improve from 6.28420\n",
      "Epoch 9153/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 137.3530 - mae: 9.4944 - val_loss: 205.1052 - val_mae: 7.7084\n",
      "\n",
      "Epoch 09153: val_mae did not improve from 6.28420\n",
      "Epoch 9154/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 70.9665 - mae: 6.6351 - val_loss: 241.5505 - val_mae: 9.6911\n",
      "\n",
      "Epoch 09154: val_mae did not improve from 6.28420\n",
      "Epoch 9155/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 111.2317 - mae: 8.4479 - val_loss: 243.7460 - val_mae: 9.7234\n",
      "\n",
      "Epoch 09155: val_mae did not improve from 6.28420\n",
      "Epoch 9156/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 79.9325 - mae: 7.0422 - val_loss: 249.5737 - val_mae: 10.0411\n",
      "\n",
      "Epoch 09156: val_mae did not improve from 6.28420\n",
      "Epoch 9157/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 114.5769 - mae: 8.4438 - val_loss: 227.5156 - val_mae: 9.1225\n",
      "\n",
      "Epoch 09157: val_mae did not improve from 6.28420\n",
      "Epoch 9158/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 95.3818 - mae: 7.8416 - val_loss: 241.0594 - val_mae: 9.6482\n",
      "\n",
      "Epoch 09158: val_mae did not improve from 6.28420\n",
      "Epoch 9159/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 88.9043 - mae: 7.3470 - val_loss: 208.5775 - val_mae: 8.0337\n",
      "\n",
      "Epoch 09159: val_mae did not improve from 6.28420\n",
      "Epoch 9160/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 134.8495 - mae: 9.2348 - val_loss: 276.6977 - val_mae: 10.8139\n",
      "\n",
      "Epoch 09160: val_mae did not improve from 6.28420\n",
      "Epoch 9161/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 98.7585 - mae: 8.0821 - val_loss: 215.7363 - val_mae: 8.3541\n",
      "\n",
      "Epoch 09161: val_mae did not improve from 6.28420\n",
      "Epoch 9162/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 101.2490 - mae: 8.0716 - val_loss: 254.4376 - val_mae: 9.9901\n",
      "\n",
      "Epoch 09162: val_mae did not improve from 6.28420\n",
      "Epoch 9163/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 85.8409 - mae: 7.3579 - val_loss: 223.7907 - val_mae: 8.7459\n",
      "\n",
      "Epoch 09163: val_mae did not improve from 6.28420\n",
      "Epoch 9164/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 67.9811 - mae: 6.4588 - val_loss: 292.8742 - val_mae: 11.7875\n",
      "\n",
      "Epoch 09164: val_mae did not improve from 6.28420\n",
      "Epoch 9165/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 97.7308 - mae: 7.9239 - val_loss: 232.0829 - val_mae: 9.2471\n",
      "\n",
      "Epoch 09165: val_mae did not improve from 6.28420\n",
      "Epoch 9166/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 100.2471 - mae: 8.1057 - val_loss: 196.9124 - val_mae: 7.6542\n",
      "\n",
      "Epoch 09166: val_mae did not improve from 6.28420\n",
      "Epoch 9167/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 96.0997 - mae: 7.6754 - val_loss: 222.3555 - val_mae: 8.6215\n",
      "\n",
      "Epoch 09167: val_mae did not improve from 6.28420\n",
      "Epoch 9168/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 94.2349 - mae: 7.6284 - val_loss: 208.8850 - val_mae: 8.2269\n",
      "\n",
      "Epoch 09168: val_mae did not improve from 6.28420\n",
      "Epoch 9169/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 80.1255 - mae: 7.2516 - val_loss: 234.2135 - val_mae: 9.2842\n",
      "\n",
      "Epoch 09169: val_mae did not improve from 6.28420\n",
      "Epoch 9170/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 100.3287 - mae: 8.0367 - val_loss: 235.9099 - val_mae: 9.1044\n",
      "\n",
      "Epoch 09170: val_mae did not improve from 6.28420\n",
      "Epoch 9171/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 101.7143 - mae: 8.0079 - val_loss: 209.7362 - val_mae: 7.9497\n",
      "\n",
      "Epoch 09171: val_mae did not improve from 6.28420\n",
      "Epoch 9172/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 101.5848 - mae: 8.0888 - val_loss: 203.3807 - val_mae: 8.1481\n",
      "\n",
      "Epoch 09172: val_mae did not improve from 6.28420\n",
      "Epoch 9173/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 70.1532 - mae: 6.7157 - val_loss: 203.9240 - val_mae: 8.0537\n",
      "\n",
      "Epoch 09173: val_mae did not improve from 6.28420\n",
      "Epoch 9174/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 83.6450 - mae: 7.3744 - val_loss: 203.9525 - val_mae: 7.8112\n",
      "\n",
      "Epoch 09174: val_mae did not improve from 6.28420\n",
      "Epoch 9175/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 103.1380 - mae: 7.9965 - val_loss: 225.1138 - val_mae: 8.8552\n",
      "\n",
      "Epoch 09175: val_mae did not improve from 6.28420\n",
      "Epoch 9176/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 105.6731 - mae: 8.0234 - val_loss: 208.0655 - val_mae: 8.0606\n",
      "\n",
      "Epoch 09176: val_mae did not improve from 6.28420\n",
      "Epoch 9177/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 65.4918 - mae: 6.4055 - val_loss: 195.8392 - val_mae: 7.5448\n",
      "\n",
      "Epoch 09177: val_mae did not improve from 6.28420\n",
      "Epoch 9178/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 110.6887 - mae: 8.3722 - val_loss: 231.3310 - val_mae: 9.3888\n",
      "\n",
      "Epoch 09178: val_mae did not improve from 6.28420\n",
      "Epoch 9179/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 92.3944 - mae: 7.8307 - val_loss: 247.1848 - val_mae: 9.8725\n",
      "\n",
      "Epoch 09179: val_mae did not improve from 6.28420\n",
      "Epoch 9180/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 124.8794 - mae: 8.9996 - val_loss: 262.5905 - val_mae: 10.3603\n",
      "\n",
      "Epoch 09180: val_mae did not improve from 6.28420\n",
      "Epoch 9181/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 98.1951 - mae: 7.8674 - val_loss: 213.2813 - val_mae: 8.2655\n",
      "\n",
      "Epoch 09181: val_mae did not improve from 6.28420\n",
      "Epoch 9182/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.0300 - mae: 6.5735 - val_loss: 197.5238 - val_mae: 7.3352\n",
      "\n",
      "Epoch 09182: val_mae did not improve from 6.28420\n",
      "Epoch 9183/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.9202 - mae: 7.1398 - val_loss: 288.7169 - val_mae: 11.2967\n",
      "\n",
      "Epoch 09183: val_mae did not improve from 6.28420\n",
      "Epoch 9184/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 141.4651 - mae: 9.5745 - val_loss: 236.6088 - val_mae: 9.4006\n",
      "\n",
      "Epoch 09184: val_mae did not improve from 6.28420\n",
      "Epoch 9185/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 116.2550 - mae: 8.6013 - val_loss: 229.8738 - val_mae: 9.0662\n",
      "\n",
      "Epoch 09185: val_mae did not improve from 6.28420\n",
      "Epoch 9186/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.0180 - mae: 7.3016 - val_loss: 251.7061 - val_mae: 10.1660\n",
      "\n",
      "Epoch 09186: val_mae did not improve from 6.28420\n",
      "Epoch 9187/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 149.5311 - mae: 9.7924 - val_loss: 250.8471 - val_mae: 9.9636\n",
      "\n",
      "Epoch 09187: val_mae did not improve from 6.28420\n",
      "Epoch 9188/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 116.0932 - mae: 8.7149 - val_loss: 263.3199 - val_mae: 10.3555\n",
      "\n",
      "Epoch 09188: val_mae did not improve from 6.28420\n",
      "Epoch 9189/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 105.1648 - mae: 8.2391 - val_loss: 227.3052 - val_mae: 9.0195\n",
      "\n",
      "Epoch 09189: val_mae did not improve from 6.28420\n",
      "Epoch 9190/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 88.6289 - mae: 7.3231 - val_loss: 209.1662 - val_mae: 7.9598\n",
      "\n",
      "Epoch 09190: val_mae did not improve from 6.28420\n",
      "Epoch 9191/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 105.1033 - mae: 8.1953 - val_loss: 220.9916 - val_mae: 8.1401\n",
      "\n",
      "Epoch 09191: val_mae did not improve from 6.28420\n",
      "Epoch 9192/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.2974 - mae: 7.5225 - val_loss: 248.5796 - val_mae: 9.7381\n",
      "\n",
      "Epoch 09192: val_mae did not improve from 6.28420\n",
      "Epoch 9193/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 84.6070 - mae: 7.4177 - val_loss: 213.9675 - val_mae: 8.1460\n",
      "\n",
      "Epoch 09193: val_mae did not improve from 6.28420\n",
      "Epoch 9194/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 125.0634 - mae: 9.0020 - val_loss: 226.5232 - val_mae: 8.8696\n",
      "\n",
      "Epoch 09194: val_mae did not improve from 6.28420\n",
      "Epoch 9195/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 96.4343 - mae: 7.7540 - val_loss: 250.3043 - val_mae: 9.6164\n",
      "\n",
      "Epoch 09195: val_mae did not improve from 6.28420\n",
      "Epoch 9196/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 129.2377 - mae: 9.0057 - val_loss: 215.2264 - val_mae: 8.0835\n",
      "\n",
      "Epoch 09196: val_mae did not improve from 6.28420\n",
      "Epoch 9197/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.3504 - mae: 7.0552 - val_loss: 241.4428 - val_mae: 9.4678\n",
      "\n",
      "Epoch 09197: val_mae did not improve from 6.28420\n",
      "Epoch 9198/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 70.0609 - mae: 6.5743 - val_loss: 239.8818 - val_mae: 9.5706\n",
      "\n",
      "Epoch 09198: val_mae did not improve from 6.28420\n",
      "Epoch 9199/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 88.6304 - mae: 7.6467 - val_loss: 224.8624 - val_mae: 8.9762\n",
      "\n",
      "Epoch 09199: val_mae did not improve from 6.28420\n",
      "Epoch 9200/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.4189 - mae: 6.6946 - val_loss: 223.8119 - val_mae: 8.6170\n",
      "\n",
      "Epoch 09200: val_mae did not improve from 6.28420\n",
      "Epoch 9201/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 78.3290 - mae: 7.0282 - val_loss: 219.3984 - val_mae: 8.7838\n",
      "\n",
      "Epoch 09201: val_mae did not improve from 6.28420\n",
      "Epoch 9202/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 79.7351 - mae: 7.0535 - val_loss: 220.4463 - val_mae: 8.6008\n",
      "\n",
      "Epoch 09202: val_mae did not improve from 6.28420\n",
      "Epoch 9203/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 73.4262 - mae: 6.7246 - val_loss: 195.9286 - val_mae: 7.3916\n",
      "\n",
      "Epoch 09203: val_mae did not improve from 6.28420\n",
      "Epoch 9204/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 72.4470 - mae: 6.8753 - val_loss: 233.2457 - val_mae: 9.4756\n",
      "\n",
      "Epoch 09204: val_mae did not improve from 6.28420\n",
      "Epoch 9205/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 74.9657 - mae: 6.8688 - val_loss: 210.7974 - val_mae: 8.4264\n",
      "\n",
      "Epoch 09205: val_mae did not improve from 6.28420\n",
      "Epoch 9206/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 81.2863 - mae: 7.2517 - val_loss: 194.8790 - val_mae: 7.3099\n",
      "\n",
      "Epoch 09206: val_mae did not improve from 6.28420\n",
      "Epoch 9207/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 76.8439 - mae: 7.0681 - val_loss: 208.1185 - val_mae: 8.1035\n",
      "\n",
      "Epoch 09207: val_mae did not improve from 6.28420\n",
      "Epoch 9208/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 66.3973 - mae: 6.3992 - val_loss: 203.7336 - val_mae: 8.1783\n",
      "\n",
      "Epoch 09208: val_mae did not improve from 6.28420\n",
      "Epoch 9209/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 91.6915 - mae: 7.5798 - val_loss: 198.8287 - val_mae: 7.7958\n",
      "\n",
      "Epoch 09209: val_mae did not improve from 6.28420\n",
      "Epoch 9210/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 65.2366 - mae: 6.3633 - val_loss: 232.7797 - val_mae: 9.5217\n",
      "\n",
      "Epoch 09210: val_mae did not improve from 6.28420\n",
      "Epoch 9211/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 79.6601 - mae: 7.0742 - val_loss: 205.3548 - val_mae: 8.1585\n",
      "\n",
      "Epoch 09211: val_mae did not improve from 6.28420\n",
      "Epoch 9212/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 76.3663 - mae: 7.0814 - val_loss: 208.2215 - val_mae: 8.3365\n",
      "\n",
      "Epoch 09212: val_mae did not improve from 6.28420\n",
      "Epoch 9213/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 67.5079 - mae: 6.6022 - val_loss: 197.2208 - val_mae: 7.7327\n",
      "\n",
      "Epoch 09213: val_mae did not improve from 6.28420\n",
      "Epoch 9214/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 100.2013 - mae: 7.9768 - val_loss: 289.7288 - val_mae: 12.0364\n",
      "\n",
      "Epoch 09214: val_mae did not improve from 6.28420\n",
      "Epoch 9215/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 103.4559 - mae: 8.2682 - val_loss: 225.7263 - val_mae: 9.1029\n",
      "\n",
      "Epoch 09215: val_mae did not improve from 6.28420\n",
      "Epoch 9216/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 121.0694 - mae: 8.8782 - val_loss: 208.7460 - val_mae: 8.2144\n",
      "\n",
      "Epoch 09216: val_mae did not improve from 6.28420\n",
      "Epoch 9217/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 81.7759 - mae: 7.2617 - val_loss: 259.7882 - val_mae: 10.4073\n",
      "\n",
      "Epoch 09217: val_mae did not improve from 6.28420\n",
      "Epoch 9218/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 136.6383 - mae: 9.3543 - val_loss: 231.7031 - val_mae: 9.1235\n",
      "\n",
      "Epoch 09218: val_mae did not improve from 6.28420\n",
      "Epoch 9219/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 99.4573 - mae: 7.9636 - val_loss: 202.7914 - val_mae: 7.8573\n",
      "\n",
      "Epoch 09219: val_mae did not improve from 6.28420\n",
      "Epoch 9220/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 82.7368 - mae: 7.1132 - val_loss: 251.9566 - val_mae: 10.0581\n",
      "\n",
      "Epoch 09220: val_mae did not improve from 6.28420\n",
      "Epoch 9221/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 96.3922 - mae: 7.9681 - val_loss: 214.8379 - val_mae: 8.7167\n",
      "\n",
      "Epoch 09221: val_mae did not improve from 6.28420\n",
      "Epoch 9222/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 114.5018 - mae: 8.1986 - val_loss: 211.1155 - val_mae: 8.5557\n",
      "\n",
      "Epoch 09222: val_mae did not improve from 6.28420\n",
      "Epoch 9223/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 102.6041 - mae: 8.2235 - val_loss: 198.0707 - val_mae: 7.7343\n",
      "\n",
      "Epoch 09223: val_mae did not improve from 6.28420\n",
      "Epoch 9224/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 97.2249 - mae: 8.0949 - val_loss: 206.2715 - val_mae: 8.2099\n",
      "\n",
      "Epoch 09224: val_mae did not improve from 6.28420\n",
      "Epoch 9225/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 105.1016 - mae: 7.8576 - val_loss: 204.6519 - val_mae: 8.1149\n",
      "\n",
      "Epoch 09225: val_mae did not improve from 6.28420\n",
      "Epoch 9226/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 79.7034 - mae: 7.0980 - val_loss: 229.9334 - val_mae: 9.1343\n",
      "\n",
      "Epoch 09226: val_mae did not improve from 6.28420\n",
      "Epoch 9227/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 82.9631 - mae: 7.2419 - val_loss: 206.1740 - val_mae: 8.2480\n",
      "\n",
      "Epoch 09227: val_mae did not improve from 6.28420\n",
      "Epoch 9228/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 98.7883 - mae: 7.7516 - val_loss: 199.1069 - val_mae: 7.8239\n",
      "\n",
      "Epoch 09228: val_mae did not improve from 6.28420\n",
      "Epoch 9229/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 72.1065 - mae: 6.6588 - val_loss: 249.1856 - val_mae: 9.8040\n",
      "\n",
      "Epoch 09229: val_mae did not improve from 6.28420\n",
      "Epoch 9230/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 76.0681 - mae: 6.9228 - val_loss: 216.5506 - val_mae: 8.6120\n",
      "\n",
      "Epoch 09230: val_mae did not improve from 6.28420\n",
      "Epoch 9231/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.6702 - mae: 7.2865 - val_loss: 234.8611 - val_mae: 9.3219\n",
      "\n",
      "Epoch 09231: val_mae did not improve from 6.28420\n",
      "Epoch 9232/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 126.6172 - mae: 8.9299 - val_loss: 332.4099 - val_mae: 12.7251\n",
      "\n",
      "Epoch 09232: val_mae did not improve from 6.28420\n",
      "Epoch 9233/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 146.7085 - mae: 9.6271 - val_loss: 247.3864 - val_mae: 10.1150\n",
      "\n",
      "Epoch 09233: val_mae did not improve from 6.28420\n",
      "Epoch 9234/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 97.5714 - mae: 7.5321 - val_loss: 198.6848 - val_mae: 7.8083\n",
      "\n",
      "Epoch 09234: val_mae did not improve from 6.28420\n",
      "Epoch 9235/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 110.3808 - mae: 8.5541 - val_loss: 227.3578 - val_mae: 8.8977\n",
      "\n",
      "Epoch 09235: val_mae did not improve from 6.28420\n",
      "Epoch 9236/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 114.0973 - mae: 8.5433 - val_loss: 221.5915 - val_mae: 8.6782\n",
      "\n",
      "Epoch 09236: val_mae did not improve from 6.28420\n",
      "Epoch 9237/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 99.5728 - mae: 7.7027 - val_loss: 232.6647 - val_mae: 9.5645\n",
      "\n",
      "Epoch 09237: val_mae did not improve from 6.28420\n",
      "Epoch 9238/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 68.8474 - mae: 6.5469 - val_loss: 298.1605 - val_mae: 12.2047\n",
      "\n",
      "Epoch 09238: val_mae did not improve from 6.28420\n",
      "Epoch 9239/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 130.3352 - mae: 9.0733 - val_loss: 210.9035 - val_mae: 8.3898\n",
      "\n",
      "Epoch 09239: val_mae did not improve from 6.28420\n",
      "Epoch 9240/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 74.0743 - mae: 6.9190 - val_loss: 227.3489 - val_mae: 9.1517\n",
      "\n",
      "Epoch 09240: val_mae did not improve from 6.28420\n",
      "Epoch 9241/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 114.1696 - mae: 8.6628 - val_loss: 211.4720 - val_mae: 8.3618\n",
      "\n",
      "Epoch 09241: val_mae did not improve from 6.28420\n",
      "Epoch 9242/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 90.6824 - mae: 7.6481 - val_loss: 239.8127 - val_mae: 9.9823\n",
      "\n",
      "Epoch 09242: val_mae did not improve from 6.28420\n",
      "Epoch 9243/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.7138 - mae: 6.8078 - val_loss: 189.6582 - val_mae: 7.2355\n",
      "\n",
      "Epoch 09243: val_mae did not improve from 6.28420\n",
      "Epoch 9244/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 61.9308 - mae: 6.2536 - val_loss: 295.0288 - val_mae: 12.4457\n",
      "\n",
      "Epoch 09244: val_mae did not improve from 6.28420\n",
      "Epoch 9245/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 72.1500 - mae: 6.7968 - val_loss: 199.8971 - val_mae: 7.9282\n",
      "\n",
      "Epoch 09245: val_mae did not improve from 6.28420\n",
      "Epoch 9246/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 70.8160 - mae: 6.7092 - val_loss: 236.9416 - val_mae: 9.4815\n",
      "\n",
      "Epoch 09246: val_mae did not improve from 6.28420\n",
      "Epoch 9247/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 109.2118 - mae: 8.5945 - val_loss: 216.3459 - val_mae: 8.5577\n",
      "\n",
      "Epoch 09247: val_mae did not improve from 6.28420\n",
      "Epoch 9248/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 60.1740 - mae: 6.2372 - val_loss: 198.8566 - val_mae: 7.7855\n",
      "\n",
      "Epoch 09248: val_mae did not improve from 6.28420\n",
      "Epoch 9249/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 100.2160 - mae: 7.9232 - val_loss: 210.3164 - val_mae: 8.3518\n",
      "\n",
      "Epoch 09249: val_mae did not improve from 6.28420\n",
      "Epoch 9250/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 75.2716 - mae: 6.8069 - val_loss: 215.1201 - val_mae: 8.5568\n",
      "\n",
      "Epoch 09250: val_mae did not improve from 6.28420\n",
      "Epoch 9251/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 144.3754 - mae: 9.5557 - val_loss: 209.8792 - val_mae: 8.0834\n",
      "\n",
      "Epoch 09251: val_mae did not improve from 6.28420\n",
      "Epoch 9252/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 140.9037 - mae: 9.5867 - val_loss: 278.8577 - val_mae: 11.2537\n",
      "\n",
      "Epoch 09252: val_mae did not improve from 6.28420\n",
      "Epoch 9253/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 125.6202 - mae: 8.9229 - val_loss: 238.5798 - val_mae: 9.5885\n",
      "\n",
      "Epoch 09253: val_mae did not improve from 6.28420\n",
      "Epoch 9254/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 113.2305 - mae: 8.3998 - val_loss: 214.0183 - val_mae: 8.4747\n",
      "\n",
      "Epoch 09254: val_mae did not improve from 6.28420\n",
      "Epoch 9255/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 72.5116 - mae: 6.8352 - val_loss: 262.0904 - val_mae: 10.3801\n",
      "\n",
      "Epoch 09255: val_mae did not improve from 6.28420\n",
      "Epoch 9256/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 125.5390 - mae: 9.0635 - val_loss: 196.9357 - val_mae: 7.6858\n",
      "\n",
      "Epoch 09256: val_mae did not improve from 6.28420\n",
      "Epoch 9257/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 92.3289 - mae: 7.7498 - val_loss: 217.4744 - val_mae: 8.7201\n",
      "\n",
      "Epoch 09257: val_mae did not improve from 6.28420\n",
      "Epoch 9258/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 125.2719 - mae: 9.0350 - val_loss: 206.6479 - val_mae: 8.1375\n",
      "\n",
      "Epoch 09258: val_mae did not improve from 6.28420\n",
      "Epoch 9259/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 139.1250 - mae: 9.3569 - val_loss: 207.0754 - val_mae: 8.1872\n",
      "\n",
      "Epoch 09259: val_mae did not improve from 6.28420\n",
      "Epoch 9260/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 85.3499 - mae: 7.3365 - val_loss: 292.6766 - val_mae: 11.4501\n",
      "\n",
      "Epoch 09260: val_mae did not improve from 6.28420\n",
      "Epoch 9261/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 115.8944 - mae: 8.6172 - val_loss: 212.1969 - val_mae: 8.4956\n",
      "\n",
      "Epoch 09261: val_mae did not improve from 6.28420\n",
      "Epoch 9262/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 92.3738 - mae: 7.6261 - val_loss: 213.9188 - val_mae: 8.4448\n",
      "\n",
      "Epoch 09262: val_mae did not improve from 6.28420\n",
      "Epoch 9263/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 86.6217 - mae: 7.4487 - val_loss: 254.4337 - val_mae: 10.6126\n",
      "\n",
      "Epoch 09263: val_mae did not improve from 6.28420\n",
      "Epoch 9264/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 87.0451 - mae: 7.4732 - val_loss: 250.1594 - val_mae: 10.1457\n",
      "\n",
      "Epoch 09264: val_mae did not improve from 6.28420\n",
      "Epoch 9265/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.0848 - mae: 7.6206 - val_loss: 213.5751 - val_mae: 8.4975\n",
      "\n",
      "Epoch 09265: val_mae did not improve from 6.28420\n",
      "Epoch 9266/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 70.1867 - mae: 6.6547 - val_loss: 239.0515 - val_mae: 9.2065\n",
      "\n",
      "Epoch 09266: val_mae did not improve from 6.28420\n",
      "Epoch 9267/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 114.5338 - mae: 8.3745 - val_loss: 227.5113 - val_mae: 9.3314\n",
      "\n",
      "Epoch 09267: val_mae did not improve from 6.28420\n",
      "Epoch 9268/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 85.3170 - mae: 7.1737 - val_loss: 230.3590 - val_mae: 9.4502\n",
      "\n",
      "Epoch 09268: val_mae did not improve from 6.28420\n",
      "Epoch 9269/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 105.4493 - mae: 8.2984 - val_loss: 206.8775 - val_mae: 8.0010\n",
      "\n",
      "Epoch 09269: val_mae did not improve from 6.28420\n",
      "Epoch 9270/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 97.3443 - mae: 7.8143 - val_loss: 220.1348 - val_mae: 8.5742\n",
      "\n",
      "Epoch 09270: val_mae did not improve from 6.28420\n",
      "Epoch 9271/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 105.9275 - mae: 8.2665 - val_loss: 230.3604 - val_mae: 9.0559\n",
      "\n",
      "Epoch 09271: val_mae did not improve from 6.28420\n",
      "Epoch 9272/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 119.2267 - mae: 8.7785 - val_loss: 319.9140 - val_mae: 12.3655\n",
      "\n",
      "Epoch 09272: val_mae did not improve from 6.28420\n",
      "Epoch 9273/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 125.1540 - mae: 8.9639 - val_loss: 247.3202 - val_mae: 9.7779\n",
      "\n",
      "Epoch 09273: val_mae did not improve from 6.28420\n",
      "Epoch 9274/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 77.5351 - mae: 7.0229 - val_loss: 216.8208 - val_mae: 8.6861\n",
      "\n",
      "Epoch 09274: val_mae did not improve from 6.28420\n",
      "Epoch 9275/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.8286 - mae: 6.8852 - val_loss: 258.1890 - val_mae: 10.2504\n",
      "\n",
      "Epoch 09275: val_mae did not improve from 6.28420\n",
      "Epoch 9276/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.2327 - mae: 6.9734 - val_loss: 194.9459 - val_mae: 7.5163\n",
      "\n",
      "Epoch 09276: val_mae did not improve from 6.28420\n",
      "Epoch 9277/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 82.1728 - mae: 7.2697 - val_loss: 337.6118 - val_mae: 13.6586\n",
      "\n",
      "Epoch 09277: val_mae did not improve from 6.28420\n",
      "Epoch 9278/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 142.7799 - mae: 9.4762 - val_loss: 199.1250 - val_mae: 7.8367\n",
      "\n",
      "Epoch 09278: val_mae did not improve from 6.28420\n",
      "Epoch 9279/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 80.2284 - mae: 7.2765 - val_loss: 219.8006 - val_mae: 8.7314\n",
      "\n",
      "Epoch 09279: val_mae did not improve from 6.28420\n",
      "Epoch 9280/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 92.0000 - mae: 7.6221 - val_loss: 224.6448 - val_mae: 8.9473\n",
      "\n",
      "Epoch 09280: val_mae did not improve from 6.28420\n",
      "Epoch 9281/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 100.8828 - mae: 7.9261 - val_loss: 202.7651 - val_mae: 7.8378\n",
      "\n",
      "Epoch 09281: val_mae did not improve from 6.28420\n",
      "Epoch 9282/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 111.4337 - mae: 8.4123 - val_loss: 269.7554 - val_mae: 11.0633\n",
      "\n",
      "Epoch 09282: val_mae did not improve from 6.28420\n",
      "Epoch 9283/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 96.8296 - mae: 8.0009 - val_loss: 219.9746 - val_mae: 8.6056\n",
      "\n",
      "Epoch 09283: val_mae did not improve from 6.28420\n",
      "Epoch 9284/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 93.0646 - mae: 7.6838 - val_loss: 250.9389 - val_mae: 10.1643\n",
      "\n",
      "Epoch 09284: val_mae did not improve from 6.28420\n",
      "Epoch 9285/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 124.9834 - mae: 8.4924 - val_loss: 229.2606 - val_mae: 9.2184\n",
      "\n",
      "Epoch 09285: val_mae did not improve from 6.28420\n",
      "Epoch 9286/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 98.0691 - mae: 7.7500 - val_loss: 219.2943 - val_mae: 8.6516\n",
      "\n",
      "Epoch 09286: val_mae did not improve from 6.28420\n",
      "Epoch 9287/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.5573 - mae: 6.6790 - val_loss: 228.1432 - val_mae: 9.2499\n",
      "\n",
      "Epoch 09287: val_mae did not improve from 6.28420\n",
      "Epoch 9288/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 91.4452 - mae: 7.6415 - val_loss: 263.9814 - val_mae: 10.0400\n",
      "\n",
      "Epoch 09288: val_mae did not improve from 6.28420\n",
      "Epoch 9289/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 95.3153 - mae: 7.9130 - val_loss: 249.7941 - val_mae: 10.2096\n",
      "\n",
      "Epoch 09289: val_mae did not improve from 6.28420\n",
      "Epoch 9290/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.7373 - mae: 7.2659 - val_loss: 236.0520 - val_mae: 9.2492\n",
      "\n",
      "Epoch 09290: val_mae did not improve from 6.28420\n",
      "Epoch 9291/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 110.9186 - mae: 8.2753 - val_loss: 235.4537 - val_mae: 9.2998\n",
      "\n",
      "Epoch 09291: val_mae did not improve from 6.28420\n",
      "Epoch 9292/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 82.6664 - mae: 7.1483 - val_loss: 273.3203 - val_mae: 10.8118\n",
      "\n",
      "Epoch 09292: val_mae did not improve from 6.28420\n",
      "Epoch 9293/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 123.4953 - mae: 8.7975 - val_loss: 204.5686 - val_mae: 7.5763\n",
      "\n",
      "Epoch 09293: val_mae did not improve from 6.28420\n",
      "Epoch 9294/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 87.1010 - mae: 7.2905 - val_loss: 206.6238 - val_mae: 8.0126\n",
      "\n",
      "Epoch 09294: val_mae did not improve from 6.28420\n",
      "Epoch 9295/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 89.8106 - mae: 7.6258 - val_loss: 242.2396 - val_mae: 9.2124\n",
      "\n",
      "Epoch 09295: val_mae did not improve from 6.28420\n",
      "Epoch 9296/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 87.4651 - mae: 7.5051 - val_loss: 213.9295 - val_mae: 7.6795\n",
      "\n",
      "Epoch 09296: val_mae did not improve from 6.28420\n",
      "Epoch 9297/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 65.3300 - mae: 6.2904 - val_loss: 220.1642 - val_mae: 8.1475\n",
      "\n",
      "Epoch 09297: val_mae did not improve from 6.28420\n",
      "Epoch 9298/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 63.5701 - mae: 6.3523 - val_loss: 219.2902 - val_mae: 8.5491\n",
      "\n",
      "Epoch 09298: val_mae did not improve from 6.28420\n",
      "Epoch 9299/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 86.1219 - mae: 7.3080 - val_loss: 235.4154 - val_mae: 9.4195\n",
      "\n",
      "Epoch 09299: val_mae did not improve from 6.28420\n",
      "Epoch 9300/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.0952 - mae: 7.4546 - val_loss: 237.5781 - val_mae: 9.3364\n",
      "\n",
      "Epoch 09300: val_mae did not improve from 6.28420\n",
      "Epoch 9301/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.4356 - mae: 7.6338 - val_loss: 208.8030 - val_mae: 7.9382\n",
      "\n",
      "Epoch 09301: val_mae did not improve from 6.28420\n",
      "Epoch 9302/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.1835 - mae: 7.0233 - val_loss: 221.7505 - val_mae: 8.5807\n",
      "\n",
      "Epoch 09302: val_mae did not improve from 6.28420\n",
      "Epoch 9303/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 87.4940 - mae: 7.5001 - val_loss: 200.6931 - val_mae: 7.5493\n",
      "\n",
      "Epoch 09303: val_mae did not improve from 6.28420\n",
      "Epoch 9304/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 95.5409 - mae: 7.7856 - val_loss: 240.8823 - val_mae: 9.3905\n",
      "\n",
      "Epoch 09304: val_mae did not improve from 6.28420\n",
      "Epoch 9305/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 128.4289 - mae: 8.9895 - val_loss: 235.3879 - val_mae: 9.2946\n",
      "\n",
      "Epoch 09305: val_mae did not improve from 6.28420\n",
      "Epoch 9306/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 151.2454 - mae: 9.6881 - val_loss: 220.6672 - val_mae: 8.6152\n",
      "\n",
      "Epoch 09306: val_mae did not improve from 6.28420\n",
      "Epoch 9307/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 103.5601 - mae: 8.0728 - val_loss: 219.2197 - val_mae: 8.4854\n",
      "\n",
      "Epoch 09307: val_mae did not improve from 6.28420\n",
      "Epoch 9308/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 81.8110 - mae: 7.2931 - val_loss: 253.7495 - val_mae: 10.1458\n",
      "\n",
      "Epoch 09308: val_mae did not improve from 6.28420\n",
      "Epoch 9309/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 108.4448 - mae: 8.1648 - val_loss: 206.0800 - val_mae: 7.7291\n",
      "\n",
      "Epoch 09309: val_mae did not improve from 6.28420\n",
      "Epoch 9310/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.8169 - mae: 6.6769 - val_loss: 250.8368 - val_mae: 9.9437\n",
      "\n",
      "Epoch 09310: val_mae did not improve from 6.28420\n",
      "Epoch 9311/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 88.5045 - mae: 7.5360 - val_loss: 207.5052 - val_mae: 7.9947\n",
      "\n",
      "Epoch 09311: val_mae did not improve from 6.28420\n",
      "Epoch 9312/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 85.4815 - mae: 7.3745 - val_loss: 297.0828 - val_mae: 11.2195\n",
      "\n",
      "Epoch 09312: val_mae did not improve from 6.28420\n",
      "Epoch 9313/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 101.9522 - mae: 8.1126 - val_loss: 315.0138 - val_mae: 12.3649\n",
      "\n",
      "Epoch 09313: val_mae did not improve from 6.28420\n",
      "Epoch 9314/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 128.3816 - mae: 9.0806 - val_loss: 390.8426 - val_mae: 14.0275\n",
      "\n",
      "Epoch 09314: val_mae did not improve from 6.28420\n",
      "Epoch 9315/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 166.2301 - mae: 9.9085 - val_loss: 240.5668 - val_mae: 9.4552\n",
      "\n",
      "Epoch 09315: val_mae did not improve from 6.28420\n",
      "Epoch 9316/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 91.3081 - mae: 7.6164 - val_loss: 250.6475 - val_mae: 9.9820\n",
      "\n",
      "Epoch 09316: val_mae did not improve from 6.28420\n",
      "Epoch 9317/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 87.2965 - mae: 7.4791 - val_loss: 237.7866 - val_mae: 9.1987\n",
      "\n",
      "Epoch 09317: val_mae did not improve from 6.28420\n",
      "Epoch 9318/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 122.0412 - mae: 8.7809 - val_loss: 242.6448 - val_mae: 9.7342\n",
      "\n",
      "Epoch 09318: val_mae did not improve from 6.28420\n",
      "Epoch 9319/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 99.3626 - mae: 7.9635 - val_loss: 209.7785 - val_mae: 8.2413\n",
      "\n",
      "Epoch 09319: val_mae did not improve from 6.28420\n",
      "Epoch 9320/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 95.6431 - mae: 8.0220 - val_loss: 196.4567 - val_mae: 7.6564\n",
      "\n",
      "Epoch 09320: val_mae did not improve from 6.28420\n",
      "Epoch 9321/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 106.8568 - mae: 8.0528 - val_loss: 248.6791 - val_mae: 10.0690\n",
      "\n",
      "Epoch 09321: val_mae did not improve from 6.28420\n",
      "Epoch 9322/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 77.8182 - mae: 7.0235 - val_loss: 224.2056 - val_mae: 8.9869\n",
      "\n",
      "Epoch 09322: val_mae did not improve from 6.28420\n",
      "Epoch 9323/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 80.3925 - mae: 7.1918 - val_loss: 230.6160 - val_mae: 8.8792\n",
      "\n",
      "Epoch 09323: val_mae did not improve from 6.28420\n",
      "Epoch 9324/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 94.6570 - mae: 7.7982 - val_loss: 225.1015 - val_mae: 8.7762\n",
      "\n",
      "Epoch 09324: val_mae did not improve from 6.28420\n",
      "Epoch 9325/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 76.4381 - mae: 6.9114 - val_loss: 231.2266 - val_mae: 9.2809\n",
      "\n",
      "Epoch 09325: val_mae did not improve from 6.28420\n",
      "Epoch 9326/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 89.5811 - mae: 7.3140 - val_loss: 220.5322 - val_mae: 8.3522\n",
      "\n",
      "Epoch 09326: val_mae did not improve from 6.28420\n",
      "Epoch 9327/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.4981 - mae: 6.7127 - val_loss: 216.9093 - val_mae: 8.2816\n",
      "\n",
      "Epoch 09327: val_mae did not improve from 6.28420\n",
      "Epoch 9328/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 68.4611 - mae: 6.4987 - val_loss: 227.7161 - val_mae: 8.5057\n",
      "\n",
      "Epoch 09328: val_mae did not improve from 6.28420\n",
      "Epoch 9329/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 114.0438 - mae: 8.3626 - val_loss: 218.9228 - val_mae: 8.4090\n",
      "\n",
      "Epoch 09329: val_mae did not improve from 6.28420\n",
      "Epoch 9330/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.4146 - mae: 6.8112 - val_loss: 230.7727 - val_mae: 8.9170\n",
      "\n",
      "Epoch 09330: val_mae did not improve from 6.28420\n",
      "Epoch 9331/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 76.8764 - mae: 6.9135 - val_loss: 242.5773 - val_mae: 9.2023\n",
      "\n",
      "Epoch 09331: val_mae did not improve from 6.28420\n",
      "Epoch 9332/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 99.6482 - mae: 8.0999 - val_loss: 243.1589 - val_mae: 8.8984\n",
      "\n",
      "Epoch 09332: val_mae did not improve from 6.28420\n",
      "Epoch 9333/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 101.0778 - mae: 8.1833 - val_loss: 206.5834 - val_mae: 7.4191\n",
      "\n",
      "Epoch 09333: val_mae did not improve from 6.28420\n",
      "Epoch 9334/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.4006 - mae: 6.5932 - val_loss: 219.3272 - val_mae: 8.3812\n",
      "\n",
      "Epoch 09334: val_mae did not improve from 6.28420\n",
      "Epoch 9335/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 92.2046 - mae: 7.5956 - val_loss: 240.6304 - val_mae: 9.0519\n",
      "\n",
      "Epoch 09335: val_mae did not improve from 6.28420\n",
      "Epoch 9336/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 99.7080 - mae: 7.9198 - val_loss: 214.1520 - val_mae: 7.8236\n",
      "\n",
      "Epoch 09336: val_mae did not improve from 6.28420\n",
      "Epoch 9337/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 65.1044 - mae: 6.4030 - val_loss: 215.6112 - val_mae: 7.6981\n",
      "\n",
      "Epoch 09337: val_mae did not improve from 6.28420\n",
      "Epoch 9338/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 80.9744 - mae: 6.9645 - val_loss: 221.7200 - val_mae: 7.9554\n",
      "\n",
      "Epoch 09338: val_mae did not improve from 6.28420\n",
      "Epoch 9339/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 70.0038 - mae: 6.5091 - val_loss: 208.9635 - val_mae: 7.4477\n",
      "\n",
      "Epoch 09339: val_mae did not improve from 6.28420\n",
      "Epoch 9340/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 92.6154 - mae: 7.8330 - val_loss: 252.5864 - val_mae: 9.5290\n",
      "\n",
      "Epoch 09340: val_mae did not improve from 6.28420\n",
      "Epoch 9341/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 83.0318 - mae: 7.3726 - val_loss: 216.5178 - val_mae: 8.0408\n",
      "\n",
      "Epoch 09341: val_mae did not improve from 6.28420\n",
      "Epoch 9342/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 74.2371 - mae: 6.9082 - val_loss: 224.4704 - val_mae: 8.6961\n",
      "\n",
      "Epoch 09342: val_mae did not improve from 6.28420\n",
      "Epoch 9343/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 88.4942 - mae: 7.4554 - val_loss: 198.5140 - val_mae: 7.4258\n",
      "\n",
      "Epoch 09343: val_mae did not improve from 6.28420\n",
      "Epoch 9344/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 75.2873 - mae: 6.9603 - val_loss: 252.8571 - val_mae: 10.4698\n",
      "\n",
      "Epoch 09344: val_mae did not improve from 6.28420\n",
      "Epoch 9345/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.4972 - mae: 6.6533 - val_loss: 220.1908 - val_mae: 8.3808\n",
      "\n",
      "Epoch 09345: val_mae did not improve from 6.28420\n",
      "Epoch 9346/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 90.8397 - mae: 7.6040 - val_loss: 231.5061 - val_mae: 9.1601\n",
      "\n",
      "Epoch 09346: val_mae did not improve from 6.28420\n",
      "Epoch 9347/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 78.2018 - mae: 6.8512 - val_loss: 218.2923 - val_mae: 8.4308\n",
      "\n",
      "Epoch 09347: val_mae did not improve from 6.28420\n",
      "Epoch 9348/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 93.0268 - mae: 7.7458 - val_loss: 221.5056 - val_mae: 8.6346\n",
      "\n",
      "Epoch 09348: val_mae did not improve from 6.28420\n",
      "Epoch 9349/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 74.1971 - mae: 6.6376 - val_loss: 220.1730 - val_mae: 8.5029\n",
      "\n",
      "Epoch 09349: val_mae did not improve from 6.28420\n",
      "Epoch 9350/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 80.9740 - mae: 7.2820 - val_loss: 252.4985 - val_mae: 9.7290\n",
      "\n",
      "Epoch 09350: val_mae did not improve from 6.28420\n",
      "Epoch 9351/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 80.8450 - mae: 7.1034 - val_loss: 224.5872 - val_mae: 8.2535\n",
      "\n",
      "Epoch 09351: val_mae did not improve from 6.28420\n",
      "Epoch 9352/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 72.0690 - mae: 6.9019 - val_loss: 216.6095 - val_mae: 7.9577\n",
      "\n",
      "Epoch 09352: val_mae did not improve from 6.28420\n",
      "Epoch 9353/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 69.3696 - mae: 6.5902 - val_loss: 221.1719 - val_mae: 8.4764\n",
      "\n",
      "Epoch 09353: val_mae did not improve from 6.28420\n",
      "Epoch 9354/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 104.9061 - mae: 7.9998 - val_loss: 297.1573 - val_mae: 11.6117\n",
      "\n",
      "Epoch 09354: val_mae did not improve from 6.28420\n",
      "Epoch 9355/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 114.6758 - mae: 8.4861 - val_loss: 238.1454 - val_mae: 9.5210\n",
      "\n",
      "Epoch 09355: val_mae did not improve from 6.28420\n",
      "Epoch 9356/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 92.0060 - mae: 7.5742 - val_loss: 203.4302 - val_mae: 7.6797\n",
      "\n",
      "Epoch 09356: val_mae did not improve from 6.28420\n",
      "Epoch 9357/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 75.6353 - mae: 6.7832 - val_loss: 247.1499 - val_mae: 9.7046\n",
      "\n",
      "Epoch 09357: val_mae did not improve from 6.28420\n",
      "Epoch 9358/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.1037 - mae: 6.7644 - val_loss: 222.9431 - val_mae: 8.5829\n",
      "\n",
      "Epoch 09358: val_mae did not improve from 6.28420\n",
      "Epoch 9359/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 78.6598 - mae: 7.1462 - val_loss: 219.7925 - val_mae: 8.2469\n",
      "\n",
      "Epoch 09359: val_mae did not improve from 6.28420\n",
      "Epoch 9360/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.0145 - mae: 6.9074 - val_loss: 228.6850 - val_mae: 9.0327\n",
      "\n",
      "Epoch 09360: val_mae did not improve from 6.28420\n",
      "Epoch 9361/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 98.7710 - mae: 7.9823 - val_loss: 214.1496 - val_mae: 8.4733\n",
      "\n",
      "Epoch 09361: val_mae did not improve from 6.28420\n",
      "Epoch 9362/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.1471 - mae: 6.6504 - val_loss: 205.7549 - val_mae: 7.5400\n",
      "\n",
      "Epoch 09362: val_mae did not improve from 6.28420\n",
      "Epoch 9363/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 116.3042 - mae: 8.2644 - val_loss: 230.0256 - val_mae: 9.0311\n",
      "\n",
      "Epoch 09363: val_mae did not improve from 6.28420\n",
      "Epoch 9364/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.1340 - mae: 7.3820 - val_loss: 276.1949 - val_mae: 11.0224\n",
      "\n",
      "Epoch 09364: val_mae did not improve from 6.28420\n",
      "Epoch 9365/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 99.0221 - mae: 8.0219 - val_loss: 213.1489 - val_mae: 8.4266\n",
      "\n",
      "Epoch 09365: val_mae did not improve from 6.28420\n",
      "Epoch 9366/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 64.4740 - mae: 6.3430 - val_loss: 196.5864 - val_mae: 7.4245\n",
      "\n",
      "Epoch 09366: val_mae did not improve from 6.28420\n",
      "Epoch 9367/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.2854 - mae: 6.8650 - val_loss: 204.8821 - val_mae: 7.7495\n",
      "\n",
      "Epoch 09367: val_mae did not improve from 6.28420\n",
      "Epoch 9368/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 60.7791 - mae: 6.2282 - val_loss: 211.3062 - val_mae: 8.3163\n",
      "\n",
      "Epoch 09368: val_mae did not improve from 6.28420\n",
      "Epoch 9369/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 79.8710 - mae: 7.0165 - val_loss: 194.5815 - val_mae: 7.4565\n",
      "\n",
      "Epoch 09369: val_mae did not improve from 6.28420\n",
      "Epoch 9370/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 77.7282 - mae: 6.7181 - val_loss: 215.1072 - val_mae: 8.4455\n",
      "\n",
      "Epoch 09370: val_mae did not improve from 6.28420\n",
      "Epoch 9371/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.6177 - mae: 6.7469 - val_loss: 280.7004 - val_mae: 11.1034\n",
      "\n",
      "Epoch 09371: val_mae did not improve from 6.28420\n",
      "Epoch 9372/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 118.9607 - mae: 8.7447 - val_loss: 246.3857 - val_mae: 10.2246\n",
      "\n",
      "Epoch 09372: val_mae did not improve from 6.28420\n",
      "Epoch 9373/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 121.9288 - mae: 8.7483 - val_loss: 308.1808 - val_mae: 11.7677\n",
      "\n",
      "Epoch 09373: val_mae did not improve from 6.28420\n",
      "Epoch 9374/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 124.1847 - mae: 8.7660 - val_loss: 295.1129 - val_mae: 11.5467\n",
      "\n",
      "Epoch 09374: val_mae did not improve from 6.28420\n",
      "Epoch 9375/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 111.5766 - mae: 8.5887 - val_loss: 248.4951 - val_mae: 10.1310\n",
      "\n",
      "Epoch 09375: val_mae did not improve from 6.28420\n",
      "Epoch 9376/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 87.3778 - mae: 7.4870 - val_loss: 199.3161 - val_mae: 7.5822\n",
      "\n",
      "Epoch 09376: val_mae did not improve from 6.28420\n",
      "Epoch 9377/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 65.3886 - mae: 6.4437 - val_loss: 214.0409 - val_mae: 8.4580\n",
      "\n",
      "Epoch 09377: val_mae did not improve from 6.28420\n",
      "Epoch 9378/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 68.0563 - mae: 6.4769 - val_loss: 242.1196 - val_mae: 9.5428\n",
      "\n",
      "Epoch 09378: val_mae did not improve from 6.28420\n",
      "Epoch 9379/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 78.3700 - mae: 7.0818 - val_loss: 201.1754 - val_mae: 7.6621\n",
      "\n",
      "Epoch 09379: val_mae did not improve from 6.28420\n",
      "Epoch 9380/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 61.7522 - mae: 6.3003 - val_loss: 206.0858 - val_mae: 7.9681\n",
      "\n",
      "Epoch 09380: val_mae did not improve from 6.28420\n",
      "Epoch 9381/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 74.8862 - mae: 6.7987 - val_loss: 206.9729 - val_mae: 7.9729\n",
      "\n",
      "Epoch 09381: val_mae did not improve from 6.28420\n",
      "Epoch 9382/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 105.5152 - mae: 8.1126 - val_loss: 337.8291 - val_mae: 13.1720\n",
      "\n",
      "Epoch 09382: val_mae did not improve from 6.28420\n",
      "Epoch 9383/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 121.6777 - mae: 8.8530 - val_loss: 243.7339 - val_mae: 9.7739\n",
      "\n",
      "Epoch 09383: val_mae did not improve from 6.28420\n",
      "Epoch 9384/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 87.2603 - mae: 7.2956 - val_loss: 232.1087 - val_mae: 9.2365\n",
      "\n",
      "Epoch 09384: val_mae did not improve from 6.28420\n",
      "Epoch 9385/10000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 85.1891 - mae: 7.3121 - val_loss: 221.7545 - val_mae: 9.0014\n",
      "\n",
      "Epoch 09385: val_mae did not improve from 6.28420\n",
      "Epoch 9386/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 98.0383 - mae: 7.9020 - val_loss: 223.9505 - val_mae: 8.8440\n",
      "\n",
      "Epoch 09386: val_mae did not improve from 6.28420\n",
      "Epoch 9387/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 89.9667 - mae: 7.6019 - val_loss: 245.4362 - val_mae: 10.0416\n",
      "\n",
      "Epoch 09387: val_mae did not improve from 6.28420\n",
      "Epoch 9388/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 107.4696 - mae: 8.0241 - val_loss: 233.7281 - val_mae: 9.5034\n",
      "\n",
      "Epoch 09388: val_mae did not improve from 6.28420\n",
      "Epoch 9389/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 76.8764 - mae: 7.0660 - val_loss: 204.1347 - val_mae: 7.8225\n",
      "\n",
      "Epoch 09389: val_mae did not improve from 6.28420\n",
      "Epoch 9390/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 61.4799 - mae: 6.1244 - val_loss: 238.5727 - val_mae: 9.6997\n",
      "\n",
      "Epoch 09390: val_mae did not improve from 6.28420\n",
      "Epoch 9391/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 77.6615 - mae: 6.9854 - val_loss: 215.5737 - val_mae: 8.7163\n",
      "\n",
      "Epoch 09391: val_mae did not improve from 6.28420\n",
      "Epoch 9392/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 74.4796 - mae: 6.8511 - val_loss: 227.0905 - val_mae: 9.1699\n",
      "\n",
      "Epoch 09392: val_mae did not improve from 6.28420\n",
      "Epoch 9393/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 97.7394 - mae: 7.9781 - val_loss: 236.6644 - val_mae: 9.4769\n",
      "\n",
      "Epoch 09393: val_mae did not improve from 6.28420\n",
      "Epoch 9394/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 102.2253 - mae: 8.2017 - val_loss: 210.3473 - val_mae: 8.1391\n",
      "\n",
      "Epoch 09394: val_mae did not improve from 6.28420\n",
      "Epoch 9395/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.5028 - mae: 6.4684 - val_loss: 224.5963 - val_mae: 8.7199\n",
      "\n",
      "Epoch 09395: val_mae did not improve from 6.28420\n",
      "Epoch 9396/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.8739 - mae: 6.9014 - val_loss: 231.8934 - val_mae: 9.1123\n",
      "\n",
      "Epoch 09396: val_mae did not improve from 6.28420\n",
      "Epoch 9397/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 107.7446 - mae: 8.1493 - val_loss: 216.7778 - val_mae: 8.4325\n",
      "\n",
      "Epoch 09397: val_mae did not improve from 6.28420\n",
      "Epoch 9398/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 110.9906 - mae: 8.2858 - val_loss: 242.0962 - val_mae: 9.1869\n",
      "\n",
      "Epoch 09398: val_mae did not improve from 6.28420\n",
      "Epoch 9399/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.4415 - mae: 6.8020 - val_loss: 263.2746 - val_mae: 10.1083\n",
      "\n",
      "Epoch 09399: val_mae did not improve from 6.28420\n",
      "Epoch 9400/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 92.3914 - mae: 7.6974 - val_loss: 247.5563 - val_mae: 9.9518\n",
      "\n",
      "Epoch 09400: val_mae did not improve from 6.28420\n",
      "Epoch 9401/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 83.1071 - mae: 7.2838 - val_loss: 202.6490 - val_mae: 7.8238\n",
      "\n",
      "Epoch 09401: val_mae did not improve from 6.28420\n",
      "Epoch 9402/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 75.8547 - mae: 6.8110 - val_loss: 199.1372 - val_mae: 7.4834\n",
      "\n",
      "Epoch 09402: val_mae did not improve from 6.28420\n",
      "Epoch 9403/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 66.3400 - mae: 6.4845 - val_loss: 214.5124 - val_mae: 8.3081\n",
      "\n",
      "Epoch 09403: val_mae did not improve from 6.28420\n",
      "Epoch 9404/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 75.0799 - mae: 6.8060 - val_loss: 206.9064 - val_mae: 7.6996\n",
      "\n",
      "Epoch 09404: val_mae did not improve from 6.28420\n",
      "Epoch 9405/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 90.8553 - mae: 7.6223 - val_loss: 243.4545 - val_mae: 9.6456\n",
      "\n",
      "Epoch 09405: val_mae did not improve from 6.28420\n",
      "Epoch 9406/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.8688 - mae: 7.6328 - val_loss: 221.0000 - val_mae: 8.4645\n",
      "\n",
      "Epoch 09406: val_mae did not improve from 6.28420\n",
      "Epoch 9407/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 79.6767 - mae: 7.1486 - val_loss: 213.7283 - val_mae: 8.3594\n",
      "\n",
      "Epoch 09407: val_mae did not improve from 6.28420\n",
      "Epoch 9408/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 72.0181 - mae: 6.6146 - val_loss: 211.6063 - val_mae: 8.1721\n",
      "\n",
      "Epoch 09408: val_mae did not improve from 6.28420\n",
      "Epoch 9409/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 94.1346 - mae: 7.6329 - val_loss: 238.1553 - val_mae: 9.4329\n",
      "\n",
      "Epoch 09409: val_mae did not improve from 6.28420\n",
      "Epoch 9410/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 117.0164 - mae: 8.6699 - val_loss: 233.0421 - val_mae: 9.0820\n",
      "\n",
      "Epoch 09410: val_mae did not improve from 6.28420\n",
      "Epoch 9411/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 69.5501 - mae: 6.6584 - val_loss: 205.3278 - val_mae: 7.6654\n",
      "\n",
      "Epoch 09411: val_mae did not improve from 6.28420\n",
      "Epoch 9412/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.9212 - mae: 6.5497 - val_loss: 232.5180 - val_mae: 9.0266\n",
      "\n",
      "Epoch 09412: val_mae did not improve from 6.28420\n",
      "Epoch 9413/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.3136 - mae: 6.6329 - val_loss: 201.8457 - val_mae: 7.5716\n",
      "\n",
      "Epoch 09413: val_mae did not improve from 6.28420\n",
      "Epoch 9414/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 66.9707 - mae: 6.4013 - val_loss: 232.6667 - val_mae: 9.4305\n",
      "\n",
      "Epoch 09414: val_mae did not improve from 6.28420\n",
      "Epoch 9415/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 81.0376 - mae: 7.1097 - val_loss: 212.3385 - val_mae: 8.4133\n",
      "\n",
      "Epoch 09415: val_mae did not improve from 6.28420\n",
      "Epoch 9416/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 92.1715 - mae: 7.7033 - val_loss: 239.5519 - val_mae: 9.7428\n",
      "\n",
      "Epoch 09416: val_mae did not improve from 6.28420\n",
      "Epoch 9417/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 85.1090 - mae: 7.4017 - val_loss: 212.2840 - val_mae: 8.3340\n",
      "\n",
      "Epoch 09417: val_mae did not improve from 6.28420\n",
      "Epoch 9418/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 76.3673 - mae: 6.8941 - val_loss: 240.9290 - val_mae: 9.6491\n",
      "\n",
      "Epoch 09418: val_mae did not improve from 6.28420\n",
      "Epoch 9419/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 82.5211 - mae: 7.3264 - val_loss: 246.3915 - val_mae: 10.0168\n",
      "\n",
      "Epoch 09419: val_mae did not improve from 6.28420\n",
      "Epoch 9420/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 110.8077 - mae: 8.2843 - val_loss: 264.9010 - val_mae: 10.7136\n",
      "\n",
      "Epoch 09420: val_mae did not improve from 6.28420\n",
      "Epoch 9421/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 93.1883 - mae: 7.7776 - val_loss: 239.6476 - val_mae: 9.4377\n",
      "\n",
      "Epoch 09421: val_mae did not improve from 6.28420\n",
      "Epoch 9422/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 106.9937 - mae: 8.0493 - val_loss: 256.5136 - val_mae: 9.8975\n",
      "\n",
      "Epoch 09422: val_mae did not improve from 6.28420\n",
      "Epoch 9423/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 79.0837 - mae: 6.9638 - val_loss: 240.1799 - val_mae: 9.3608\n",
      "\n",
      "Epoch 09423: val_mae did not improve from 6.28420\n",
      "Epoch 9424/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 124.8656 - mae: 8.5523 - val_loss: 297.6094 - val_mae: 11.4396\n",
      "\n",
      "Epoch 09424: val_mae did not improve from 6.28420\n",
      "Epoch 9425/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 117.4809 - mae: 8.4647 - val_loss: 207.5173 - val_mae: 7.7206\n",
      "\n",
      "Epoch 09425: val_mae did not improve from 6.28420\n",
      "Epoch 9426/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 95.7416 - mae: 7.7215 - val_loss: 299.4086 - val_mae: 11.3962\n",
      "\n",
      "Epoch 09426: val_mae did not improve from 6.28420\n",
      "Epoch 9427/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 94.0730 - mae: 7.6895 - val_loss: 211.9098 - val_mae: 8.0364\n",
      "\n",
      "Epoch 09427: val_mae did not improve from 6.28420\n",
      "Epoch 9428/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 70.8730 - mae: 6.5594 - val_loss: 207.4499 - val_mae: 7.9616\n",
      "\n",
      "Epoch 09428: val_mae did not improve from 6.28420\n",
      "Epoch 9429/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 83.5831 - mae: 7.2421 - val_loss: 241.1589 - val_mae: 9.5406\n",
      "\n",
      "Epoch 09429: val_mae did not improve from 6.28420\n",
      "Epoch 9430/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 105.5853 - mae: 8.2187 - val_loss: 267.0172 - val_mae: 10.6260\n",
      "\n",
      "Epoch 09430: val_mae did not improve from 6.28420\n",
      "Epoch 9431/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 117.4073 - mae: 8.7950 - val_loss: 211.2779 - val_mae: 7.7614\n",
      "\n",
      "Epoch 09431: val_mae did not improve from 6.28420\n",
      "Epoch 9432/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 78.2322 - mae: 6.8543 - val_loss: 222.3447 - val_mae: 8.5103\n",
      "\n",
      "Epoch 09432: val_mae did not improve from 6.28420\n",
      "Epoch 9433/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 101.6034 - mae: 8.0204 - val_loss: 204.6117 - val_mae: 7.5052\n",
      "\n",
      "Epoch 09433: val_mae did not improve from 6.28420\n",
      "Epoch 9434/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 83.1035 - mae: 7.2245 - val_loss: 300.5400 - val_mae: 12.2968\n",
      "\n",
      "Epoch 09434: val_mae did not improve from 6.28420\n",
      "Epoch 9435/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.4130 - mae: 7.1235 - val_loss: 199.2221 - val_mae: 7.7137\n",
      "\n",
      "Epoch 09435: val_mae did not improve from 6.28420\n",
      "Epoch 9436/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 76.2053 - mae: 6.9965 - val_loss: 324.2356 - val_mae: 12.9682\n",
      "\n",
      "Epoch 09436: val_mae did not improve from 6.28420\n",
      "Epoch 9437/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 82.7573 - mae: 7.3657 - val_loss: 207.9294 - val_mae: 7.9357\n",
      "\n",
      "Epoch 09437: val_mae did not improve from 6.28420\n",
      "Epoch 9438/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 80.1196 - mae: 6.9709 - val_loss: 323.2427 - val_mae: 12.2038\n",
      "\n",
      "Epoch 09438: val_mae did not improve from 6.28420\n",
      "Epoch 9439/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 133.8946 - mae: 9.5113 - val_loss: 201.0931 - val_mae: 7.5692\n",
      "\n",
      "Epoch 09439: val_mae did not improve from 6.28420\n",
      "Epoch 9440/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 111.0964 - mae: 8.1181 - val_loss: 218.3660 - val_mae: 8.2633\n",
      "\n",
      "Epoch 09440: val_mae did not improve from 6.28420\n",
      "Epoch 9441/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 117.1401 - mae: 8.4558 - val_loss: 223.6075 - val_mae: 8.7724\n",
      "\n",
      "Epoch 09441: val_mae did not improve from 6.28420\n",
      "Epoch 9442/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 130.4912 - mae: 9.1771 - val_loss: 259.9371 - val_mae: 10.3391\n",
      "\n",
      "Epoch 09442: val_mae did not improve from 6.28420\n",
      "Epoch 9443/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 85.7363 - mae: 7.4148 - val_loss: 228.1919 - val_mae: 9.0737\n",
      "\n",
      "Epoch 09443: val_mae did not improve from 6.28420\n",
      "Epoch 9444/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 106.1919 - mae: 8.1345 - val_loss: 247.0963 - val_mae: 9.4123\n",
      "\n",
      "Epoch 09444: val_mae did not improve from 6.28420\n",
      "Epoch 9445/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 94.9821 - mae: 7.7331 - val_loss: 187.8402 - val_mae: 7.3208\n",
      "\n",
      "Epoch 09445: val_mae did not improve from 6.28420\n",
      "Epoch 9446/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 75.2923 - mae: 6.9677 - val_loss: 253.8013 - val_mae: 10.4852\n",
      "\n",
      "Epoch 09446: val_mae did not improve from 6.28420\n",
      "Epoch 9447/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 119.7587 - mae: 8.8190 - val_loss: 222.1746 - val_mae: 8.9843\n",
      "\n",
      "Epoch 09447: val_mae did not improve from 6.28420\n",
      "Epoch 9448/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 81.4842 - mae: 7.0174 - val_loss: 217.1393 - val_mae: 8.6841\n",
      "\n",
      "Epoch 09448: val_mae did not improve from 6.28420\n",
      "Epoch 9449/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.4416 - mae: 6.4249 - val_loss: 189.7787 - val_mae: 7.1209\n",
      "\n",
      "Epoch 09449: val_mae did not improve from 6.28420\n",
      "Epoch 9450/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.0599 - mae: 6.4705 - val_loss: 204.5639 - val_mae: 8.0046\n",
      "\n",
      "Epoch 09450: val_mae did not improve from 6.28420\n",
      "Epoch 9451/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 78.1237 - mae: 7.0184 - val_loss: 196.0882 - val_mae: 7.7610\n",
      "\n",
      "Epoch 09451: val_mae did not improve from 6.28420\n",
      "Epoch 9452/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 75.1748 - mae: 6.8863 - val_loss: 217.9514 - val_mae: 8.6715\n",
      "\n",
      "Epoch 09452: val_mae did not improve from 6.28420\n",
      "Epoch 9453/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 84.1861 - mae: 7.3816 - val_loss: 320.1314 - val_mae: 12.2127\n",
      "\n",
      "Epoch 09453: val_mae did not improve from 6.28420\n",
      "Epoch 9454/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 104.5447 - mae: 8.2556 - val_loss: 210.2707 - val_mae: 8.4490\n",
      "\n",
      "Epoch 09454: val_mae did not improve from 6.28420\n",
      "Epoch 9455/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.7588 - mae: 6.7387 - val_loss: 222.3211 - val_mae: 8.9125\n",
      "\n",
      "Epoch 09455: val_mae did not improve from 6.28420\n",
      "Epoch 9456/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 106.9754 - mae: 8.1889 - val_loss: 227.3319 - val_mae: 8.9001\n",
      "\n",
      "Epoch 09456: val_mae did not improve from 6.28420\n",
      "Epoch 9457/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 103.8800 - mae: 8.1464 - val_loss: 237.0950 - val_mae: 9.3290\n",
      "\n",
      "Epoch 09457: val_mae did not improve from 6.28420\n",
      "Epoch 9458/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 76.0727 - mae: 6.8413 - val_loss: 222.1615 - val_mae: 8.7312\n",
      "\n",
      "Epoch 09458: val_mae did not improve from 6.28420\n",
      "Epoch 9459/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 92.3233 - mae: 7.6863 - val_loss: 193.7573 - val_mae: 7.3067\n",
      "\n",
      "Epoch 09459: val_mae did not improve from 6.28420\n",
      "Epoch 9460/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 109.0906 - mae: 8.4165 - val_loss: 242.5966 - val_mae: 9.8351\n",
      "\n",
      "Epoch 09460: val_mae did not improve from 6.28420\n",
      "Epoch 9461/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 113.2231 - mae: 8.3648 - val_loss: 234.4717 - val_mae: 9.3263\n",
      "\n",
      "Epoch 09461: val_mae did not improve from 6.28420\n",
      "Epoch 9462/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 104.6013 - mae: 8.0793 - val_loss: 227.1179 - val_mae: 9.2308\n",
      "\n",
      "Epoch 09462: val_mae did not improve from 6.28420\n",
      "Epoch 9463/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.1335 - mae: 6.9966 - val_loss: 213.4707 - val_mae: 8.4668\n",
      "\n",
      "Epoch 09463: val_mae did not improve from 6.28420\n",
      "Epoch 9464/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 85.8174 - mae: 7.3631 - val_loss: 355.9916 - val_mae: 14.4199\n",
      "\n",
      "Epoch 09464: val_mae did not improve from 6.28420\n",
      "Epoch 9465/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 135.7644 - mae: 9.4795 - val_loss: 201.5968 - val_mae: 8.0299\n",
      "\n",
      "Epoch 09465: val_mae did not improve from 6.28420\n",
      "Epoch 9466/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.4086 - mae: 6.7191 - val_loss: 219.2305 - val_mae: 8.8302\n",
      "\n",
      "Epoch 09466: val_mae did not improve from 6.28420\n",
      "Epoch 9467/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 68.8566 - mae: 6.6255 - val_loss: 196.7749 - val_mae: 7.4250\n",
      "\n",
      "Epoch 09467: val_mae did not improve from 6.28420\n",
      "Epoch 9468/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.3761 - mae: 7.4260 - val_loss: 223.0157 - val_mae: 8.9908\n",
      "\n",
      "Epoch 09468: val_mae did not improve from 6.28420\n",
      "Epoch 9469/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.8435 - mae: 7.0847 - val_loss: 201.1241 - val_mae: 7.7995\n",
      "\n",
      "Epoch 09469: val_mae did not improve from 6.28420\n",
      "Epoch 9470/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 72.0332 - mae: 6.6596 - val_loss: 236.8210 - val_mae: 9.7242\n",
      "\n",
      "Epoch 09470: val_mae did not improve from 6.28420\n",
      "Epoch 9471/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 99.4371 - mae: 7.8915 - val_loss: 205.7919 - val_mae: 7.9841\n",
      "\n",
      "Epoch 09471: val_mae did not improve from 6.28420\n",
      "Epoch 9472/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 97.4597 - mae: 7.8816 - val_loss: 316.3473 - val_mae: 12.1085\n",
      "\n",
      "Epoch 09472: val_mae did not improve from 6.28420\n",
      "Epoch 9473/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 139.9834 - mae: 9.4880 - val_loss: 218.2948 - val_mae: 8.7547\n",
      "\n",
      "Epoch 09473: val_mae did not improve from 6.28420\n",
      "Epoch 9474/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.4031 - mae: 6.9471 - val_loss: 199.7985 - val_mae: 7.7468\n",
      "\n",
      "Epoch 09474: val_mae did not improve from 6.28420\n",
      "Epoch 9475/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.7311 - mae: 6.9503 - val_loss: 275.1632 - val_mae: 10.5250\n",
      "\n",
      "Epoch 09475: val_mae did not improve from 6.28420\n",
      "Epoch 9476/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 111.3273 - mae: 8.3207 - val_loss: 194.0458 - val_mae: 7.5781\n",
      "\n",
      "Epoch 09476: val_mae did not improve from 6.28420\n",
      "Epoch 9477/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 68.5589 - mae: 6.5799 - val_loss: 219.6443 - val_mae: 8.8184\n",
      "\n",
      "Epoch 09477: val_mae did not improve from 6.28420\n",
      "Epoch 9478/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 73.1315 - mae: 6.8117 - val_loss: 217.3343 - val_mae: 8.6747\n",
      "\n",
      "Epoch 09478: val_mae did not improve from 6.28420\n",
      "Epoch 9479/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 96.0164 - mae: 7.8420 - val_loss: 193.4028 - val_mae: 7.3721\n",
      "\n",
      "Epoch 09479: val_mae did not improve from 6.28420\n",
      "Epoch 9480/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.3525 - mae: 6.5019 - val_loss: 217.6848 - val_mae: 8.7879\n",
      "\n",
      "Epoch 09480: val_mae did not improve from 6.28420\n",
      "Epoch 9481/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 89.2400 - mae: 7.6802 - val_loss: 194.4429 - val_mae: 7.3467\n",
      "\n",
      "Epoch 09481: val_mae did not improve from 6.28420\n",
      "Epoch 9482/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 87.0439 - mae: 7.2284 - val_loss: 193.1089 - val_mae: 7.1595\n",
      "\n",
      "Epoch 09482: val_mae did not improve from 6.28420\n",
      "Epoch 9483/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 66.7875 - mae: 6.4814 - val_loss: 193.4132 - val_mae: 7.3494\n",
      "\n",
      "Epoch 09483: val_mae did not improve from 6.28420\n",
      "Epoch 9484/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 58.7639 - mae: 6.0952 - val_loss: 206.6631 - val_mae: 7.9965\n",
      "\n",
      "Epoch 09484: val_mae did not improve from 6.28420\n",
      "Epoch 9485/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 96.3158 - mae: 7.6601 - val_loss: 228.1960 - val_mae: 9.2906\n",
      "\n",
      "Epoch 09485: val_mae did not improve from 6.28420\n",
      "Epoch 9486/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 120.7098 - mae: 8.7027 - val_loss: 204.4712 - val_mae: 7.9796\n",
      "\n",
      "Epoch 09486: val_mae did not improve from 6.28420\n",
      "Epoch 9487/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 62.9764 - mae: 6.2771 - val_loss: 202.9064 - val_mae: 7.9712\n",
      "\n",
      "Epoch 09487: val_mae did not improve from 6.28420\n",
      "Epoch 9488/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.6695 - mae: 6.6767 - val_loss: 197.8205 - val_mae: 7.6451\n",
      "\n",
      "Epoch 09488: val_mae did not improve from 6.28420\n",
      "Epoch 9489/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 72.8603 - mae: 6.7959 - val_loss: 211.3332 - val_mae: 8.2763\n",
      "\n",
      "Epoch 09489: val_mae did not improve from 6.28420\n",
      "Epoch 9490/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.2984 - mae: 6.9249 - val_loss: 234.2388 - val_mae: 9.5661\n",
      "\n",
      "Epoch 09490: val_mae did not improve from 6.28420\n",
      "Epoch 9491/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 95.1088 - mae: 7.7903 - val_loss: 223.1372 - val_mae: 8.8350\n",
      "\n",
      "Epoch 09491: val_mae did not improve from 6.28420\n",
      "Epoch 9492/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 88.0094 - mae: 7.5484 - val_loss: 237.5027 - val_mae: 9.5420\n",
      "\n",
      "Epoch 09492: val_mae did not improve from 6.28420\n",
      "Epoch 9493/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 86.7725 - mae: 7.4204 - val_loss: 208.8609 - val_mae: 8.2811\n",
      "\n",
      "Epoch 09493: val_mae did not improve from 6.28420\n",
      "Epoch 9494/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 106.6826 - mae: 8.1109 - val_loss: 227.6936 - val_mae: 9.0656\n",
      "\n",
      "Epoch 09494: val_mae did not improve from 6.28420\n",
      "Epoch 9495/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 82.9498 - mae: 7.2453 - val_loss: 194.2371 - val_mae: 7.3240\n",
      "\n",
      "Epoch 09495: val_mae did not improve from 6.28420\n",
      "Epoch 9496/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.5506 - mae: 6.7407 - val_loss: 221.2513 - val_mae: 8.7093\n",
      "\n",
      "Epoch 09496: val_mae did not improve from 6.28420\n",
      "Epoch 9497/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 67.2780 - mae: 6.4595 - val_loss: 199.4412 - val_mae: 7.6578\n",
      "\n",
      "Epoch 09497: val_mae did not improve from 6.28420\n",
      "Epoch 9498/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 55.8962 - mae: 5.9173 - val_loss: 197.5209 - val_mae: 7.4739\n",
      "\n",
      "Epoch 09498: val_mae did not improve from 6.28420\n",
      "Epoch 9499/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 68.5092 - mae: 6.5221 - val_loss: 227.1228 - val_mae: 9.0702\n",
      "\n",
      "Epoch 09499: val_mae did not improve from 6.28420\n",
      "Epoch 9500/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 106.6849 - mae: 8.2121 - val_loss: 219.0316 - val_mae: 8.5096\n",
      "\n",
      "Epoch 09500: val_mae did not improve from 6.28420\n",
      "Epoch 9501/10000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 84.5065 - mae: 7.3775 - val_loss: 222.9914 - val_mae: 8.6631\n",
      "\n",
      "Epoch 09501: val_mae did not improve from 6.28420\n",
      "Epoch 9502/10000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 62.2777 - mae: 6.3604 - val_loss: 223.7837 - val_mae: 8.7987\n",
      "\n",
      "Epoch 09502: val_mae did not improve from 6.28420\n",
      "Epoch 9503/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.7959 - mae: 6.9040 - val_loss: 261.1229 - val_mae: 10.8891\n",
      "\n",
      "Epoch 09503: val_mae did not improve from 6.28420\n",
      "Epoch 9504/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 148.3398 - mae: 9.7913 - val_loss: 230.2156 - val_mae: 9.4437\n",
      "\n",
      "Epoch 09504: val_mae did not improve from 6.28420\n",
      "Epoch 9505/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 92.8318 - mae: 7.8175 - val_loss: 200.1530 - val_mae: 7.6293\n",
      "\n",
      "Epoch 09505: val_mae did not improve from 6.28420\n",
      "Epoch 9506/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 119.4618 - mae: 8.7187 - val_loss: 208.4099 - val_mae: 8.0762\n",
      "\n",
      "Epoch 09506: val_mae did not improve from 6.28420\n",
      "Epoch 9507/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 91.8821 - mae: 7.7836 - val_loss: 190.0910 - val_mae: 7.2783\n",
      "\n",
      "Epoch 09507: val_mae did not improve from 6.28420\n",
      "Epoch 9508/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 82.8061 - mae: 7.3220 - val_loss: 214.4432 - val_mae: 8.4098\n",
      "\n",
      "Epoch 09508: val_mae did not improve from 6.28420\n",
      "Epoch 9509/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 67.4759 - mae: 6.6643 - val_loss: 241.5484 - val_mae: 9.5520\n",
      "\n",
      "Epoch 09509: val_mae did not improve from 6.28420\n",
      "Epoch 9510/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 131.5532 - mae: 9.1217 - val_loss: 301.2698 - val_mae: 11.7928\n",
      "\n",
      "Epoch 09510: val_mae did not improve from 6.28420\n",
      "Epoch 9511/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 156.4963 - mae: 9.9801 - val_loss: 204.3742 - val_mae: 8.1196\n",
      "\n",
      "Epoch 09511: val_mae did not improve from 6.28420\n",
      "Epoch 9512/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 97.8453 - mae: 7.7922 - val_loss: 257.8953 - val_mae: 10.6473\n",
      "\n",
      "Epoch 09512: val_mae did not improve from 6.28420\n",
      "Epoch 9513/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 123.9380 - mae: 8.7914 - val_loss: 207.9704 - val_mae: 8.2685\n",
      "\n",
      "Epoch 09513: val_mae did not improve from 6.28420\n",
      "Epoch 9514/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.6816 - mae: 7.3913 - val_loss: 201.3454 - val_mae: 7.8405\n",
      "\n",
      "Epoch 09514: val_mae did not improve from 6.28420\n",
      "Epoch 9515/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 87.0056 - mae: 7.3915 - val_loss: 230.6599 - val_mae: 8.9713\n",
      "\n",
      "Epoch 09515: val_mae did not improve from 6.28420\n",
      "Epoch 9516/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 110.6171 - mae: 8.3249 - val_loss: 204.8830 - val_mae: 7.9376\n",
      "\n",
      "Epoch 09516: val_mae did not improve from 6.28420\n",
      "Epoch 9517/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 82.3932 - mae: 7.2514 - val_loss: 250.3606 - val_mae: 10.0139\n",
      "\n",
      "Epoch 09517: val_mae did not improve from 6.28420\n",
      "Epoch 9518/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 121.3816 - mae: 8.6743 - val_loss: 241.3541 - val_mae: 9.6386\n",
      "\n",
      "Epoch 09518: val_mae did not improve from 6.28420\n",
      "Epoch 9519/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 82.5065 - mae: 7.2632 - val_loss: 219.5813 - val_mae: 8.9058\n",
      "\n",
      "Epoch 09519: val_mae did not improve from 6.28420\n",
      "Epoch 9520/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 116.8691 - mae: 8.4948 - val_loss: 197.4733 - val_mae: 7.5969\n",
      "\n",
      "Epoch 09520: val_mae did not improve from 6.28420\n",
      "Epoch 9521/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 93.3878 - mae: 7.6509 - val_loss: 254.2260 - val_mae: 10.1738\n",
      "\n",
      "Epoch 09521: val_mae did not improve from 6.28420\n",
      "Epoch 9522/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 100.6749 - mae: 7.9263 - val_loss: 205.0461 - val_mae: 8.1819\n",
      "\n",
      "Epoch 09522: val_mae did not improve from 6.28420\n",
      "Epoch 9523/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.5662 - mae: 6.7228 - val_loss: 207.7598 - val_mae: 8.2080\n",
      "\n",
      "Epoch 09523: val_mae did not improve from 6.28420\n",
      "Epoch 9524/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.4276 - mae: 6.9441 - val_loss: 383.6501 - val_mae: 14.7085\n",
      "\n",
      "Epoch 09524: val_mae did not improve from 6.28420\n",
      "Epoch 9525/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 132.5257 - mae: 9.3694 - val_loss: 209.1128 - val_mae: 8.0819\n",
      "\n",
      "Epoch 09525: val_mae did not improve from 6.28420\n",
      "Epoch 9526/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 110.9547 - mae: 8.3562 - val_loss: 224.4195 - val_mae: 8.9034\n",
      "\n",
      "Epoch 09526: val_mae did not improve from 6.28420\n",
      "Epoch 9527/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 66.1591 - mae: 6.4475 - val_loss: 193.3604 - val_mae: 7.4186\n",
      "\n",
      "Epoch 09527: val_mae did not improve from 6.28420\n",
      "Epoch 9528/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 68.7678 - mae: 6.5162 - val_loss: 214.0745 - val_mae: 8.4319\n",
      "\n",
      "Epoch 09528: val_mae did not improve from 6.28420\n",
      "Epoch 9529/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.7689 - mae: 6.8168 - val_loss: 253.3104 - val_mae: 9.9646\n",
      "\n",
      "Epoch 09529: val_mae did not improve from 6.28420\n",
      "Epoch 9530/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 114.7988 - mae: 8.5211 - val_loss: 240.8057 - val_mae: 9.7407\n",
      "\n",
      "Epoch 09530: val_mae did not improve from 6.28420\n",
      "Epoch 9531/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 72.0091 - mae: 6.7428 - val_loss: 199.5489 - val_mae: 7.3942\n",
      "\n",
      "Epoch 09531: val_mae did not improve from 6.28420\n",
      "Epoch 9532/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 85.3832 - mae: 7.3785 - val_loss: 270.7836 - val_mae: 10.3730\n",
      "\n",
      "Epoch 09532: val_mae did not improve from 6.28420\n",
      "Epoch 9533/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 102.4345 - mae: 8.0633 - val_loss: 216.6764 - val_mae: 8.2267\n",
      "\n",
      "Epoch 09533: val_mae did not improve from 6.28420\n",
      "Epoch 9534/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 79.7700 - mae: 6.9080 - val_loss: 201.4280 - val_mae: 7.4682\n",
      "\n",
      "Epoch 09534: val_mae did not improve from 6.28420\n",
      "Epoch 9535/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 51.6600 - mae: 5.6938 - val_loss: 196.0236 - val_mae: 7.3186\n",
      "\n",
      "Epoch 09535: val_mae did not improve from 6.28420\n",
      "Epoch 9536/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.7058 - mae: 7.1333 - val_loss: 241.4064 - val_mae: 9.7845\n",
      "\n",
      "Epoch 09536: val_mae did not improve from 6.28420\n",
      "Epoch 9537/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 79.9690 - mae: 7.1443 - val_loss: 196.3490 - val_mae: 7.5368\n",
      "\n",
      "Epoch 09537: val_mae did not improve from 6.28420\n",
      "Epoch 9538/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.4144 - mae: 7.1276 - val_loss: 208.4327 - val_mae: 8.4292\n",
      "\n",
      "Epoch 09538: val_mae did not improve from 6.28420\n",
      "Epoch 9539/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 92.9623 - mae: 7.5485 - val_loss: 245.9630 - val_mae: 9.6894\n",
      "\n",
      "Epoch 09539: val_mae did not improve from 6.28420\n",
      "Epoch 9540/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 85.5077 - mae: 7.2744 - val_loss: 199.6758 - val_mae: 7.7721\n",
      "\n",
      "Epoch 09540: val_mae did not improve from 6.28420\n",
      "Epoch 9541/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 79.9483 - mae: 7.3304 - val_loss: 208.3210 - val_mae: 8.3131\n",
      "\n",
      "Epoch 09541: val_mae did not improve from 6.28420\n",
      "Epoch 9542/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 80.2633 - mae: 7.0920 - val_loss: 214.8627 - val_mae: 8.6351\n",
      "\n",
      "Epoch 09542: val_mae did not improve from 6.28420\n",
      "Epoch 9543/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 82.7471 - mae: 6.9955 - val_loss: 209.3234 - val_mae: 8.2760\n",
      "\n",
      "Epoch 09543: val_mae did not improve from 6.28420\n",
      "Epoch 9544/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 82.8801 - mae: 7.1906 - val_loss: 234.0158 - val_mae: 9.4050\n",
      "\n",
      "Epoch 09544: val_mae did not improve from 6.28420\n",
      "Epoch 9545/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 81.0615 - mae: 7.2526 - val_loss: 204.9681 - val_mae: 8.0466\n",
      "\n",
      "Epoch 09545: val_mae did not improve from 6.28420\n",
      "Epoch 9546/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 83.3684 - mae: 7.3334 - val_loss: 208.4422 - val_mae: 8.0948\n",
      "\n",
      "Epoch 09546: val_mae did not improve from 6.28420\n",
      "Epoch 9547/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 71.0893 - mae: 6.5719 - val_loss: 209.5526 - val_mae: 8.4623\n",
      "\n",
      "Epoch 09547: val_mae did not improve from 6.28420\n",
      "Epoch 9548/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 80.0096 - mae: 6.9009 - val_loss: 202.0647 - val_mae: 7.9150\n",
      "\n",
      "Epoch 09548: val_mae did not improve from 6.28420\n",
      "Epoch 9549/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 61.4947 - mae: 6.1019 - val_loss: 190.2241 - val_mae: 7.1115\n",
      "\n",
      "Epoch 09549: val_mae did not improve from 6.28420\n",
      "Epoch 9550/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 58.5496 - mae: 6.1206 - val_loss: 216.9585 - val_mae: 8.6850\n",
      "\n",
      "Epoch 09550: val_mae did not improve from 6.28420\n",
      "Epoch 9551/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.0342 - mae: 7.0742 - val_loss: 200.2962 - val_mae: 7.4614\n",
      "\n",
      "Epoch 09551: val_mae did not improve from 6.28420\n",
      "Epoch 9552/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 66.1303 - mae: 6.4951 - val_loss: 218.5338 - val_mae: 8.4966\n",
      "\n",
      "Epoch 09552: val_mae did not improve from 6.28420\n",
      "Epoch 9553/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 79.1974 - mae: 7.0381 - val_loss: 206.0283 - val_mae: 7.8897\n",
      "\n",
      "Epoch 09553: val_mae did not improve from 6.28420\n",
      "Epoch 9554/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 111.9904 - mae: 8.3115 - val_loss: 212.9668 - val_mae: 8.1875\n",
      "\n",
      "Epoch 09554: val_mae did not improve from 6.28420\n",
      "Epoch 9555/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 114.8275 - mae: 8.4983 - val_loss: 235.2248 - val_mae: 9.4804\n",
      "\n",
      "Epoch 09555: val_mae did not improve from 6.28420\n",
      "Epoch 9556/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.3690 - mae: 7.0298 - val_loss: 238.1863 - val_mae: 9.3658\n",
      "\n",
      "Epoch 09556: val_mae did not improve from 6.28420\n",
      "Epoch 9557/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 89.5956 - mae: 7.4374 - val_loss: 230.1458 - val_mae: 8.9558\n",
      "\n",
      "Epoch 09557: val_mae did not improve from 6.28420\n",
      "Epoch 9558/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 86.5284 - mae: 7.3457 - val_loss: 206.5097 - val_mae: 8.0499\n",
      "\n",
      "Epoch 09558: val_mae did not improve from 6.28420\n",
      "Epoch 9559/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 89.0802 - mae: 7.4371 - val_loss: 225.8743 - val_mae: 9.2328\n",
      "\n",
      "Epoch 09559: val_mae did not improve from 6.28420\n",
      "Epoch 9560/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 75.4097 - mae: 7.0197 - val_loss: 204.5830 - val_mae: 8.0434\n",
      "\n",
      "Epoch 09560: val_mae did not improve from 6.28420\n",
      "Epoch 9561/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.2046 - mae: 6.5055 - val_loss: 266.9049 - val_mae: 10.9651\n",
      "\n",
      "Epoch 09561: val_mae did not improve from 6.28420\n",
      "Epoch 9562/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 101.4244 - mae: 8.0274 - val_loss: 238.8728 - val_mae: 9.6762\n",
      "\n",
      "Epoch 09562: val_mae did not improve from 6.28420\n",
      "Epoch 9563/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 137.1108 - mae: 9.3252 - val_loss: 296.4500 - val_mae: 11.5828\n",
      "\n",
      "Epoch 09563: val_mae did not improve from 6.28420\n",
      "Epoch 9564/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 114.7083 - mae: 8.4263 - val_loss: 299.3233 - val_mae: 10.6007\n",
      "\n",
      "Epoch 09564: val_mae did not improve from 6.28420\n",
      "Epoch 9565/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 87.4437 - mae: 7.4668 - val_loss: 224.9573 - val_mae: 9.0710\n",
      "\n",
      "Epoch 09565: val_mae did not improve from 6.28420\n",
      "Epoch 9566/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 86.1470 - mae: 7.5863 - val_loss: 209.3236 - val_mae: 8.1182\n",
      "\n",
      "Epoch 09566: val_mae did not improve from 6.28420\n",
      "Epoch 9567/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 96.2083 - mae: 7.7771 - val_loss: 211.3924 - val_mae: 8.1080\n",
      "\n",
      "Epoch 09567: val_mae did not improve from 6.28420\n",
      "Epoch 9568/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 67.6710 - mae: 6.4663 - val_loss: 202.3922 - val_mae: 7.7227\n",
      "\n",
      "Epoch 09568: val_mae did not improve from 6.28420\n",
      "Epoch 9569/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 72.2519 - mae: 6.7109 - val_loss: 203.9624 - val_mae: 7.4397\n",
      "\n",
      "Epoch 09569: val_mae did not improve from 6.28420\n",
      "Epoch 9570/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 88.0317 - mae: 7.6278 - val_loss: 287.8401 - val_mae: 11.6243\n",
      "\n",
      "Epoch 09570: val_mae did not improve from 6.28420\n",
      "Epoch 9571/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 145.6939 - mae: 9.8346 - val_loss: 265.8144 - val_mae: 10.2138\n",
      "\n",
      "Epoch 09571: val_mae did not improve from 6.28420\n",
      "Epoch 9572/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 100.4182 - mae: 7.8970 - val_loss: 277.6118 - val_mae: 9.0750\n",
      "\n",
      "Epoch 09572: val_mae did not improve from 6.28420\n",
      "Epoch 9573/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 95.5520 - mae: 7.6033 - val_loss: 239.5517 - val_mae: 8.1242\n",
      "\n",
      "Epoch 09573: val_mae did not improve from 6.28420\n",
      "Epoch 9574/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 93.5980 - mae: 7.8002 - val_loss: 241.2659 - val_mae: 8.0405\n",
      "\n",
      "Epoch 09574: val_mae did not improve from 6.28420\n",
      "Epoch 9575/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.2502 - mae: 7.2959 - val_loss: 352.7806 - val_mae: 12.7525\n",
      "\n",
      "Epoch 09575: val_mae did not improve from 6.28420\n",
      "Epoch 9576/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 125.7351 - mae: 8.8095 - val_loss: 267.9528 - val_mae: 9.4386\n",
      "\n",
      "Epoch 09576: val_mae did not improve from 6.28420\n",
      "Epoch 9577/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 92.4573 - mae: 7.7910 - val_loss: 269.0934 - val_mae: 9.3989\n",
      "\n",
      "Epoch 09577: val_mae did not improve from 6.28420\n",
      "Epoch 9578/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 83.2485 - mae: 7.2469 - val_loss: 258.8628 - val_mae: 8.6644\n",
      "\n",
      "Epoch 09578: val_mae did not improve from 6.28420\n",
      "Epoch 9579/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.3883 - mae: 7.3889 - val_loss: 249.6879 - val_mae: 8.3079\n",
      "\n",
      "Epoch 09579: val_mae did not improve from 6.28420\n",
      "Epoch 9580/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 91.7945 - mae: 7.5090 - val_loss: 279.7278 - val_mae: 9.4315\n",
      "\n",
      "Epoch 09580: val_mae did not improve from 6.28420\n",
      "Epoch 9581/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 78.1267 - mae: 7.1170 - val_loss: 260.2901 - val_mae: 8.7699\n",
      "\n",
      "Epoch 09581: val_mae did not improve from 6.28420\n",
      "Epoch 9582/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 120.3649 - mae: 8.5518 - val_loss: 250.9791 - val_mae: 8.7143\n",
      "\n",
      "Epoch 09582: val_mae did not improve from 6.28420\n",
      "Epoch 9583/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 103.3792 - mae: 8.1948 - val_loss: 294.4171 - val_mae: 10.3158\n",
      "\n",
      "Epoch 09583: val_mae did not improve from 6.28420\n",
      "Epoch 9584/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 103.6865 - mae: 8.3942 - val_loss: 268.0824 - val_mae: 9.4760\n",
      "\n",
      "Epoch 09584: val_mae did not improve from 6.28420\n",
      "Epoch 9585/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 79.2971 - mae: 7.1367 - val_loss: 282.6440 - val_mae: 10.1178\n",
      "\n",
      "Epoch 09585: val_mae did not improve from 6.28420\n",
      "Epoch 9586/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 78.1489 - mae: 7.1640 - val_loss: 274.8574 - val_mae: 9.4742\n",
      "\n",
      "Epoch 09586: val_mae did not improve from 6.28420\n",
      "Epoch 9587/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 82.9808 - mae: 7.0828 - val_loss: 282.6079 - val_mae: 9.9715\n",
      "\n",
      "Epoch 09587: val_mae did not improve from 6.28420\n",
      "Epoch 9588/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 106.4153 - mae: 8.1698 - val_loss: 242.8738 - val_mae: 7.9972\n",
      "\n",
      "Epoch 09588: val_mae did not improve from 6.28420\n",
      "Epoch 9589/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.9098 - mae: 6.7197 - val_loss: 262.3551 - val_mae: 8.9831\n",
      "\n",
      "Epoch 09589: val_mae did not improve from 6.28420\n",
      "Epoch 9590/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 71.5499 - mae: 6.6494 - val_loss: 274.7520 - val_mae: 9.9175\n",
      "\n",
      "Epoch 09590: val_mae did not improve from 6.28420\n",
      "Epoch 9591/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 70.9296 - mae: 6.6757 - val_loss: 250.5309 - val_mae: 8.5037\n",
      "\n",
      "Epoch 09591: val_mae did not improve from 6.28420\n",
      "Epoch 9592/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 70.9515 - mae: 6.7118 - val_loss: 264.5764 - val_mae: 9.4554\n",
      "\n",
      "Epoch 09592: val_mae did not improve from 6.28420\n",
      "Epoch 9593/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 85.8596 - mae: 7.3674 - val_loss: 225.2460 - val_mae: 7.1872\n",
      "\n",
      "Epoch 09593: val_mae did not improve from 6.28420\n",
      "Epoch 9594/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 66.9716 - mae: 6.4352 - val_loss: 295.7561 - val_mae: 10.7258\n",
      "\n",
      "Epoch 09594: val_mae did not improve from 6.28420\n",
      "Epoch 9595/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 94.4083 - mae: 7.6155 - val_loss: 244.4799 - val_mae: 8.3188\n",
      "\n",
      "Epoch 09595: val_mae did not improve from 6.28420\n",
      "Epoch 9596/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 75.9521 - mae: 6.8274 - val_loss: 287.2895 - val_mae: 9.8123\n",
      "\n",
      "Epoch 09596: val_mae did not improve from 6.28420\n",
      "Epoch 9597/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 97.6299 - mae: 7.8022 - val_loss: 239.0287 - val_mae: 8.0716\n",
      "\n",
      "Epoch 09597: val_mae did not improve from 6.28420\n",
      "Epoch 9598/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 91.2993 - mae: 7.5599 - val_loss: 305.2747 - val_mae: 10.8202\n",
      "\n",
      "Epoch 09598: val_mae did not improve from 6.28420\n",
      "Epoch 9599/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 103.9006 - mae: 8.0722 - val_loss: 328.3889 - val_mae: 11.3035\n",
      "\n",
      "Epoch 09599: val_mae did not improve from 6.28420\n",
      "Epoch 9600/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 153.1975 - mae: 9.6758 - val_loss: 272.0018 - val_mae: 9.7147\n",
      "\n",
      "Epoch 09600: val_mae did not improve from 6.28420\n",
      "Epoch 9601/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 65.8776 - mae: 6.3340 - val_loss: 249.5253 - val_mae: 8.4481\n",
      "\n",
      "Epoch 09601: val_mae did not improve from 6.28420\n",
      "Epoch 9602/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 89.9650 - mae: 7.7359 - val_loss: 246.8880 - val_mae: 8.3265\n",
      "\n",
      "Epoch 09602: val_mae did not improve from 6.28420\n",
      "Epoch 9603/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 75.4500 - mae: 7.0132 - val_loss: 244.1127 - val_mae: 7.9964\n",
      "\n",
      "Epoch 09603: val_mae did not improve from 6.28420\n",
      "Epoch 9604/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 70.4562 - mae: 6.6521 - val_loss: 279.3610 - val_mae: 9.6199\n",
      "\n",
      "Epoch 09604: val_mae did not improve from 6.28420\n",
      "Epoch 9605/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 77.0933 - mae: 6.9955 - val_loss: 304.2539 - val_mae: 10.8025\n",
      "\n",
      "Epoch 09605: val_mae did not improve from 6.28420\n",
      "Epoch 9606/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 85.7133 - mae: 7.3580 - val_loss: 240.5568 - val_mae: 7.8944\n",
      "\n",
      "Epoch 09606: val_mae did not improve from 6.28420\n",
      "Epoch 9607/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 75.8535 - mae: 6.8116 - val_loss: 234.5095 - val_mae: 7.6287\n",
      "\n",
      "Epoch 09607: val_mae did not improve from 6.28420\n",
      "Epoch 9608/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 57.1371 - mae: 6.0098 - val_loss: 284.0563 - val_mae: 10.1956\n",
      "\n",
      "Epoch 09608: val_mae did not improve from 6.28420\n",
      "Epoch 9609/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 64.7540 - mae: 6.2935 - val_loss: 238.2443 - val_mae: 7.9958\n",
      "\n",
      "Epoch 09609: val_mae did not improve from 6.28420\n",
      "Epoch 9610/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 75.0186 - mae: 6.8727 - val_loss: 233.6482 - val_mae: 7.7359\n",
      "\n",
      "Epoch 09610: val_mae did not improve from 6.28420\n",
      "Epoch 9611/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 84.1336 - mae: 7.4458 - val_loss: 232.2553 - val_mae: 7.5919\n",
      "\n",
      "Epoch 09611: val_mae did not improve from 6.28420\n",
      "Epoch 9612/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 85.1328 - mae: 7.5538 - val_loss: 256.2296 - val_mae: 8.7689\n",
      "\n",
      "Epoch 09612: val_mae did not improve from 6.28420\n",
      "Epoch 9613/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.9752 - mae: 6.5476 - val_loss: 234.0043 - val_mae: 7.7306\n",
      "\n",
      "Epoch 09613: val_mae did not improve from 6.28420\n",
      "Epoch 9614/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 104.7914 - mae: 7.9726 - val_loss: 238.9519 - val_mae: 7.9380\n",
      "\n",
      "Epoch 09614: val_mae did not improve from 6.28420\n",
      "Epoch 9615/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.5384 - mae: 6.7710 - val_loss: 259.5750 - val_mae: 8.9167\n",
      "\n",
      "Epoch 09615: val_mae did not improve from 6.28420\n",
      "Epoch 9616/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 76.7489 - mae: 7.0388 - val_loss: 264.8940 - val_mae: 9.2599\n",
      "\n",
      "Epoch 09616: val_mae did not improve from 6.28420\n",
      "Epoch 9617/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 64.1224 - mae: 6.2994 - val_loss: 257.5199 - val_mae: 8.7868\n",
      "\n",
      "Epoch 09617: val_mae did not improve from 6.28420\n",
      "Epoch 9618/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 103.7445 - mae: 8.1211 - val_loss: 237.4148 - val_mae: 7.9025\n",
      "\n",
      "Epoch 09618: val_mae did not improve from 6.28420\n",
      "Epoch 9619/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.6901 - mae: 6.9269 - val_loss: 247.2330 - val_mae: 8.3845\n",
      "\n",
      "Epoch 09619: val_mae did not improve from 6.28420\n",
      "Epoch 9620/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 73.6427 - mae: 6.7937 - val_loss: 243.4380 - val_mae: 7.9831\n",
      "\n",
      "Epoch 09620: val_mae did not improve from 6.28420\n",
      "Epoch 9621/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 63.8994 - mae: 6.3668 - val_loss: 261.9949 - val_mae: 8.9932\n",
      "\n",
      "Epoch 09621: val_mae did not improve from 6.28420\n",
      "Epoch 9622/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 72.1350 - mae: 6.7457 - val_loss: 285.9446 - val_mae: 9.9765\n",
      "\n",
      "Epoch 09622: val_mae did not improve from 6.28420\n",
      "Epoch 9623/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 126.3724 - mae: 9.0941 - val_loss: 273.3784 - val_mae: 9.3590\n",
      "\n",
      "Epoch 09623: val_mae did not improve from 6.28420\n",
      "Epoch 9624/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 82.1045 - mae: 7.1300 - val_loss: 239.2234 - val_mae: 8.1350\n",
      "\n",
      "Epoch 09624: val_mae did not improve from 6.28420\n",
      "Epoch 9625/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 74.7638 - mae: 6.8942 - val_loss: 237.6315 - val_mae: 7.9639\n",
      "\n",
      "Epoch 09625: val_mae did not improve from 6.28420\n",
      "Epoch 9626/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 83.7135 - mae: 7.1045 - val_loss: 245.9842 - val_mae: 8.3196\n",
      "\n",
      "Epoch 09626: val_mae did not improve from 6.28420\n",
      "Epoch 9627/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 99.0317 - mae: 7.9364 - val_loss: 239.7699 - val_mae: 7.9852\n",
      "\n",
      "Epoch 09627: val_mae did not improve from 6.28420\n",
      "Epoch 9628/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 59.3637 - mae: 6.0153 - val_loss: 249.2259 - val_mae: 8.5495\n",
      "\n",
      "Epoch 09628: val_mae did not improve from 6.28420\n",
      "Epoch 9629/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 75.2078 - mae: 6.8003 - val_loss: 326.2989 - val_mae: 11.3492\n",
      "\n",
      "Epoch 09629: val_mae did not improve from 6.28420\n",
      "Epoch 9630/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 81.2131 - mae: 6.9484 - val_loss: 252.3707 - val_mae: 8.6206\n",
      "\n",
      "Epoch 09630: val_mae did not improve from 6.28420\n",
      "Epoch 9631/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 88.4500 - mae: 7.4474 - val_loss: 234.8416 - val_mae: 7.5936\n",
      "\n",
      "Epoch 09631: val_mae did not improve from 6.28420\n",
      "Epoch 9632/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 66.6596 - mae: 6.4680 - val_loss: 290.6623 - val_mae: 10.6616\n",
      "\n",
      "Epoch 09632: val_mae did not improve from 6.28420\n",
      "Epoch 9633/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 95.1920 - mae: 7.8638 - val_loss: 275.0853 - val_mae: 9.7984\n",
      "\n",
      "Epoch 09633: val_mae did not improve from 6.28420\n",
      "Epoch 9634/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 103.6660 - mae: 8.1847 - val_loss: 260.0008 - val_mae: 8.9121\n",
      "\n",
      "Epoch 09634: val_mae did not improve from 6.28420\n",
      "Epoch 9635/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 92.1611 - mae: 7.6521 - val_loss: 281.7507 - val_mae: 10.0962\n",
      "\n",
      "Epoch 09635: val_mae did not improve from 6.28420\n",
      "Epoch 9636/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 81.7669 - mae: 7.2159 - val_loss: 253.6324 - val_mae: 8.7449\n",
      "\n",
      "Epoch 09636: val_mae did not improve from 6.28420\n",
      "Epoch 9637/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 118.2778 - mae: 8.5005 - val_loss: 238.8035 - val_mae: 7.8574\n",
      "\n",
      "Epoch 09637: val_mae did not improve from 6.28420\n",
      "Epoch 9638/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 72.4578 - mae: 6.8661 - val_loss: 229.2224 - val_mae: 7.2389\n",
      "\n",
      "Epoch 09638: val_mae did not improve from 6.28420\n",
      "Epoch 9639/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 70.5869 - mae: 6.7936 - val_loss: 274.2762 - val_mae: 9.6923\n",
      "\n",
      "Epoch 09639: val_mae did not improve from 6.28420\n",
      "Epoch 9640/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 67.7674 - mae: 6.5114 - val_loss: 235.8630 - val_mae: 7.6166\n",
      "\n",
      "Epoch 09640: val_mae did not improve from 6.28420\n",
      "Epoch 9641/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 61.8726 - mae: 6.3362 - val_loss: 293.5478 - val_mae: 10.1484\n",
      "\n",
      "Epoch 09641: val_mae did not improve from 6.28420\n",
      "Epoch 9642/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 114.0527 - mae: 8.6290 - val_loss: 277.4957 - val_mae: 9.4232\n",
      "\n",
      "Epoch 09642: val_mae did not improve from 6.28420\n",
      "Epoch 9643/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 66.4993 - mae: 6.5111 - val_loss: 239.9109 - val_mae: 7.7347\n",
      "\n",
      "Epoch 09643: val_mae did not improve from 6.28420\n",
      "Epoch 9644/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 119.5393 - mae: 8.8282 - val_loss: 308.3366 - val_mae: 10.8467\n",
      "\n",
      "Epoch 09644: val_mae did not improve from 6.28420\n",
      "Epoch 9645/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 92.5669 - mae: 7.6598 - val_loss: 262.4994 - val_mae: 9.1859\n",
      "\n",
      "Epoch 09645: val_mae did not improve from 6.28420\n",
      "Epoch 9646/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 81.8497 - mae: 7.3666 - val_loss: 262.7238 - val_mae: 9.2314\n",
      "\n",
      "Epoch 09646: val_mae did not improve from 6.28420\n",
      "Epoch 9647/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 102.2204 - mae: 7.9784 - val_loss: 283.3835 - val_mae: 10.0382\n",
      "\n",
      "Epoch 09647: val_mae did not improve from 6.28420\n",
      "Epoch 9648/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 103.5713 - mae: 8.0975 - val_loss: 255.4552 - val_mae: 8.7935\n",
      "\n",
      "Epoch 09648: val_mae did not improve from 6.28420\n",
      "Epoch 9649/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 85.4681 - mae: 7.4333 - val_loss: 272.2425 - val_mae: 9.4928\n",
      "\n",
      "Epoch 09649: val_mae did not improve from 6.28420\n",
      "Epoch 9650/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 112.0905 - mae: 8.5182 - val_loss: 251.9517 - val_mae: 8.4662\n",
      "\n",
      "Epoch 09650: val_mae did not improve from 6.28420\n",
      "Epoch 9651/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 60.2321 - mae: 5.9670 - val_loss: 240.0900 - val_mae: 7.9478\n",
      "\n",
      "Epoch 09651: val_mae did not improve from 6.28420\n",
      "Epoch 9652/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.2706 - mae: 6.8062 - val_loss: 233.8138 - val_mae: 7.8198\n",
      "\n",
      "Epoch 09652: val_mae did not improve from 6.28420\n",
      "Epoch 9653/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 68.0926 - mae: 6.4191 - val_loss: 370.1664 - val_mae: 13.7810\n",
      "\n",
      "Epoch 09653: val_mae did not improve from 6.28420\n",
      "Epoch 9654/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 109.6762 - mae: 8.4451 - val_loss: 238.2616 - val_mae: 7.9007\n",
      "\n",
      "Epoch 09654: val_mae did not improve from 6.28420\n",
      "Epoch 9655/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 57.9697 - mae: 6.0656 - val_loss: 234.6556 - val_mae: 7.7574\n",
      "\n",
      "Epoch 09655: val_mae did not improve from 6.28420\n",
      "Epoch 9656/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 67.3273 - mae: 6.3655 - val_loss: 243.2330 - val_mae: 8.3109\n",
      "\n",
      "Epoch 09656: val_mae did not improve from 6.28420\n",
      "Epoch 9657/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 70.1683 - mae: 6.6494 - val_loss: 239.5917 - val_mae: 8.1201\n",
      "\n",
      "Epoch 09657: val_mae did not improve from 6.28420\n",
      "Epoch 9658/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 94.7183 - mae: 7.7492 - val_loss: 253.1194 - val_mae: 8.5512\n",
      "\n",
      "Epoch 09658: val_mae did not improve from 6.28420\n",
      "Epoch 9659/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 105.2237 - mae: 8.2018 - val_loss: 247.9372 - val_mae: 8.3271\n",
      "\n",
      "Epoch 09659: val_mae did not improve from 6.28420\n",
      "Epoch 9660/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 75.2418 - mae: 7.0027 - val_loss: 241.5417 - val_mae: 8.0681\n",
      "\n",
      "Epoch 09660: val_mae did not improve from 6.28420\n",
      "Epoch 9661/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 91.3799 - mae: 7.5305 - val_loss: 280.1682 - val_mae: 9.9337\n",
      "\n",
      "Epoch 09661: val_mae did not improve from 6.28420\n",
      "Epoch 9662/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 64.4680 - mae: 6.3952 - val_loss: 232.8054 - val_mae: 7.7070\n",
      "\n",
      "Epoch 09662: val_mae did not improve from 6.28420\n",
      "Epoch 9663/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 70.7429 - mae: 6.8022 - val_loss: 285.4571 - val_mae: 10.3524\n",
      "\n",
      "Epoch 09663: val_mae did not improve from 6.28420\n",
      "Epoch 9664/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 75.6565 - mae: 7.0250 - val_loss: 254.1556 - val_mae: 8.8344\n",
      "\n",
      "Epoch 09664: val_mae did not improve from 6.28420\n",
      "Epoch 9665/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 68.6768 - mae: 6.4441 - val_loss: 324.2342 - val_mae: 11.4079\n",
      "\n",
      "Epoch 09665: val_mae did not improve from 6.28420\n",
      "Epoch 9666/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 116.2964 - mae: 8.2637 - val_loss: 247.3562 - val_mae: 8.7170\n",
      "\n",
      "Epoch 09666: val_mae did not improve from 6.28420\n",
      "Epoch 9667/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.3903 - mae: 6.5694 - val_loss: 229.7794 - val_mae: 7.4375\n",
      "\n",
      "Epoch 09667: val_mae did not improve from 6.28420\n",
      "Epoch 9668/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 58.3845 - mae: 6.0471 - val_loss: 253.5247 - val_mae: 8.9659\n",
      "\n",
      "Epoch 09668: val_mae did not improve from 6.28420\n",
      "Epoch 9669/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 82.6997 - mae: 7.2518 - val_loss: 227.0579 - val_mae: 7.3086\n",
      "\n",
      "Epoch 09669: val_mae did not improve from 6.28420\n",
      "Epoch 9670/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 82.7359 - mae: 7.2931 - val_loss: 248.7427 - val_mae: 8.3842\n",
      "\n",
      "Epoch 09670: val_mae did not improve from 6.28420\n",
      "Epoch 9671/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 60.1168 - mae: 6.1828 - val_loss: 242.2575 - val_mae: 8.3673\n",
      "\n",
      "Epoch 09671: val_mae did not improve from 6.28420\n",
      "Epoch 9672/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 68.9957 - mae: 6.5959 - val_loss: 243.6774 - val_mae: 8.1083\n",
      "\n",
      "Epoch 09672: val_mae did not improve from 6.28420\n",
      "Epoch 9673/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 58.1228 - mae: 6.0784 - val_loss: 245.1727 - val_mae: 8.3979\n",
      "\n",
      "Epoch 09673: val_mae did not improve from 6.28420\n",
      "Epoch 9674/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 91.1714 - mae: 7.7104 - val_loss: 262.6640 - val_mae: 9.2040\n",
      "\n",
      "Epoch 09674: val_mae did not improve from 6.28420\n",
      "Epoch 9675/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.4000 - mae: 7.5914 - val_loss: 242.0379 - val_mae: 8.3021\n",
      "\n",
      "Epoch 09675: val_mae did not improve from 6.28420\n",
      "Epoch 9676/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 72.7892 - mae: 6.7465 - val_loss: 305.3065 - val_mae: 11.6525\n",
      "\n",
      "Epoch 09676: val_mae did not improve from 6.28420\n",
      "Epoch 9677/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 123.7608 - mae: 8.9139 - val_loss: 250.0685 - val_mae: 8.8415\n",
      "\n",
      "Epoch 09677: val_mae did not improve from 6.28420\n",
      "Epoch 9678/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 100.3184 - mae: 7.8396 - val_loss: 381.2599 - val_mae: 13.9325\n",
      "\n",
      "Epoch 09678: val_mae did not improve from 6.28420\n",
      "Epoch 9679/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 100.2168 - mae: 7.9925 - val_loss: 236.1914 - val_mae: 8.1330\n",
      "\n",
      "Epoch 09679: val_mae did not improve from 6.28420\n",
      "Epoch 9680/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.9133 - mae: 6.6998 - val_loss: 236.4987 - val_mae: 7.8994\n",
      "\n",
      "Epoch 09680: val_mae did not improve from 6.28420\n",
      "Epoch 9681/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 80.4510 - mae: 7.0862 - val_loss: 275.9373 - val_mae: 10.2895\n",
      "\n",
      "Epoch 09681: val_mae did not improve from 6.28420\n",
      "Epoch 9682/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 65.5244 - mae: 6.4009 - val_loss: 253.3372 - val_mae: 9.1262\n",
      "\n",
      "Epoch 09682: val_mae did not improve from 6.28420\n",
      "Epoch 9683/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 63.3182 - mae: 6.3536 - val_loss: 229.6723 - val_mae: 7.8197\n",
      "\n",
      "Epoch 09683: val_mae did not improve from 6.28420\n",
      "Epoch 9684/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 93.2277 - mae: 7.7652 - val_loss: 245.5656 - val_mae: 8.8647\n",
      "\n",
      "Epoch 09684: val_mae did not improve from 6.28420\n",
      "Epoch 9685/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 77.8582 - mae: 7.0122 - val_loss: 265.9145 - val_mae: 9.7568\n",
      "\n",
      "Epoch 09685: val_mae did not improve from 6.28420\n",
      "Epoch 9686/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 70.8971 - mae: 6.6432 - val_loss: 313.8097 - val_mae: 11.2525\n",
      "\n",
      "Epoch 09686: val_mae did not improve from 6.28420\n",
      "Epoch 9687/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 96.9140 - mae: 7.8426 - val_loss: 243.7772 - val_mae: 8.9006\n",
      "\n",
      "Epoch 09687: val_mae did not improve from 6.28420\n",
      "Epoch 9688/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 89.0864 - mae: 7.4707 - val_loss: 264.6809 - val_mae: 9.5090\n",
      "\n",
      "Epoch 09688: val_mae did not improve from 6.28420\n",
      "Epoch 9689/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 79.4915 - mae: 7.1047 - val_loss: 237.7771 - val_mae: 8.2104\n",
      "\n",
      "Epoch 09689: val_mae did not improve from 6.28420\n",
      "Epoch 9690/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 70.7171 - mae: 6.7179 - val_loss: 248.7961 - val_mae: 8.8965\n",
      "\n",
      "Epoch 09690: val_mae did not improve from 6.28420\n",
      "Epoch 9691/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.5835 - mae: 7.0929 - val_loss: 265.3615 - val_mae: 9.3420\n",
      "\n",
      "Epoch 09691: val_mae did not improve from 6.28420\n",
      "Epoch 9692/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 96.7986 - mae: 7.7561 - val_loss: 255.2276 - val_mae: 9.0763\n",
      "\n",
      "Epoch 09692: val_mae did not improve from 6.28420\n",
      "Epoch 9693/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 61.6458 - mae: 6.2272 - val_loss: 231.5535 - val_mae: 7.9229\n",
      "\n",
      "Epoch 09693: val_mae did not improve from 6.28420\n",
      "Epoch 9694/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.6664 - mae: 7.6165 - val_loss: 234.2613 - val_mae: 7.8514\n",
      "\n",
      "Epoch 09694: val_mae did not improve from 6.28420\n",
      "Epoch 9695/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 55.8506 - mae: 5.9055 - val_loss: 227.0746 - val_mae: 7.3751\n",
      "\n",
      "Epoch 09695: val_mae did not improve from 6.28420\n",
      "Epoch 9696/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 80.4622 - mae: 6.8975 - val_loss: 253.4043 - val_mae: 8.7208\n",
      "\n",
      "Epoch 09696: val_mae did not improve from 6.28420\n",
      "Epoch 9697/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 79.2298 - mae: 7.1923 - val_loss: 316.4716 - val_mae: 11.5332\n",
      "\n",
      "Epoch 09697: val_mae did not improve from 6.28420\n",
      "Epoch 9698/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 124.4112 - mae: 8.7205 - val_loss: 249.7450 - val_mae: 8.6727\n",
      "\n",
      "Epoch 09698: val_mae did not improve from 6.28420\n",
      "Epoch 9699/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 94.8974 - mae: 7.6561 - val_loss: 270.2919 - val_mae: 9.4698\n",
      "\n",
      "Epoch 09699: val_mae did not improve from 6.28420\n",
      "Epoch 9700/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 86.9850 - mae: 7.4875 - val_loss: 274.5052 - val_mae: 9.8958\n",
      "\n",
      "Epoch 09700: val_mae did not improve from 6.28420\n",
      "Epoch 9701/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 81.6243 - mae: 7.1848 - val_loss: 255.3693 - val_mae: 8.8407\n",
      "\n",
      "Epoch 09701: val_mae did not improve from 6.28420\n",
      "Epoch 9702/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 81.8102 - mae: 7.1564 - val_loss: 251.1949 - val_mae: 8.7935\n",
      "\n",
      "Epoch 09702: val_mae did not improve from 6.28420\n",
      "Epoch 9703/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.0010 - mae: 6.8253 - val_loss: 228.1435 - val_mae: 7.3531\n",
      "\n",
      "Epoch 09703: val_mae did not improve from 6.28420\n",
      "Epoch 9704/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 74.0043 - mae: 6.9431 - val_loss: 225.7172 - val_mae: 7.5196\n",
      "\n",
      "Epoch 09704: val_mae did not improve from 6.28420\n",
      "Epoch 9705/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.3674 - mae: 6.9990 - val_loss: 246.1736 - val_mae: 8.9878\n",
      "\n",
      "Epoch 09705: val_mae did not improve from 6.28420\n",
      "Epoch 9706/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 87.2261 - mae: 7.4968 - val_loss: 311.0682 - val_mae: 10.7126\n",
      "\n",
      "Epoch 09706: val_mae did not improve from 6.28420\n",
      "Epoch 9707/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 121.1879 - mae: 8.8475 - val_loss: 246.2130 - val_mae: 8.6982\n",
      "\n",
      "Epoch 09707: val_mae did not improve from 6.28420\n",
      "Epoch 9708/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 96.6330 - mae: 7.8256 - val_loss: 259.3669 - val_mae: 8.8214\n",
      "\n",
      "Epoch 09708: val_mae did not improve from 6.28420\n",
      "Epoch 9709/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 102.6281 - mae: 8.0215 - val_loss: 274.9651 - val_mae: 9.8129\n",
      "\n",
      "Epoch 09709: val_mae did not improve from 6.28420\n",
      "Epoch 9710/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 83.9453 - mae: 7.3377 - val_loss: 238.4283 - val_mae: 7.9041\n",
      "\n",
      "Epoch 09710: val_mae did not improve from 6.28420\n",
      "Epoch 9711/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 89.8520 - mae: 7.4875 - val_loss: 243.2951 - val_mae: 8.5788\n",
      "\n",
      "Epoch 09711: val_mae did not improve from 6.28420\n",
      "Epoch 9712/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 64.6560 - mae: 6.0500 - val_loss: 249.2407 - val_mae: 9.0955\n",
      "\n",
      "Epoch 09712: val_mae did not improve from 6.28420\n",
      "Epoch 9713/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 73.0312 - mae: 6.6046 - val_loss: 229.9753 - val_mae: 7.6930\n",
      "\n",
      "Epoch 09713: val_mae did not improve from 6.28420\n",
      "Epoch 9714/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 87.7221 - mae: 7.5740 - val_loss: 338.8103 - val_mae: 12.7967\n",
      "\n",
      "Epoch 09714: val_mae did not improve from 6.28420\n",
      "Epoch 9715/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 133.9304 - mae: 9.4787 - val_loss: 253.3413 - val_mae: 8.8430\n",
      "\n",
      "Epoch 09715: val_mae did not improve from 6.28420\n",
      "Epoch 9716/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 61.9475 - mae: 6.2222 - val_loss: 251.2643 - val_mae: 9.0280\n",
      "\n",
      "Epoch 09716: val_mae did not improve from 6.28420\n",
      "Epoch 9717/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 92.9701 - mae: 7.8526 - val_loss: 293.0004 - val_mae: 10.6192\n",
      "\n",
      "Epoch 09717: val_mae did not improve from 6.28420\n",
      "Epoch 9718/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 146.0787 - mae: 9.7742 - val_loss: 235.4432 - val_mae: 8.2376\n",
      "\n",
      "Epoch 09718: val_mae did not improve from 6.28420\n",
      "Epoch 9719/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 79.6843 - mae: 6.8337 - val_loss: 245.0135 - val_mae: 9.0488\n",
      "\n",
      "Epoch 09719: val_mae did not improve from 6.28420\n",
      "Epoch 9720/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 66.7588 - mae: 6.5435 - val_loss: 234.8717 - val_mae: 8.2527\n",
      "\n",
      "Epoch 09720: val_mae did not improve from 6.28420\n",
      "Epoch 9721/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.1583 - mae: 6.7365 - val_loss: 227.3789 - val_mae: 8.0547\n",
      "\n",
      "Epoch 09721: val_mae did not improve from 6.28420\n",
      "Epoch 9722/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 83.9993 - mae: 7.3438 - val_loss: 228.1811 - val_mae: 7.6113\n",
      "\n",
      "Epoch 09722: val_mae did not improve from 6.28420\n",
      "Epoch 9723/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 61.1534 - mae: 6.1293 - val_loss: 257.5456 - val_mae: 9.0906\n",
      "\n",
      "Epoch 09723: val_mae did not improve from 6.28420\n",
      "Epoch 9724/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 58.5817 - mae: 6.1707 - val_loss: 250.4555 - val_mae: 8.7667\n",
      "\n",
      "Epoch 09724: val_mae did not improve from 6.28420\n",
      "Epoch 9725/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 86.6295 - mae: 7.4686 - val_loss: 274.3168 - val_mae: 9.5982\n",
      "\n",
      "Epoch 09725: val_mae did not improve from 6.28420\n",
      "Epoch 9726/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 94.6053 - mae: 7.8731 - val_loss: 243.8552 - val_mae: 8.2212\n",
      "\n",
      "Epoch 09726: val_mae did not improve from 6.28420\n",
      "Epoch 9727/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 79.9154 - mae: 7.0843 - val_loss: 263.5103 - val_mae: 9.0681\n",
      "\n",
      "Epoch 09727: val_mae did not improve from 6.28420\n",
      "Epoch 9728/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 103.0762 - mae: 8.3441 - val_loss: 293.9095 - val_mae: 10.4702\n",
      "\n",
      "Epoch 09728: val_mae did not improve from 6.28420\n",
      "Epoch 9729/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 92.7757 - mae: 7.7894 - val_loss: 273.2285 - val_mae: 9.3618\n",
      "\n",
      "Epoch 09729: val_mae did not improve from 6.28420\n",
      "Epoch 9730/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 77.0142 - mae: 7.1148 - val_loss: 249.4394 - val_mae: 8.5406\n",
      "\n",
      "Epoch 09730: val_mae did not improve from 6.28420\n",
      "Epoch 9731/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 74.7460 - mae: 6.8753 - val_loss: 249.7767 - val_mae: 8.4920\n",
      "\n",
      "Epoch 09731: val_mae did not improve from 6.28420\n",
      "Epoch 9732/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.4169 - mae: 7.8495 - val_loss: 231.5323 - val_mae: 8.1007\n",
      "\n",
      "Epoch 09732: val_mae did not improve from 6.28420\n",
      "Epoch 9733/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.8867 - mae: 6.6702 - val_loss: 227.0626 - val_mae: 7.8341\n",
      "\n",
      "Epoch 09733: val_mae did not improve from 6.28420\n",
      "Epoch 9734/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.0079 - mae: 6.8014 - val_loss: 232.5150 - val_mae: 8.0109\n",
      "\n",
      "Epoch 09734: val_mae did not improve from 6.28420\n",
      "Epoch 9735/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 86.8724 - mae: 7.3859 - val_loss: 289.1382 - val_mae: 10.9615\n",
      "\n",
      "Epoch 09735: val_mae did not improve from 6.28420\n",
      "Epoch 9736/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 91.9831 - mae: 7.6688 - val_loss: 242.7197 - val_mae: 8.3819\n",
      "\n",
      "Epoch 09736: val_mae did not improve from 6.28420\n",
      "Epoch 9737/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 91.6442 - mae: 7.6727 - val_loss: 230.0911 - val_mae: 7.5549\n",
      "\n",
      "Epoch 09737: val_mae did not improve from 6.28420\n",
      "Epoch 9738/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 91.7977 - mae: 7.5827 - val_loss: 246.2903 - val_mae: 8.7670\n",
      "\n",
      "Epoch 09738: val_mae did not improve from 6.28420\n",
      "Epoch 9739/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 112.0404 - mae: 8.3274 - val_loss: 235.3057 - val_mae: 8.0440\n",
      "\n",
      "Epoch 09739: val_mae did not improve from 6.28420\n",
      "Epoch 9740/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 88.7214 - mae: 7.4944 - val_loss: 238.7508 - val_mae: 8.5522\n",
      "\n",
      "Epoch 09740: val_mae did not improve from 6.28420\n",
      "Epoch 9741/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 71.5800 - mae: 6.7552 - val_loss: 226.9608 - val_mae: 7.7836\n",
      "\n",
      "Epoch 09741: val_mae did not improve from 6.28420\n",
      "Epoch 9742/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 97.0981 - mae: 7.6167 - val_loss: 248.0159 - val_mae: 8.4875\n",
      "\n",
      "Epoch 09742: val_mae did not improve from 6.28420\n",
      "Epoch 9743/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.6863 - mae: 6.5278 - val_loss: 246.6559 - val_mae: 8.4180\n",
      "\n",
      "Epoch 09743: val_mae did not improve from 6.28420\n",
      "Epoch 9744/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 62.0747 - mae: 6.2330 - val_loss: 233.9333 - val_mae: 8.5067\n",
      "\n",
      "Epoch 09744: val_mae did not improve from 6.28420\n",
      "Epoch 9745/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 53.0506 - mae: 5.7058 - val_loss: 220.3036 - val_mae: 7.7266\n",
      "\n",
      "Epoch 09745: val_mae did not improve from 6.28420\n",
      "Epoch 9746/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 68.5570 - mae: 6.4759 - val_loss: 215.6046 - val_mae: 7.2105\n",
      "\n",
      "Epoch 09746: val_mae did not improve from 6.28420\n",
      "Epoch 9747/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.3130 - mae: 7.3834 - val_loss: 221.9388 - val_mae: 7.6537\n",
      "\n",
      "Epoch 09747: val_mae did not improve from 6.28420\n",
      "Epoch 9748/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 79.4639 - mae: 7.0604 - val_loss: 243.6266 - val_mae: 8.5199\n",
      "\n",
      "Epoch 09748: val_mae did not improve from 6.28420\n",
      "Epoch 9749/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 97.3259 - mae: 7.8378 - val_loss: 248.5715 - val_mae: 8.6884\n",
      "\n",
      "Epoch 09749: val_mae did not improve from 6.28420\n",
      "Epoch 9750/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 71.8584 - mae: 6.7051 - val_loss: 333.9773 - val_mae: 12.0822\n",
      "\n",
      "Epoch 09750: val_mae did not improve from 6.28420\n",
      "Epoch 9751/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.4071 - mae: 7.2737 - val_loss: 275.3235 - val_mae: 9.5859\n",
      "\n",
      "Epoch 09751: val_mae did not improve from 6.28420\n",
      "Epoch 9752/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 72.4203 - mae: 6.6344 - val_loss: 237.3764 - val_mae: 7.9404\n",
      "\n",
      "Epoch 09752: val_mae did not improve from 6.28420\n",
      "Epoch 9753/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 73.6304 - mae: 6.7043 - val_loss: 316.4099 - val_mae: 11.6727\n",
      "\n",
      "Epoch 09753: val_mae did not improve from 6.28420\n",
      "Epoch 9754/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 78.3582 - mae: 6.8753 - val_loss: 260.2020 - val_mae: 9.0187\n",
      "\n",
      "Epoch 09754: val_mae did not improve from 6.28420\n",
      "Epoch 9755/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 86.0477 - mae: 7.3781 - val_loss: 224.6386 - val_mae: 7.5266\n",
      "\n",
      "Epoch 09755: val_mae did not improve from 6.28420\n",
      "Epoch 9756/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 76.3860 - mae: 7.0610 - val_loss: 295.8188 - val_mae: 10.7868\n",
      "\n",
      "Epoch 09756: val_mae did not improve from 6.28420\n",
      "Epoch 9757/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 96.3650 - mae: 7.6400 - val_loss: 278.5135 - val_mae: 9.6641\n",
      "\n",
      "Epoch 09757: val_mae did not improve from 6.28420\n",
      "Epoch 9758/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 99.3636 - mae: 8.0284 - val_loss: 243.8570 - val_mae: 8.9737\n",
      "\n",
      "Epoch 09758: val_mae did not improve from 6.28420\n",
      "Epoch 9759/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 85.7538 - mae: 7.4849 - val_loss: 229.0938 - val_mae: 8.0753\n",
      "\n",
      "Epoch 09759: val_mae did not improve from 6.28420\n",
      "Epoch 9760/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 115.7307 - mae: 9.0349 - val_loss: 244.5434 - val_mae: 8.9776\n",
      "\n",
      "Epoch 09760: val_mae did not improve from 6.28420\n",
      "Epoch 9761/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 79.1258 - mae: 7.1232 - val_loss: 251.8947 - val_mae: 9.1102\n",
      "\n",
      "Epoch 09761: val_mae did not improve from 6.28420\n",
      "Epoch 9762/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 104.7180 - mae: 8.2236 - val_loss: 283.3383 - val_mae: 10.1919\n",
      "\n",
      "Epoch 09762: val_mae did not improve from 6.28420\n",
      "Epoch 9763/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 108.4374 - mae: 8.2722 - val_loss: 250.6863 - val_mae: 9.1584\n",
      "\n",
      "Epoch 09763: val_mae did not improve from 6.28420\n",
      "Epoch 9764/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 87.0904 - mae: 7.4897 - val_loss: 223.9966 - val_mae: 8.0519\n",
      "\n",
      "Epoch 09764: val_mae did not improve from 6.28420\n",
      "Epoch 9765/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 76.0308 - mae: 6.8926 - val_loss: 243.0036 - val_mae: 9.0192\n",
      "\n",
      "Epoch 09765: val_mae did not improve from 6.28420\n",
      "Epoch 9766/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 88.5929 - mae: 7.5726 - val_loss: 234.3727 - val_mae: 8.1482\n",
      "\n",
      "Epoch 09766: val_mae did not improve from 6.28420\n",
      "Epoch 9767/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 81.5650 - mae: 7.2158 - val_loss: 210.0140 - val_mae: 7.0654\n",
      "\n",
      "Epoch 09767: val_mae did not improve from 6.28420\n",
      "Epoch 9768/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 73.1821 - mae: 6.9660 - val_loss: 250.7547 - val_mae: 9.0147\n",
      "\n",
      "Epoch 09768: val_mae did not improve from 6.28420\n",
      "Epoch 9769/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 73.3298 - mae: 6.7070 - val_loss: 222.5343 - val_mae: 7.4098\n",
      "\n",
      "Epoch 09769: val_mae did not improve from 6.28420\n",
      "Epoch 9770/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 61.3308 - mae: 6.2813 - val_loss: 233.9815 - val_mae: 8.5461\n",
      "\n",
      "Epoch 09770: val_mae did not improve from 6.28420\n",
      "Epoch 9771/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 85.4924 - mae: 7.3091 - val_loss: 250.4551 - val_mae: 9.4875\n",
      "\n",
      "Epoch 09771: val_mae did not improve from 6.28420\n",
      "Epoch 9772/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 94.1711 - mae: 7.5682 - val_loss: 258.1037 - val_mae: 9.3456\n",
      "\n",
      "Epoch 09772: val_mae did not improve from 6.28420\n",
      "Epoch 9773/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 82.7958 - mae: 7.2053 - val_loss: 233.4870 - val_mae: 8.4050\n",
      "\n",
      "Epoch 09773: val_mae did not improve from 6.28420\n",
      "Epoch 9774/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 100.5511 - mae: 8.0517 - val_loss: 221.0341 - val_mae: 7.7939\n",
      "\n",
      "Epoch 09774: val_mae did not improve from 6.28420\n",
      "Epoch 9775/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 62.6053 - mae: 6.3621 - val_loss: 228.2892 - val_mae: 8.2016\n",
      "\n",
      "Epoch 09775: val_mae did not improve from 6.28420\n",
      "Epoch 9776/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 96.3487 - mae: 7.8537 - val_loss: 243.7150 - val_mae: 8.8392\n",
      "\n",
      "Epoch 09776: val_mae did not improve from 6.28420\n",
      "Epoch 9777/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 62.4745 - mae: 6.3165 - val_loss: 224.4997 - val_mae: 7.4128\n",
      "\n",
      "Epoch 09777: val_mae did not improve from 6.28420\n",
      "Epoch 9778/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 65.6699 - mae: 6.4267 - val_loss: 241.7246 - val_mae: 8.7950\n",
      "\n",
      "Epoch 09778: val_mae did not improve from 6.28420\n",
      "Epoch 9779/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 77.7036 - mae: 6.9660 - val_loss: 252.0307 - val_mae: 9.2725\n",
      "\n",
      "Epoch 09779: val_mae did not improve from 6.28420\n",
      "Epoch 9780/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 91.1062 - mae: 7.6098 - val_loss: 249.0003 - val_mae: 9.0061\n",
      "\n",
      "Epoch 09780: val_mae did not improve from 6.28420\n",
      "Epoch 9781/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 68.4367 - mae: 6.4978 - val_loss: 234.0732 - val_mae: 8.0111\n",
      "\n",
      "Epoch 09781: val_mae did not improve from 6.28420\n",
      "Epoch 9782/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 63.9821 - mae: 6.4531 - val_loss: 223.3354 - val_mae: 7.7855\n",
      "\n",
      "Epoch 09782: val_mae did not improve from 6.28420\n",
      "Epoch 9783/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 113.9381 - mae: 8.2523 - val_loss: 219.3384 - val_mae: 7.2369\n",
      "\n",
      "Epoch 09783: val_mae did not improve from 6.28420\n",
      "Epoch 9784/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 62.4125 - mae: 6.2294 - val_loss: 252.5498 - val_mae: 9.2615\n",
      "\n",
      "Epoch 09784: val_mae did not improve from 6.28420\n",
      "Epoch 9785/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 72.1008 - mae: 6.7437 - val_loss: 252.3071 - val_mae: 8.9617\n",
      "\n",
      "Epoch 09785: val_mae did not improve from 6.28420\n",
      "Epoch 9786/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 160.8550 - mae: 9.6896 - val_loss: 291.3353 - val_mae: 10.4356\n",
      "\n",
      "Epoch 09786: val_mae did not improve from 6.28420\n",
      "Epoch 9787/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 94.5076 - mae: 7.9404 - val_loss: 236.2908 - val_mae: 7.7868\n",
      "\n",
      "Epoch 09787: val_mae did not improve from 6.28420\n",
      "Epoch 9788/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.0864 - mae: 6.6270 - val_loss: 228.7225 - val_mae: 7.6721\n",
      "\n",
      "Epoch 09788: val_mae did not improve from 6.28420\n",
      "Epoch 9789/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 76.6295 - mae: 6.9141 - val_loss: 231.3827 - val_mae: 7.3582\n",
      "\n",
      "Epoch 09789: val_mae did not improve from 6.28420\n",
      "Epoch 9790/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 61.1975 - mae: 6.2714 - val_loss: 250.6396 - val_mae: 8.3998\n",
      "\n",
      "Epoch 09790: val_mae did not improve from 6.28420\n",
      "Epoch 9791/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 66.9211 - mae: 6.5119 - val_loss: 239.1582 - val_mae: 7.9966\n",
      "\n",
      "Epoch 09791: val_mae did not improve from 6.28420\n",
      "Epoch 9792/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 77.2309 - mae: 7.0449 - val_loss: 258.2076 - val_mae: 9.0321\n",
      "\n",
      "Epoch 09792: val_mae did not improve from 6.28420\n",
      "Epoch 9793/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 79.8132 - mae: 7.1632 - val_loss: 239.5324 - val_mae: 7.9948\n",
      "\n",
      "Epoch 09793: val_mae did not improve from 6.28420\n",
      "Epoch 9794/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 76.2093 - mae: 6.9257 - val_loss: 229.9743 - val_mae: 7.1231\n",
      "\n",
      "Epoch 09794: val_mae did not improve from 6.28420\n",
      "Epoch 9795/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 64.1051 - mae: 6.4169 - val_loss: 240.4509 - val_mae: 8.0356\n",
      "\n",
      "Epoch 09795: val_mae did not improve from 6.28420\n",
      "Epoch 9796/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 83.9083 - mae: 7.0938 - val_loss: 269.7310 - val_mae: 9.3605\n",
      "\n",
      "Epoch 09796: val_mae did not improve from 6.28420\n",
      "Epoch 9797/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 86.5890 - mae: 7.4606 - val_loss: 255.4630 - val_mae: 8.6794\n",
      "\n",
      "Epoch 09797: val_mae did not improve from 6.28420\n",
      "Epoch 9798/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 82.4057 - mae: 7.1411 - val_loss: 257.3380 - val_mae: 9.1554\n",
      "\n",
      "Epoch 09798: val_mae did not improve from 6.28420\n",
      "Epoch 9799/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 100.5358 - mae: 7.9016 - val_loss: 232.8254 - val_mae: 7.5037\n",
      "\n",
      "Epoch 09799: val_mae did not improve from 6.28420\n",
      "Epoch 9800/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 61.9873 - mae: 6.2097 - val_loss: 237.4339 - val_mae: 7.8165\n",
      "\n",
      "Epoch 09800: val_mae did not improve from 6.28420\n",
      "Epoch 9801/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 88.4849 - mae: 7.3463 - val_loss: 258.5716 - val_mae: 8.5329\n",
      "\n",
      "Epoch 09801: val_mae did not improve from 6.28420\n",
      "Epoch 9802/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.2946 - mae: 6.7885 - val_loss: 224.8766 - val_mae: 7.5468\n",
      "\n",
      "Epoch 09802: val_mae did not improve from 6.28420\n",
      "Epoch 9803/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 79.0368 - mae: 6.9891 - val_loss: 237.8636 - val_mae: 8.1802\n",
      "\n",
      "Epoch 09803: val_mae did not improve from 6.28420\n",
      "Epoch 9804/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 96.6079 - mae: 7.8872 - val_loss: 228.4310 - val_mae: 8.0579\n",
      "\n",
      "Epoch 09804: val_mae did not improve from 6.28420\n",
      "Epoch 9805/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 59.5458 - mae: 6.0508 - val_loss: 230.4881 - val_mae: 8.0397\n",
      "\n",
      "Epoch 09805: val_mae did not improve from 6.28420\n",
      "Epoch 9806/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 96.0157 - mae: 7.6935 - val_loss: 287.6518 - val_mae: 10.6140\n",
      "\n",
      "Epoch 09806: val_mae did not improve from 6.28420\n",
      "Epoch 9807/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 106.6431 - mae: 8.1280 - val_loss: 247.6966 - val_mae: 8.5436\n",
      "\n",
      "Epoch 09807: val_mae did not improve from 6.28420\n",
      "Epoch 9808/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 93.8602 - mae: 7.6396 - val_loss: 239.2803 - val_mae: 7.9858\n",
      "\n",
      "Epoch 09808: val_mae did not improve from 6.28420\n",
      "Epoch 9809/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 75.0168 - mae: 6.8514 - val_loss: 245.5056 - val_mae: 8.2275\n",
      "\n",
      "Epoch 09809: val_mae did not improve from 6.28420\n",
      "Epoch 9810/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 95.8170 - mae: 7.5967 - val_loss: 245.8810 - val_mae: 8.6312\n",
      "\n",
      "Epoch 09810: val_mae did not improve from 6.28420\n",
      "Epoch 9811/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 100.3466 - mae: 7.9001 - val_loss: 247.6618 - val_mae: 8.9529\n",
      "\n",
      "Epoch 09811: val_mae did not improve from 6.28420\n",
      "Epoch 9812/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 98.3011 - mae: 8.1093 - val_loss: 245.6278 - val_mae: 8.9896\n",
      "\n",
      "Epoch 09812: val_mae did not improve from 6.28420\n",
      "Epoch 9813/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 97.8391 - mae: 7.8225 - val_loss: 224.7692 - val_mae: 7.6850\n",
      "\n",
      "Epoch 09813: val_mae did not improve from 6.28420\n",
      "Epoch 9814/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 68.4140 - mae: 6.5652 - val_loss: 234.8615 - val_mae: 7.8657\n",
      "\n",
      "Epoch 09814: val_mae did not improve from 6.28420\n",
      "Epoch 9815/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.0832 - mae: 6.5187 - val_loss: 285.0464 - val_mae: 10.6958\n",
      "\n",
      "Epoch 09815: val_mae did not improve from 6.28420\n",
      "Epoch 9816/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 74.1996 - mae: 6.8276 - val_loss: 305.3775 - val_mae: 11.3929\n",
      "\n",
      "Epoch 09816: val_mae did not improve from 6.28420\n",
      "Epoch 9817/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 111.5312 - mae: 8.3133 - val_loss: 291.5885 - val_mae: 10.5735\n",
      "\n",
      "Epoch 09817: val_mae did not improve from 6.28420\n",
      "Epoch 9818/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 112.7085 - mae: 8.8645 - val_loss: 238.2287 - val_mae: 8.0190\n",
      "\n",
      "Epoch 09818: val_mae did not improve from 6.28420\n",
      "Epoch 9819/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 85.4972 - mae: 7.4810 - val_loss: 257.1157 - val_mae: 9.6110\n",
      "\n",
      "Epoch 09819: val_mae did not improve from 6.28420\n",
      "Epoch 9820/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.5523 - mae: 7.0229 - val_loss: 235.6495 - val_mae: 8.2288\n",
      "\n",
      "Epoch 09820: val_mae did not improve from 6.28420\n",
      "Epoch 9821/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 53.6050 - mae: 5.8017 - val_loss: 253.7537 - val_mae: 9.4927\n",
      "\n",
      "Epoch 09821: val_mae did not improve from 6.28420\n",
      "Epoch 9822/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 78.6884 - mae: 6.9973 - val_loss: 231.6313 - val_mae: 8.7508\n",
      "\n",
      "Epoch 09822: val_mae did not improve from 6.28420\n",
      "Epoch 9823/10000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 66.8051 - mae: 6.5071 - val_loss: 236.7241 - val_mae: 8.8927\n",
      "\n",
      "Epoch 09823: val_mae did not improve from 6.28420\n",
      "Epoch 9824/10000\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 73.4811 - mae: 6.8771 - val_loss: 269.2287 - val_mae: 9.6995\n",
      "\n",
      "Epoch 09824: val_mae did not improve from 6.28420\n",
      "Epoch 9825/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 85.8287 - mae: 7.4556 - val_loss: 286.4915 - val_mae: 10.5197\n",
      "\n",
      "Epoch 09825: val_mae did not improve from 6.28420\n",
      "Epoch 9826/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 96.6676 - mae: 7.6047 - val_loss: 251.3654 - val_mae: 9.1681\n",
      "\n",
      "Epoch 09826: val_mae did not improve from 6.28420\n",
      "Epoch 9827/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 67.2612 - mae: 6.5574 - val_loss: 230.3042 - val_mae: 7.8455\n",
      "\n",
      "Epoch 09827: val_mae did not improve from 6.28420\n",
      "Epoch 9828/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 89.3772 - mae: 7.6841 - val_loss: 236.9813 - val_mae: 8.1710\n",
      "\n",
      "Epoch 09828: val_mae did not improve from 6.28420\n",
      "Epoch 9829/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 86.8444 - mae: 7.2847 - val_loss: 227.5841 - val_mae: 7.9424\n",
      "\n",
      "Epoch 09829: val_mae did not improve from 6.28420\n",
      "Epoch 9830/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.8675 - mae: 6.7031 - val_loss: 239.9508 - val_mae: 8.5764\n",
      "\n",
      "Epoch 09830: val_mae did not improve from 6.28420\n",
      "Epoch 9831/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 80.7740 - mae: 7.1207 - val_loss: 218.6944 - val_mae: 7.8718\n",
      "\n",
      "Epoch 09831: val_mae did not improve from 6.28420\n",
      "Epoch 9832/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 68.0000 - mae: 6.6325 - val_loss: 221.4612 - val_mae: 7.4929\n",
      "\n",
      "Epoch 09832: val_mae did not improve from 6.28420\n",
      "Epoch 9833/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 87.5392 - mae: 7.3266 - val_loss: 232.6368 - val_mae: 8.5057\n",
      "\n",
      "Epoch 09833: val_mae did not improve from 6.28420\n",
      "Epoch 9834/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 66.1046 - mae: 6.4707 - val_loss: 226.9660 - val_mae: 8.7954\n",
      "\n",
      "Epoch 09834: val_mae did not improve from 6.28420\n",
      "Epoch 9835/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.7560 - mae: 6.5029 - val_loss: 246.7162 - val_mae: 9.0584\n",
      "\n",
      "Epoch 09835: val_mae did not improve from 6.28420\n",
      "Epoch 9836/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 88.9456 - mae: 7.7185 - val_loss: 222.2750 - val_mae: 7.6977\n",
      "\n",
      "Epoch 09836: val_mae did not improve from 6.28420\n",
      "Epoch 9837/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 74.1411 - mae: 6.9354 - val_loss: 214.0592 - val_mae: 7.5227\n",
      "\n",
      "Epoch 09837: val_mae did not improve from 6.28420\n",
      "Epoch 9838/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 69.0023 - mae: 6.5261 - val_loss: 296.2584 - val_mae: 11.3756\n",
      "\n",
      "Epoch 09838: val_mae did not improve from 6.28420\n",
      "Epoch 9839/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 124.3554 - mae: 8.9124 - val_loss: 264.2517 - val_mae: 10.1730\n",
      "\n",
      "Epoch 09839: val_mae did not improve from 6.28420\n",
      "Epoch 9840/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 114.2605 - mae: 8.5118 - val_loss: 247.9591 - val_mae: 9.5250\n",
      "\n",
      "Epoch 09840: val_mae did not improve from 6.28420\n",
      "Epoch 9841/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 74.6072 - mae: 6.8315 - val_loss: 234.1486 - val_mae: 8.6575\n",
      "\n",
      "Epoch 09841: val_mae did not improve from 6.28420\n",
      "Epoch 9842/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 110.5763 - mae: 8.5777 - val_loss: 222.5378 - val_mae: 8.0965\n",
      "\n",
      "Epoch 09842: val_mae did not improve from 6.28420\n",
      "Epoch 9843/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 85.0857 - mae: 7.3524 - val_loss: 248.7008 - val_mae: 9.1765\n",
      "\n",
      "Epoch 09843: val_mae did not improve from 6.28420\n",
      "Epoch 9844/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 90.6836 - mae: 7.7238 - val_loss: 266.8236 - val_mae: 9.8744\n",
      "\n",
      "Epoch 09844: val_mae did not improve from 6.28420\n",
      "Epoch 9845/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 71.5801 - mae: 6.7946 - val_loss: 229.6922 - val_mae: 8.3240\n",
      "\n",
      "Epoch 09845: val_mae did not improve from 6.28420\n",
      "Epoch 9846/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 96.8955 - mae: 7.8751 - val_loss: 212.7626 - val_mae: 7.4070\n",
      "\n",
      "Epoch 09846: val_mae did not improve from 6.28420\n",
      "Epoch 9847/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.3008 - mae: 6.7230 - val_loss: 210.6413 - val_mae: 7.2863\n",
      "\n",
      "Epoch 09847: val_mae did not improve from 6.28420\n",
      "Epoch 9848/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 60.2938 - mae: 6.1784 - val_loss: 247.4052 - val_mae: 9.3954\n",
      "\n",
      "Epoch 09848: val_mae did not improve from 6.28420\n",
      "Epoch 9849/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 95.8099 - mae: 7.7621 - val_loss: 286.3377 - val_mae: 10.8671\n",
      "\n",
      "Epoch 09849: val_mae did not improve from 6.28420\n",
      "Epoch 9850/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 102.9644 - mae: 7.8694 - val_loss: 208.1091 - val_mae: 7.5893\n",
      "\n",
      "Epoch 09850: val_mae did not improve from 6.28420\n",
      "Epoch 9851/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 61.2265 - mae: 6.0808 - val_loss: 215.2350 - val_mae: 7.7900\n",
      "\n",
      "Epoch 09851: val_mae did not improve from 6.28420\n",
      "Epoch 9852/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 79.3916 - mae: 7.0283 - val_loss: 258.2003 - val_mae: 9.6925\n",
      "\n",
      "Epoch 09852: val_mae did not improve from 6.28420\n",
      "Epoch 9853/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 99.0036 - mae: 7.7971 - val_loss: 210.6289 - val_mae: 8.0651\n",
      "\n",
      "Epoch 09853: val_mae did not improve from 6.28420\n",
      "Epoch 9854/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.9563 - mae: 6.9839 - val_loss: 223.9663 - val_mae: 8.5702\n",
      "\n",
      "Epoch 09854: val_mae did not improve from 6.28420\n",
      "Epoch 9855/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 79.8401 - mae: 7.1506 - val_loss: 211.2080 - val_mae: 7.9867\n",
      "\n",
      "Epoch 09855: val_mae did not improve from 6.28420\n",
      "Epoch 9856/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 68.9343 - mae: 6.5340 - val_loss: 216.0667 - val_mae: 7.6662\n",
      "\n",
      "Epoch 09856: val_mae did not improve from 6.28420\n",
      "Epoch 9857/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.6372 - mae: 6.7260 - val_loss: 229.4228 - val_mae: 8.5171\n",
      "\n",
      "Epoch 09857: val_mae did not improve from 6.28420\n",
      "Epoch 9858/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.7582 - mae: 6.9464 - val_loss: 229.6161 - val_mae: 8.2739\n",
      "\n",
      "Epoch 09858: val_mae did not improve from 6.28420\n",
      "Epoch 9859/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 55.0443 - mae: 5.7670 - val_loss: 211.6577 - val_mae: 7.7672\n",
      "\n",
      "Epoch 09859: val_mae did not improve from 6.28420\n",
      "Epoch 9860/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 58.3164 - mae: 6.0093 - val_loss: 217.5482 - val_mae: 8.0571\n",
      "\n",
      "Epoch 09860: val_mae did not improve from 6.28420\n",
      "Epoch 9861/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 94.1909 - mae: 7.6166 - val_loss: 274.4296 - val_mae: 10.1421\n",
      "\n",
      "Epoch 09861: val_mae did not improve from 6.28420\n",
      "Epoch 9862/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 94.2367 - mae: 7.7528 - val_loss: 247.6047 - val_mae: 9.4092\n",
      "\n",
      "Epoch 09862: val_mae did not improve from 6.28420\n",
      "Epoch 9863/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 118.0698 - mae: 8.8362 - val_loss: 270.5489 - val_mae: 10.4958\n",
      "\n",
      "Epoch 09863: val_mae did not improve from 6.28420\n",
      "Epoch 9864/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.4798 - mae: 7.0479 - val_loss: 209.1543 - val_mae: 7.3654\n",
      "\n",
      "Epoch 09864: val_mae did not improve from 6.28420\n",
      "Epoch 9865/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 56.9845 - mae: 6.0006 - val_loss: 245.6639 - val_mae: 9.3047\n",
      "\n",
      "Epoch 09865: val_mae did not improve from 6.28420\n",
      "Epoch 9866/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 70.5742 - mae: 6.6122 - val_loss: 214.6288 - val_mae: 7.3860\n",
      "\n",
      "Epoch 09866: val_mae did not improve from 6.28420\n",
      "Epoch 9867/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.3484 - mae: 7.0970 - val_loss: 226.9236 - val_mae: 7.9542\n",
      "\n",
      "Epoch 09867: val_mae did not improve from 6.28420\n",
      "Epoch 9868/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 82.1997 - mae: 7.2999 - val_loss: 228.4645 - val_mae: 8.2956\n",
      "\n",
      "Epoch 09868: val_mae did not improve from 6.28420\n",
      "Epoch 9869/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 87.4611 - mae: 7.3835 - val_loss: 217.6548 - val_mae: 8.3034\n",
      "\n",
      "Epoch 09869: val_mae did not improve from 6.28420\n",
      "Epoch 9870/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 94.7584 - mae: 7.8667 - val_loss: 239.9164 - val_mae: 9.3139\n",
      "\n",
      "Epoch 09870: val_mae did not improve from 6.28420\n",
      "Epoch 9871/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 75.9046 - mae: 6.8348 - val_loss: 216.3847 - val_mae: 7.9780\n",
      "\n",
      "Epoch 09871: val_mae did not improve from 6.28420\n",
      "Epoch 9872/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.6586 - mae: 6.8446 - val_loss: 221.0880 - val_mae: 8.4140\n",
      "\n",
      "Epoch 09872: val_mae did not improve from 6.28420\n",
      "Epoch 9873/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 69.9808 - mae: 6.7725 - val_loss: 224.8859 - val_mae: 8.6395\n",
      "\n",
      "Epoch 09873: val_mae did not improve from 6.28420\n",
      "Epoch 9874/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.6411 - mae: 6.7196 - val_loss: 218.9374 - val_mae: 8.2283\n",
      "\n",
      "Epoch 09874: val_mae did not improve from 6.28420\n",
      "Epoch 9875/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.9137 - mae: 6.8238 - val_loss: 214.1535 - val_mae: 8.0746\n",
      "\n",
      "Epoch 09875: val_mae did not improve from 6.28420\n",
      "Epoch 9876/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 55.3399 - mae: 6.0213 - val_loss: 218.4243 - val_mae: 8.2091\n",
      "\n",
      "Epoch 09876: val_mae did not improve from 6.28420\n",
      "Epoch 9877/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 58.9895 - mae: 6.0518 - val_loss: 230.4428 - val_mae: 8.8890\n",
      "\n",
      "Epoch 09877: val_mae did not improve from 6.28420\n",
      "Epoch 9878/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 79.7795 - mae: 7.2043 - val_loss: 205.0861 - val_mae: 7.5637\n",
      "\n",
      "Epoch 09878: val_mae did not improve from 6.28420\n",
      "Epoch 9879/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 62.1051 - mae: 6.3249 - val_loss: 238.9224 - val_mae: 9.5738\n",
      "\n",
      "Epoch 09879: val_mae did not improve from 6.28420\n",
      "Epoch 9880/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 76.0495 - mae: 6.9257 - val_loss: 196.9498 - val_mae: 7.1394\n",
      "\n",
      "Epoch 09880: val_mae did not improve from 6.28420\n",
      "Epoch 9881/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 64.0160 - mae: 6.3271 - val_loss: 220.0491 - val_mae: 8.7287\n",
      "\n",
      "Epoch 09881: val_mae did not improve from 6.28420\n",
      "Epoch 9882/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 71.4866 - mae: 6.7878 - val_loss: 227.6191 - val_mae: 8.4470\n",
      "\n",
      "Epoch 09882: val_mae did not improve from 6.28420\n",
      "Epoch 9883/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 55.5365 - mae: 5.8619 - val_loss: 228.0313 - val_mae: 8.2000\n",
      "\n",
      "Epoch 09883: val_mae did not improve from 6.28420\n",
      "Epoch 9884/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 66.1812 - mae: 6.3829 - val_loss: 240.0180 - val_mae: 9.0133\n",
      "\n",
      "Epoch 09884: val_mae did not improve from 6.28420\n",
      "Epoch 9885/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 97.2837 - mae: 7.7005 - val_loss: 211.0611 - val_mae: 7.3688\n",
      "\n",
      "Epoch 09885: val_mae did not improve from 6.28420\n",
      "Epoch 9886/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 87.5091 - mae: 7.4722 - val_loss: 242.5994 - val_mae: 9.3530\n",
      "\n",
      "Epoch 09886: val_mae did not improve from 6.28420\n",
      "Epoch 9887/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 72.4821 - mae: 6.8800 - val_loss: 202.5191 - val_mae: 7.4012\n",
      "\n",
      "Epoch 09887: val_mae did not improve from 6.28420\n",
      "Epoch 9888/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 65.3113 - mae: 6.3291 - val_loss: 198.6984 - val_mae: 7.2135\n",
      "\n",
      "Epoch 09888: val_mae did not improve from 6.28420\n",
      "Epoch 9889/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 49.5908 - mae: 5.5957 - val_loss: 230.4610 - val_mae: 8.5274\n",
      "\n",
      "Epoch 09889: val_mae did not improve from 6.28420\n",
      "Epoch 9890/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 98.0074 - mae: 7.8421 - val_loss: 250.1908 - val_mae: 9.8666\n",
      "\n",
      "Epoch 09890: val_mae did not improve from 6.28420\n",
      "Epoch 9891/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 111.8670 - mae: 8.2035 - val_loss: 255.6212 - val_mae: 10.1387\n",
      "\n",
      "Epoch 09891: val_mae did not improve from 6.28420\n",
      "Epoch 9892/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 106.0989 - mae: 8.0792 - val_loss: 226.9930 - val_mae: 9.0277\n",
      "\n",
      "Epoch 09892: val_mae did not improve from 6.28420\n",
      "Epoch 9893/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 67.9483 - mae: 6.5595 - val_loss: 201.9044 - val_mae: 7.6541\n",
      "\n",
      "Epoch 09893: val_mae did not improve from 6.28420\n",
      "Epoch 9894/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 55.6016 - mae: 5.9509 - val_loss: 215.7452 - val_mae: 7.9636\n",
      "\n",
      "Epoch 09894: val_mae did not improve from 6.28420\n",
      "Epoch 9895/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 77.1021 - mae: 6.9655 - val_loss: 264.3886 - val_mae: 10.6902\n",
      "\n",
      "Epoch 09895: val_mae did not improve from 6.28420\n",
      "Epoch 9896/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 122.8053 - mae: 8.8999 - val_loss: 213.1942 - val_mae: 7.3926\n",
      "\n",
      "Epoch 09896: val_mae did not improve from 6.28420\n",
      "Epoch 9897/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.7139 - mae: 6.6510 - val_loss: 212.7377 - val_mae: 7.7729\n",
      "\n",
      "Epoch 09897: val_mae did not improve from 6.28420\n",
      "Epoch 9898/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.7769 - mae: 6.6942 - val_loss: 277.4810 - val_mae: 10.5293\n",
      "\n",
      "Epoch 09898: val_mae did not improve from 6.28420\n",
      "Epoch 9899/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 63.7697 - mae: 6.4872 - val_loss: 254.3800 - val_mae: 9.4390\n",
      "\n",
      "Epoch 09899: val_mae did not improve from 6.28420\n",
      "Epoch 9900/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 85.7645 - mae: 7.4221 - val_loss: 216.1526 - val_mae: 7.7803\n",
      "\n",
      "Epoch 09900: val_mae did not improve from 6.28420\n",
      "Epoch 9901/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 51.2307 - mae: 5.7069 - val_loss: 202.0964 - val_mae: 7.1498\n",
      "\n",
      "Epoch 09901: val_mae did not improve from 6.28420\n",
      "Epoch 9902/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.5747 - mae: 6.9930 - val_loss: 204.7987 - val_mae: 7.3481\n",
      "\n",
      "Epoch 09902: val_mae did not improve from 6.28420\n",
      "Epoch 9903/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 89.1192 - mae: 7.6588 - val_loss: 201.7404 - val_mae: 7.4667\n",
      "\n",
      "Epoch 09903: val_mae did not improve from 6.28420\n",
      "Epoch 9904/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.9161 - mae: 6.8002 - val_loss: 210.8575 - val_mae: 7.8893\n",
      "\n",
      "Epoch 09904: val_mae did not improve from 6.28420\n",
      "Epoch 9905/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 70.3189 - mae: 6.6669 - val_loss: 200.5747 - val_mae: 7.5062\n",
      "\n",
      "Epoch 09905: val_mae did not improve from 6.28420\n",
      "Epoch 9906/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 61.8293 - mae: 6.2208 - val_loss: 234.8284 - val_mae: 9.0185\n",
      "\n",
      "Epoch 09906: val_mae did not improve from 6.28420\n",
      "Epoch 9907/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.0250 - mae: 6.3931 - val_loss: 216.4587 - val_mae: 8.1397\n",
      "\n",
      "Epoch 09907: val_mae did not improve from 6.28420\n",
      "Epoch 9908/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 60.0170 - mae: 6.2602 - val_loss: 214.4735 - val_mae: 7.7914\n",
      "\n",
      "Epoch 09908: val_mae did not improve from 6.28420\n",
      "Epoch 9909/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 68.3128 - mae: 6.5089 - val_loss: 209.7120 - val_mae: 7.6628\n",
      "\n",
      "Epoch 09909: val_mae did not improve from 6.28420\n",
      "Epoch 9910/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 66.9529 - mae: 6.4919 - val_loss: 210.5793 - val_mae: 7.8872\n",
      "\n",
      "Epoch 09910: val_mae did not improve from 6.28420\n",
      "Epoch 9911/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 66.5521 - mae: 6.4309 - val_loss: 234.8394 - val_mae: 9.0555\n",
      "\n",
      "Epoch 09911: val_mae did not improve from 6.28420\n",
      "Epoch 9912/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 91.9531 - mae: 7.9173 - val_loss: 257.8838 - val_mae: 10.5121\n",
      "\n",
      "Epoch 09912: val_mae did not improve from 6.28420\n",
      "Epoch 9913/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.9623 - mae: 7.5574 - val_loss: 208.2411 - val_mae: 7.8206\n",
      "\n",
      "Epoch 09913: val_mae did not improve from 6.28420\n",
      "Epoch 9914/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 73.9724 - mae: 6.9694 - val_loss: 215.4697 - val_mae: 7.9926\n",
      "\n",
      "Epoch 09914: val_mae did not improve from 6.28420\n",
      "Epoch 9915/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 71.8073 - mae: 6.7281 - val_loss: 222.9933 - val_mae: 8.3672\n",
      "\n",
      "Epoch 09915: val_mae did not improve from 6.28420\n",
      "Epoch 9916/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 75.4383 - mae: 6.8398 - val_loss: 193.9374 - val_mae: 7.1433\n",
      "\n",
      "Epoch 09916: val_mae did not improve from 6.28420\n",
      "Epoch 9917/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 76.6112 - mae: 6.9761 - val_loss: 228.5588 - val_mae: 8.4554\n",
      "\n",
      "Epoch 09917: val_mae did not improve from 6.28420\n",
      "Epoch 9918/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 80.3671 - mae: 6.9812 - val_loss: 267.8331 - val_mae: 10.4349\n",
      "\n",
      "Epoch 09918: val_mae did not improve from 6.28420\n",
      "Epoch 9919/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.7343 - mae: 7.2252 - val_loss: 223.4197 - val_mae: 8.5234\n",
      "\n",
      "Epoch 09919: val_mae did not improve from 6.28420\n",
      "Epoch 9920/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.9815 - mae: 6.7883 - val_loss: 208.5321 - val_mae: 7.3386\n",
      "\n",
      "Epoch 09920: val_mae did not improve from 6.28420\n",
      "Epoch 9921/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 77.5292 - mae: 7.2300 - val_loss: 225.2421 - val_mae: 8.1512\n",
      "\n",
      "Epoch 09921: val_mae did not improve from 6.28420\n",
      "Epoch 9922/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 65.0375 - mae: 6.4390 - val_loss: 229.6020 - val_mae: 8.3179\n",
      "\n",
      "Epoch 09922: val_mae did not improve from 6.28420\n",
      "Epoch 9923/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 62.9754 - mae: 6.2462 - val_loss: 205.1878 - val_mae: 7.2987\n",
      "\n",
      "Epoch 09923: val_mae did not improve from 6.28420\n",
      "Epoch 9924/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 63.3959 - mae: 6.2311 - val_loss: 202.7087 - val_mae: 7.3633\n",
      "\n",
      "Epoch 09924: val_mae did not improve from 6.28420\n",
      "Epoch 9925/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 78.8677 - mae: 6.8368 - val_loss: 217.7225 - val_mae: 8.2408\n",
      "\n",
      "Epoch 09925: val_mae did not improve from 6.28420\n",
      "Epoch 9926/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 69.3020 - mae: 6.6570 - val_loss: 221.0574 - val_mae: 7.9848\n",
      "\n",
      "Epoch 09926: val_mae did not improve from 6.28420\n",
      "Epoch 9927/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 98.4344 - mae: 7.6976 - val_loss: 222.6480 - val_mae: 8.8041\n",
      "\n",
      "Epoch 09927: val_mae did not improve from 6.28420\n",
      "Epoch 9928/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 49.5047 - mae: 5.6206 - val_loss: 198.2426 - val_mae: 7.3087\n",
      "\n",
      "Epoch 09928: val_mae did not improve from 6.28420\n",
      "Epoch 9929/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 70.4959 - mae: 6.5504 - val_loss: 213.4780 - val_mae: 7.9589\n",
      "\n",
      "Epoch 09929: val_mae did not improve from 6.28420\n",
      "Epoch 9930/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 78.7217 - mae: 6.9805 - val_loss: 202.1656 - val_mae: 7.6336\n",
      "\n",
      "Epoch 09930: val_mae did not improve from 6.28420\n",
      "Epoch 9931/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 66.4488 - mae: 6.4333 - val_loss: 206.7701 - val_mae: 7.8049\n",
      "\n",
      "Epoch 09931: val_mae did not improve from 6.28420\n",
      "Epoch 9932/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 81.0911 - mae: 7.1537 - val_loss: 229.8785 - val_mae: 9.0604\n",
      "\n",
      "Epoch 09932: val_mae did not improve from 6.28420\n",
      "Epoch 9933/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 97.1251 - mae: 7.8784 - val_loss: 245.6866 - val_mae: 9.6476\n",
      "\n",
      "Epoch 09933: val_mae did not improve from 6.28420\n",
      "Epoch 9934/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.4757 - mae: 6.7477 - val_loss: 220.0980 - val_mae: 8.3679\n",
      "\n",
      "Epoch 09934: val_mae did not improve from 6.28420\n",
      "Epoch 9935/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 68.3678 - mae: 6.5610 - val_loss: 236.0561 - val_mae: 8.8794\n",
      "\n",
      "Epoch 09935: val_mae did not improve from 6.28420\n",
      "Epoch 9936/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 120.5145 - mae: 8.7578 - val_loss: 217.6093 - val_mae: 8.4064\n",
      "\n",
      "Epoch 09936: val_mae did not improve from 6.28420\n",
      "Epoch 9937/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 92.6750 - mae: 7.6731 - val_loss: 202.0471 - val_mae: 7.5321\n",
      "\n",
      "Epoch 09937: val_mae did not improve from 6.28420\n",
      "Epoch 9938/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 85.6099 - mae: 7.5276 - val_loss: 238.1026 - val_mae: 9.5244\n",
      "\n",
      "Epoch 09938: val_mae did not improve from 6.28420\n",
      "Epoch 9939/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 104.8938 - mae: 8.2304 - val_loss: 224.0002 - val_mae: 8.3917\n",
      "\n",
      "Epoch 09939: val_mae did not improve from 6.28420\n",
      "Epoch 9940/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 83.8528 - mae: 7.3905 - val_loss: 225.2771 - val_mae: 8.4362\n",
      "\n",
      "Epoch 09940: val_mae did not improve from 6.28420\n",
      "Epoch 9941/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 82.7135 - mae: 7.2057 - val_loss: 215.9793 - val_mae: 8.2423\n",
      "\n",
      "Epoch 09941: val_mae did not improve from 6.28420\n",
      "Epoch 9942/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 79.7863 - mae: 7.1200 - val_loss: 289.9830 - val_mae: 11.3725\n",
      "\n",
      "Epoch 09942: val_mae did not improve from 6.28420\n",
      "Epoch 9943/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 117.3191 - mae: 8.8205 - val_loss: 246.0939 - val_mae: 9.5068\n",
      "\n",
      "Epoch 09943: val_mae did not improve from 6.28420\n",
      "Epoch 9944/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 88.0236 - mae: 7.4246 - val_loss: 227.9670 - val_mae: 8.8866\n",
      "\n",
      "Epoch 09944: val_mae did not improve from 6.28420\n",
      "Epoch 9945/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 100.0114 - mae: 7.9073 - val_loss: 208.7019 - val_mae: 8.0215\n",
      "\n",
      "Epoch 09945: val_mae did not improve from 6.28420\n",
      "Epoch 9946/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 80.4926 - mae: 7.2782 - val_loss: 221.2246 - val_mae: 8.3715\n",
      "\n",
      "Epoch 09946: val_mae did not improve from 6.28420\n",
      "Epoch 9947/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 68.6192 - mae: 6.3776 - val_loss: 220.4833 - val_mae: 8.3923\n",
      "\n",
      "Epoch 09947: val_mae did not improve from 6.28420\n",
      "Epoch 9948/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.5374 - mae: 7.0226 - val_loss: 202.0544 - val_mae: 7.2973\n",
      "\n",
      "Epoch 09948: val_mae did not improve from 6.28420\n",
      "Epoch 9949/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.2876 - mae: 6.6465 - val_loss: 205.5181 - val_mae: 7.2899\n",
      "\n",
      "Epoch 09949: val_mae did not improve from 6.28420\n",
      "Epoch 9950/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 63.3501 - mae: 6.3142 - val_loss: 225.2885 - val_mae: 8.1555\n",
      "\n",
      "Epoch 09950: val_mae did not improve from 6.28420\n",
      "Epoch 9951/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 76.8438 - mae: 7.0462 - val_loss: 198.1921 - val_mae: 7.0232\n",
      "\n",
      "Epoch 09951: val_mae did not improve from 6.28420\n",
      "Epoch 9952/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 62.0114 - mae: 6.3330 - val_loss: 211.6857 - val_mae: 7.5152\n",
      "\n",
      "Epoch 09952: val_mae did not improve from 6.28420\n",
      "Epoch 9953/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 66.4210 - mae: 6.3950 - val_loss: 214.1079 - val_mae: 7.8461\n",
      "\n",
      "Epoch 09953: val_mae did not improve from 6.28420\n",
      "Epoch 9954/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 78.0293 - mae: 6.8380 - val_loss: 247.7211 - val_mae: 9.5601\n",
      "\n",
      "Epoch 09954: val_mae did not improve from 6.28420\n",
      "Epoch 9955/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 95.9524 - mae: 7.7699 - val_loss: 278.3516 - val_mae: 10.6822\n",
      "\n",
      "Epoch 09955: val_mae did not improve from 6.28420\n",
      "Epoch 9956/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.3601 - mae: 6.9141 - val_loss: 211.8215 - val_mae: 7.7489\n",
      "\n",
      "Epoch 09956: val_mae did not improve from 6.28420\n",
      "Epoch 9957/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 67.6844 - mae: 6.5298 - val_loss: 208.9944 - val_mae: 7.5285\n",
      "\n",
      "Epoch 09957: val_mae did not improve from 6.28420\n",
      "Epoch 9958/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 66.7808 - mae: 6.3896 - val_loss: 254.2449 - val_mae: 9.8609\n",
      "\n",
      "Epoch 09958: val_mae did not improve from 6.28420\n",
      "Epoch 9959/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.0012 - mae: 7.3189 - val_loss: 217.3280 - val_mae: 8.2416\n",
      "\n",
      "Epoch 09959: val_mae did not improve from 6.28420\n",
      "Epoch 9960/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 78.9233 - mae: 7.0942 - val_loss: 274.8335 - val_mae: 10.7730\n",
      "\n",
      "Epoch 09960: val_mae did not improve from 6.28420\n",
      "Epoch 9961/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 87.9619 - mae: 7.5864 - val_loss: 228.7784 - val_mae: 8.6774\n",
      "\n",
      "Epoch 09961: val_mae did not improve from 6.28420\n",
      "Epoch 9962/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 75.5449 - mae: 6.9553 - val_loss: 244.9601 - val_mae: 9.6680\n",
      "\n",
      "Epoch 09962: val_mae did not improve from 6.28420\n",
      "Epoch 9963/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 74.4062 - mae: 6.8263 - val_loss: 238.3523 - val_mae: 9.4781\n",
      "\n",
      "Epoch 09963: val_mae did not improve from 6.28420\n",
      "Epoch 9964/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 63.3774 - mae: 6.3829 - val_loss: 192.1499 - val_mae: 6.9992\n",
      "\n",
      "Epoch 09964: val_mae did not improve from 6.28420\n",
      "Epoch 9965/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 59.4651 - mae: 6.0867 - val_loss: 194.1513 - val_mae: 6.9836\n",
      "\n",
      "Epoch 09965: val_mae did not improve from 6.28420\n",
      "Epoch 9966/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 63.8895 - mae: 6.4245 - val_loss: 224.1885 - val_mae: 8.8583\n",
      "\n",
      "Epoch 09966: val_mae did not improve from 6.28420\n",
      "Epoch 9967/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.1872 - mae: 7.0589 - val_loss: 217.7467 - val_mae: 8.3631\n",
      "\n",
      "Epoch 09967: val_mae did not improve from 6.28420\n",
      "Epoch 9968/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 78.4712 - mae: 6.9779 - val_loss: 247.0880 - val_mae: 9.7819\n",
      "\n",
      "Epoch 09968: val_mae did not improve from 6.28420\n",
      "Epoch 9969/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 94.9361 - mae: 7.6041 - val_loss: 225.8119 - val_mae: 8.5222\n",
      "\n",
      "Epoch 09969: val_mae did not improve from 6.28420\n",
      "Epoch 9970/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 75.5524 - mae: 6.8106 - val_loss: 197.3026 - val_mae: 7.0197\n",
      "\n",
      "Epoch 09970: val_mae did not improve from 6.28420\n",
      "Epoch 9971/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 55.1654 - mae: 5.7370 - val_loss: 211.8750 - val_mae: 7.8086\n",
      "\n",
      "Epoch 09971: val_mae did not improve from 6.28420\n",
      "Epoch 9972/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 71.0147 - mae: 6.7337 - val_loss: 200.4137 - val_mae: 7.6357\n",
      "\n",
      "Epoch 09972: val_mae did not improve from 6.28420\n",
      "Epoch 9973/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 90.0957 - mae: 7.6698 - val_loss: 220.1005 - val_mae: 8.3944\n",
      "\n",
      "Epoch 09973: val_mae did not improve from 6.28420\n",
      "Epoch 9974/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 53.2429 - mae: 5.8661 - val_loss: 213.8227 - val_mae: 8.0800\n",
      "\n",
      "Epoch 09974: val_mae did not improve from 6.28420\n",
      "Epoch 9975/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 65.7358 - mae: 6.5270 - val_loss: 225.7954 - val_mae: 8.5405\n",
      "\n",
      "Epoch 09975: val_mae did not improve from 6.28420\n",
      "Epoch 9976/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 91.7604 - mae: 7.5123 - val_loss: 249.1615 - val_mae: 9.8034\n",
      "\n",
      "Epoch 09976: val_mae did not improve from 6.28420\n",
      "Epoch 9977/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 66.4744 - mae: 6.3715 - val_loss: 197.9079 - val_mae: 7.5965\n",
      "\n",
      "Epoch 09977: val_mae did not improve from 6.28420\n",
      "Epoch 9978/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 64.0577 - mae: 6.2735 - val_loss: 203.7840 - val_mae: 7.8080\n",
      "\n",
      "Epoch 09978: val_mae did not improve from 6.28420\n",
      "Epoch 9979/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 71.9964 - mae: 6.6821 - val_loss: 220.0177 - val_mae: 8.6400\n",
      "\n",
      "Epoch 09979: val_mae did not improve from 6.28420\n",
      "Epoch 9980/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 85.6492 - mae: 7.3203 - val_loss: 208.2387 - val_mae: 7.7604\n",
      "\n",
      "Epoch 09980: val_mae did not improve from 6.28420\n",
      "Epoch 9981/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 72.0003 - mae: 7.0115 - val_loss: 221.4635 - val_mae: 8.4661\n",
      "\n",
      "Epoch 09981: val_mae did not improve from 6.28420\n",
      "Epoch 9982/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 82.8503 - mae: 7.0083 - val_loss: 217.0748 - val_mae: 8.5476\n",
      "\n",
      "Epoch 09982: val_mae did not improve from 6.28420\n",
      "Epoch 9983/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 101.1250 - mae: 8.0715 - val_loss: 198.6715 - val_mae: 7.6773\n",
      "\n",
      "Epoch 09983: val_mae did not improve from 6.28420\n",
      "Epoch 9984/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 61.7319 - mae: 6.2100 - val_loss: 216.4436 - val_mae: 8.7286\n",
      "\n",
      "Epoch 09984: val_mae did not improve from 6.28420\n",
      "Epoch 9985/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 73.2263 - mae: 6.7236 - val_loss: 225.6252 - val_mae: 8.8259\n",
      "\n",
      "Epoch 09985: val_mae did not improve from 6.28420\n",
      "Epoch 9986/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 84.7667 - mae: 7.5210 - val_loss: 198.0956 - val_mae: 7.5111\n",
      "\n",
      "Epoch 09986: val_mae did not improve from 6.28420\n",
      "Epoch 9987/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 66.6948 - mae: 6.5354 - val_loss: 207.1211 - val_mae: 7.8483\n",
      "\n",
      "Epoch 09987: val_mae did not improve from 6.28420\n",
      "Epoch 9988/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 111.1667 - mae: 8.5261 - val_loss: 198.5997 - val_mae: 7.3544\n",
      "\n",
      "Epoch 09988: val_mae did not improve from 6.28420\n",
      "Epoch 9989/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 68.0156 - mae: 6.6418 - val_loss: 197.0524 - val_mae: 7.5196\n",
      "\n",
      "Epoch 09989: val_mae did not improve from 6.28420\n",
      "Epoch 9990/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 77.8708 - mae: 7.0438 - val_loss: 222.6006 - val_mae: 8.7529\n",
      "\n",
      "Epoch 09990: val_mae did not improve from 6.28420\n",
      "Epoch 9991/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 90.6341 - mae: 7.4040 - val_loss: 193.2715 - val_mae: 7.3972\n",
      "\n",
      "Epoch 09991: val_mae did not improve from 6.28420\n",
      "Epoch 9992/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 66.5957 - mae: 6.4991 - val_loss: 218.6685 - val_mae: 8.6520\n",
      "\n",
      "Epoch 09992: val_mae did not improve from 6.28420\n",
      "Epoch 9993/10000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 92.9943 - mae: 7.5465 - val_loss: 210.2427 - val_mae: 7.6630\n",
      "\n",
      "Epoch 09993: val_mae did not improve from 6.28420\n",
      "Epoch 9994/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 64.4207 - mae: 6.3862 - val_loss: 206.8065 - val_mae: 7.5962\n",
      "\n",
      "Epoch 09994: val_mae did not improve from 6.28420\n",
      "Epoch 9995/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 64.3843 - mae: 6.4165 - val_loss: 224.7544 - val_mae: 8.6868\n",
      "\n",
      "Epoch 09995: val_mae did not improve from 6.28420\n",
      "Epoch 9996/10000\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 92.4085 - mae: 7.7306 - val_loss: 254.0106 - val_mae: 10.5056\n",
      "\n",
      "Epoch 09996: val_mae did not improve from 6.28420\n",
      "Epoch 9997/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 92.6968 - mae: 7.6761 - val_loss: 242.8823 - val_mae: 9.5479\n",
      "\n",
      "Epoch 09997: val_mae did not improve from 6.28420\n",
      "Epoch 9998/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 103.8942 - mae: 8.1786 - val_loss: 233.9998 - val_mae: 8.7050\n",
      "\n",
      "Epoch 09998: val_mae did not improve from 6.28420\n",
      "Epoch 9999/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 83.8093 - mae: 7.3250 - val_loss: 218.4423 - val_mae: 8.1022\n",
      "\n",
      "Epoch 09999: val_mae did not improve from 6.28420\n",
      "Epoch 10000/10000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 67.6583 - mae: 6.4529 - val_loss: 207.2031 - val_mae: 7.9149\n",
      "\n",
      "Epoch 10000: val_mae did not improve from 6.28420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f08be071410>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_decay_steps = 100\n",
    "initial_learning_rate = 0.4\n",
    "lr_decayed_fn = (tf.keras.experimental.\n",
    "                 CosineDecayRestarts\n",
    "                 (initial_learning_rate, first_decay_steps))\n",
    "\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_mae', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "topModel.compile(loss='mse', optimizer=Adam(lr_decayed_fn), metrics=['mae'])\n",
    "topModel.fit(precomputed_train, train_y, epochs=10000, batch_size=1024, validation_data=(precomputed_val, valid_y),callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1725,
     "status": "ok",
     "timestamp": 1616818739995,
     "user": {
      "displayName": "김정환",
      "photoUrl": "",
      "userId": "11315038694579600734"
     },
     "user_tz": -540
    },
    "id": "nLNdUM1k4f59",
    "outputId": "22efb990-f686-4685-d699-88f9b6c5fb31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet152v2 (Functional)     (None, 2048)              58331648  \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 48)                4167216   \n",
      "=================================================================\n",
      "Total params: 62,498,864\n",
      "Trainable params: 4,155,952\n",
      "Non-trainable params: 58,342,912\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "final_model = Sequential([my_rnet, topModel])\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 1605,
     "status": "ok",
     "timestamp": 1616817971654,
     "user": {
      "displayName": "김정환",
      "photoUrl": "",
      "userId": "11315038694579600734"
     },
     "user_tz": -540
    },
    "id": "c1s7R89NbR7U"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "columns = test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 909,
     "status": "ok",
     "timestamp": 1616817870862,
     "user": {
      "displayName": "김정환",
      "photoUrl": "",
      "userId": "11315038694579600734"
     },
     "user_tz": -540
    },
    "id": "i0HFtD2va4r_",
    "outputId": "7ae09601-0ff3-47eb-d6fb-93fbc95216c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1600 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "test_datagen=ImageDataGenerator(rescale = 1/255)\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "     dataframe=test_df[:],\n",
    "         directory='test_imgs',\n",
    "             x_col=\"image\",\n",
    "                 y_col=columns[0],\n",
    "                     batch_size=1,\n",
    "                         shuffle=False,\n",
    "                             class_mode=\"raw\",\n",
    "                                 target_size=(1080,1920))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cWeXrWLUVD8"
   },
   "outputs": [],
   "source": [
    "pred = final_model.predict(test_generator, verbose=1)\n",
    "test_df.iloc[:,1:] = pred\n",
    "test_df.to_csv('keypoint_resNet.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Resnet152_KeyPoint.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
